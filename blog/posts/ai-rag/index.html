<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.3.4" /><meta property="og:title" content="Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes" /><meta property="og:locale" content="en" /><meta name="description" content="Preamble → This blog is a collection of my research and experimentation over two days to learn about RAG and LLMs. The main idea I was chasing was to set up RAG as a service (RAGaaS) - it’s pretty straightforward! Point a container (or a multi-container stack) to a directory containing notes or files and be able to talk to an LLM about it. I don’t know much about machine learning and this post is about me getting on the AI train." /><meta property="og:description" content="Preamble → This blog is a collection of my research and experimentation over two days to learn about RAG and LLMs. The main idea I was chasing was to set up RAG as a service (RAGaaS) - it’s pretty straightforward! Point a container (or a multi-container stack) to a directory containing notes or files and be able to talk to an LLM about it. I don’t know much about machine learning and this post is about me getting on the AI train." /><link rel="canonical" href="https://tanishq.page/blog/posts/ai-rag/" /><meta property="og:url" content="https://tanishq.page/blog/posts/ai-rag/" /><meta property="og:site_name" content="Tanishq Rupaal" /><meta property="og:image" content="https://tanishq.page/blog/assets/img/covers/rag-cover.jpg" /><meta property="og:image:alt" content="RAG Artwork" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-11-04T18:12:00-05:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://tanishq.page/blog/assets/img/covers/rag-cover.jpg" /><meta name="twitter:image:alt" content="RAG Artwork" /><meta property="twitter:title" content="Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes" /><meta name="twitter:site" content="@etheriosking" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-11-04T18:12:00-05:00","datePublished":"2024-11-04T18:12:00-05:00","description":"Preamble → This blog is a collection of my research and experimentation over two days to learn about RAG and LLMs. The main idea I was chasing was to set up RAG as a service (RAGaaS) - it’s pretty straightforward! Point a container (or a multi-container stack) to a directory containing notes or files and be able to talk to an LLM about it. I don’t know much about machine learning and this post is about me getting on the AI train.","headline":"Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes","image":{"alt":"RAG Artwork","url":"https://tanishq.page/blog/assets/img/covers/rag-cover.jpg","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://tanishq.page/blog/posts/ai-rag/"},"url":"https://tanishq.page/blog/posts/ai-rag/"}</script><title>Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes | Tanishq Rupaal</title><link rel="apple-touch-icon" sizes="180x180" href="/blog/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/blog/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/blog/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/blog/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/blog/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Tanishq Rupaal"><meta name="application-name" content="Tanishq Rupaal"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/blog/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/blog/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/blog/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script> <script defer src="/blog/assets/js/dist/post.min.js"></script> <script defer src="/blog/app.min.js?baseurl=/blog&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-B88HEPZBVJ"></script> <script> document.addEventListener('DOMContentLoaded', () => { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-B88HEPZBVJ'); }); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/blog/" id="avatar" class="rounded-circle"><img src="/blog/assets/Abstract.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/blog/">Tanishq Rupaal</a><p class="site-subtitle fst-italic mb-0">Cybersecurity Professional</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/blog/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/blog/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/blog/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/blog/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/blog/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/tanq16" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/etheriosking" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fab fa-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['dragonking47','proton.me'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://linkedin.com/in/tanishqrupaal" aria-label="linkedin" target="_blank" rel="noopener noreferrer" > <i class="fab fa-linkedin"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/blog/">Home</a> </span> <span>Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1730761920" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Nov 4, 2024 </time> </span><div class="mt-3 mb-3"> <a href="/blog/assets/img/covers/rag-cover.jpg" class="popup img-link preview-img shimmer"><img src="/blog/assets/img/covers/rag-cover.jpg" alt="RAG Artwork" width="1200" height="630" loading="lazy"></a><figcaption class="text-center pt-2 pb-2">RAG Artwork</figcaption></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://twitter.com/etheriosking">Tanishq Rupaal</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2098 words" > <em>11 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><blockquote class="prompt-info"><p><strong><em>Preamble</em></strong> → This blog is a collection of my research and experimentation over two days to learn about RAG and LLMs. The main idea I was chasing was to set up RAG as a service (<a href="https://github.com/Tanq16/RAGaaS">RAGaaS</a>) - it’s pretty straightforward! <strong><em>Point a container (or a multi-container stack) to a directory containing notes or files and be able to talk to an LLM about it.</em></strong> I don’t know much about machine learning and this post is about me getting on the AI train.</p></blockquote><p>The idea I stated above is actually very common and nothing novel. In fact, several offerings already exist on the internet, the most famous one being <a href="https://notebooklm.google/">NotebookLM</a> by Google. Several other projects on GitHub also exist, but I wanted to try it out for myself and play around with RAG parameters. Also, I wanted it to be completely local (i.e., no sending data to an online service). So, this post will include some preliminary knowledge gathering, sample code, and information on the problems I faced and solved (maybe).</p><blockquote class="prompt-warning"><p><strong><em>Keep in mind</em></strong> → All the knowledge listed below is a summarized regurgitation of concepts I’ve read from various online sources. As such, some things <em>may not</em> be academically accurate; they just represent my own high-level understanding.</p></blockquote><h2 id="concepts"><span class="me-2">Concepts</span><a href="#concepts" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Retrieval-Augmented Generation or RAG is a way of combining a data retrieval mechanism with an LLM to generate contextually relevant responses. It typically involves three key stages with distinct steps within each stage →</p><h4 id="data-ingestion"><span class="me-2">Data Ingestion</span><a href="#data-ingestion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li><strong><em><code class="language-plaintext highlighter-rouge">Data Collection and Preprocessing</code></em></strong> → RAG needs a collection of documents like notes, PDFs, web pages, etc. These documents often undergo preprocessing to trim useless data (like HTML tags), and are then split into smaller chunks to optimize them for embedding.<li><strong><em><code class="language-plaintext highlighter-rouge">Embedding</code></em></strong> → Each chunk of data is converted into a high-dimensional vector that preserves its semantic meaning. This is done using an embedding model (a smaller neural network like BERT or Sentence Transformers).<li><strong><em><code class="language-plaintext highlighter-rouge">Storage in Vector Database</code></em></strong> → The embedding vectors are stored in a vector database like Pinecone or FAISS or Chroma DB. Essentially, these databases help “retrieve” data based on the “distance” or similarity between vectors.</ul><p>More on <em>Embedding</em> →</p><ul><li>In an embedding space, texts with similar meanings are closer together, while texts with different meanings are further apart.<li>Traditional embeddings focus on words or phrases naively, but LLM embeddings leverage the models which are trained on large amounts of text and can do sequence or pattern prediction.</ul><h4 id="retrieval-process"><span class="me-2">Retrieval Process</span><a href="#retrieval-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li><strong><em><code class="language-plaintext highlighter-rouge">User Query Embedding</code></em></strong> → A user’s query is also converted into an embedding vector with the same model used for the documents. This makes it easier to search for semantically similar documents.<li><strong><em><code class="language-plaintext highlighter-rouge">Similarity Search</code></em></strong> → A similarity search helps find and return a list of documents or chunks that have vectors close or similar to the query vector. The closeness can be measured by cosine similarity or Euclidean distance or something else (these are the two common ones).<li><strong><em><code class="language-plaintext highlighter-rouge">Scoring and Filtering</code></em></strong> → Filtering or scoring can help refine results. It can use metadata or other rules to rank the documents.</ul><h4 id="forward-retrieved-documents-to-the-llm"><span class="me-2">Forward Retrieved Documents to the LLM</span><a href="#forward-retrieved-documents-to-the-llm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li><strong><em><code class="language-plaintext highlighter-rouge">System Prompt Setup</code></em></strong> → The LLM is given a system prompt that carries instructions for the model that sets the stage for the kind of responses required for the particular use case.<li><strong><em><code class="language-plaintext highlighter-rouge">Context Assembly</code></em></strong> → The retrieved documents are combined with the user query and system prompt (literally! just combined) and passed to the LLM. Some RAG implementations may employ a more structured approach like tagging or chaining input segments.<li><strong><em><code class="language-plaintext highlighter-rouge">Response Generation</code></em></strong> → Finally, the LLM uses the entire context to generate a response. Typically, this step works due to the larger context windows of modern models.</ul><p>These steps stated above are how a general RAG system is implemented and can provide contextually aware responses against highly specific information corpora. Now let’s explore the setup I landed on!</p><h2 id="exploring-a-setup"><span class="me-2">Exploring a Setup</span><a href="#exploring-a-setup" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Broadly speaking, given the details described in the previous section, the following components are needed for setting up RAG →</p><ul><li><strong><em><code class="language-plaintext highlighter-rouge">Model Execution Platform</code></em></strong> → A platform that allows us to run arbitrary LLMs, like <a href="https://lmstudio.ai/">LMStudio</a> and <a href="https://ollama.com/">Ollama</a>. I went with <em>Ollama</em>.<li><strong><em><code class="language-plaintext highlighter-rouge">Code Execution Language and Platform</code></em></strong> → Generally, Python is the most famous language for ML tasks. I followed the same and put everything in Docker containers for ease of access, execution, and replication.<li><strong><em><code class="language-plaintext highlighter-rouge">Vector Database</code></em></strong> → There are several options like Chroma DB, FAISS, etc. I used <a href="https://qdrant.tech/">Qdrant</a> for this task, as it also supports a fancy UI to look through and analyze data collections.<li><strong><em><code class="language-plaintext highlighter-rouge">Query-Response LLM</code></em></strong> → The LLM used to do the final response generation. Using an OpenAI API Key to interact with GPT-based models is a very common pattern. I chose to use Llama3.1 via Ollama instead.<li><strong><em><code class="language-plaintext highlighter-rouge">Embedding Model</code></em></strong> → There are several models available for this, and landing on a particular one requires experimentation. I chose <code class="language-plaintext highlighter-rouge">mxbai-embed-large</code> due to its relatively high-dimensional output.<li><strong><em><code class="language-plaintext highlighter-rouge">LLM Orchestration Framework</code></em></strong> → <a href="https://www.langchain.com/">Langchain</a> is the most famous one, and that’s what I stuck with.</ul><blockquote class="prompt-tip"><p><strong><em>An easily forgotten fact</em></strong> → An orchestration framework like Langchain is only required to make the task of creating LLM applications easy. It provides wrappers around common LLM-based implementations that make it easy to spin something up in just a few lines of code. However, it’s not <strong>necessary</strong>. You could use standard SDKs for platforms like OpenAI and Qdrant, and build the exact same thing without an orchestration framework.</p></blockquote><p>That’s all we need to get started with the actual implementation of a local RAG system. Now let’s look at a breakdown of some code.</p><h2 id="proof-of-concept-code-walkthrough"><span class="me-2">Proof of Concept Code Walkthrough</span><a href="#proof-of-concept-code-walkthrough" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>First. start by installing the following python packages →</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>pip <span class="nb">install </span>langchain langchain_community langchain_ollama langchain_chroma langchain-qdrant qdrant-client
</pre></table></code></div></div><p>Next, import the necessary functions and libraries →</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">sys</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span>
<span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="n">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="n">langchain_qdrant</span> <span class="kn">import</span> <span class="n">QdrantVectorStore</span>
<span class="kn">from</span> <span class="n">qdrant_client</span> <span class="kn">import</span> <span class="n">QdrantClient</span>
<span class="kn">from</span> <span class="n">qdrant_client.http.models</span> <span class="kn">import</span> <span class="n">Distance</span><span class="p">,</span> <span class="n">VectorParams</span>
<span class="kn">from</span> <span class="n">langchain_ollama</span> <span class="kn">import</span> <span class="n">OllamaEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain_ollama</span> <span class="kn">import</span> <span class="n">ChatOllama</span>
<span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>
</pre></table></code></div></div><p>The following is a template to perform RAG. It is basically a system prompt followed by variables for context and question for the retrieved documents and user query respectively.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="n">RAG_TEMPLATE</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">
You are an assistant tasked with answering a question using the following retrieved context. Follow these guidelines to provide the most accurate and relevant answer:

1. **If you don</span><span class="sh">'</span><span class="s">t know the answer based on the context, explicitly say </span><span class="sh">"</span><span class="s">I don</span><span class="sh">'</span><span class="s">t know based on the provided context.</span><span class="sh">"</span><span class="s">** Avoid guessing or adding details not found in the context.
2. **Provide information in an organized, hierarchical format**: Use headings, bullet points, and numbering for clear structure, and employ paragraphs where appropriate.
3. **Include all relevant code snippets**: If the context includes code, ensure it is reproduced accurately in the answer.
4. **Focus on relevance**: Only include details directly related to the question. Do not introduce arbitrary or unrelated information from the context.
5. **Avoid redundancy**: Summarize where possible and avoid repeating information unless necessary for clarity.
6. **Acronyms**: For any acronyms you encounter in the query, do not use pre-existing knowledge. Instead, use the context provided to determine the meaning of the acronym.

**Context**:
{context}

**Question**:
{question}

**Answer**:</span><span class="sh">"""</span>
</pre></table></code></div></div><p>With the prompt ready, the next block of code loads the <code class="language-plaintext highlighter-rouge">docs</code> directory and searches for all markdown files using <code class="language-plaintext highlighter-rouge">glob</code>. These files are then serially loaded using the <code class="language-plaintext highlighter-rouge">TextLoader</code> class (there are other classes for loading Markdown, JSON, etc., but I went with text for simplicity). The loaded documents are then split using a method called recursive character splitting such that there are at max. 1000 characters in a split, with a max. overlap of 200 characters between 2 splits.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">loader</span> <span class="o">=</span> <span class="nc">DirectoryLoader</span><span class="p">(</span><span class="sh">"</span><span class="s">docs/</span><span class="sh">"</span><span class="p">,</span> <span class="n">glob</span><span class="o">=</span><span class="sh">"</span><span class="s">**/*.md</span><span class="sh">"</span><span class="p">,</span> <span class="n">loader_cls</span><span class="o">=</span><span class="n">TextLoader</span><span class="p">,</span> <span class="n">use_multithreading</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>
<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">all_splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</pre></table></code></div></div><p>Next, the following code block instantiates an Embedding model using Ollama and a Qdrant database client against a server instance that is already running via Docker (refer to <a href="https://qdrant.tech/documentation/guides/installation/#docker-and-docker-compose">Qdrant documentation</a> to launch a container). The code block below uses the Qdrant client to load each document in batches of size 35.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="n">local_embeddings</span> <span class="o">=</span> <span class="nc">OllamaEmbeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">mxbai-embed-large</span><span class="sh">"</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="sh">"</span><span class="s">http://host.docker.internal:11434</span><span class="sh">"</span><span class="p">)</span>
<span class="n">qdrant</span> <span class="o">=</span> <span class="nc">QdrantClient</span><span class="p">(</span><span class="sh">"</span><span class="s">http://host.docker.internal</span><span class="sh">"</span><span class="p">,</span><span class="n">port</span><span class="o">=</span><span class="mi">6333</span><span class="p">)</span>
<span class="n">qdrant</span><span class="p">.</span><span class="nf">create_collection</span><span class="p">(</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="sh">"</span><span class="s">knowledgebase</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">vectors_config</span><span class="o">=</span><span class="nc">VectorParams</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="n">Distance</span><span class="p">.</span><span class="n">COSINE</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="nc">QdrantVectorStore</span><span class="p">(</span>
    <span class="n">client</span><span class="o">=</span><span class="n">qdrant</span><span class="p">,</span>
    <span class="n">collection_name</span><span class="o">=</span><span class="n">vectordbname</span><span class="p">,</span>
    <span class="n">embedding</span><span class="o">=</span><span class="n">local_embeddings</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">35</span>
<span class="n">delay</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">all_splits</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">all_splits</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
    <span class="n">vectorstore</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="n">delay</span><span class="p">)</span>
</pre></table></code></div></div><p>With the vector database ready, the next code block defines a function for performing a semantic similarity search of a user-provided query across the vector database to yield 10 resulting documents. Additionally, the code defines an Ollama chat client for interacting with <code class="language-plaintext highlighter-rouge">Llama3.1</code>.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">format_hybrid_docs</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="sh">"</span><span class="s">similarity</span><span class="sh">"</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">k</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">}).</span><span class="nf">invoke</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">returndata</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">returndata</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">ChatOllama</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">llama3.1:8b</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="sh">"</span><span class="s">http://host.docker.internal:11434</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
</pre></table></code></div></div><p>Lastly, the final code section obtains questions from a user and repeatedly answers them given the context of the set of documents retrieved as a result of semantic similarity search against the query.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="n">rag_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">RAG_TEMPLATE</span><span class="p">)</span>
<span class="n">qa_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">format_hybrid_docs</span> <span class="o">|</span> <span class="nc">RunnablePassthrough</span><span class="p">(),</span> <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="n">rag_prompt</span> <span class="o">|</span> <span class="n">model</span> <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">ques</span> <span class="o">=</span> <span class="nf">input</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Ask me a question: </span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ques</span> <span class="o">==</span> <span class="sh">"</span><span class="s">exit</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">qa_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">ques</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">)</span>
</pre></table></code></div></div><blockquote class="prompt-info"><p>Note that it doesn’t resend old chat back to the model.</p></blockquote><h2 id="some-problems"><span class="me-2">Some Problems</span><a href="#some-problems" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Here are some of the issues I encountered during my experimentation →</p><ul><li><strong><em><code class="language-plaintext highlighter-rouge">Outdated Public Blogs</code></em></strong> → There’s a large number of example blogs on this topic, and a majority of them were out of date. Even though they were just a couple of months old, the development of things like Langchain is so fast-paced that material gets outdated very quickly. This is why it is best to use blogs (even this one) as a lesson and refer to the API and SDK documentation for any sort of implementation.<li><strong><em><code class="language-plaintext highlighter-rouge">Ollama API Rate Limiting</code></em></strong> → Ollama’s API exposed on the local interface sometimes encountered errors when creating embeddings, causing loss of certain documents. This was fixed in my code by adding a short delay, which fixed the issue. Such a delay may not be needed if using embeddings through OpenAI or something else.<li><strong><em><code class="language-plaintext highlighter-rouge">Response quality</code></em></strong> →<ul><li>There is no easy way to identify the best combination of an LLM and an embedding model. It depends on use-case and results of experimentation.<li>There is no easy way to identify the best splitting algorithm. It depends on the type of data, the structure of each document, and the total number of documents.<li>Similarly, there isn’t an easy way to determine the best ratio of chunk size to overlap size. Retaining contextual data in a particular chunk is very hard and depends highly on the data slice being vectorized.<li>The response quality can significantly deteriorate if the model in use has a short context window, in which case the system prompt combined with the context can go over that threshold and make the model lose sight that data. So, review the model’s context window before using it.<li>Lastly, the “retrieval” part of the RAG pipeline is a deterministic step (similarity match), which makes it the only step in the pipeline that is not “intelligent”. A simple way to fix that is by pre-processing documents or rewriting queries (see next section).</ul></ul><h2 id="whats-next-after-poc"><span class="me-2">What’s Next After PoC?</span><a href="#whats-next-after-poc" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>With this proof of concept ready, here are some items that I <em>may</em> try to implement in the future →</p><ul><li><strong><em><code class="language-plaintext highlighter-rouge">Query Rewriting</code></em></strong> → This is a method in which the user query is first sent to an LLM to rewrite it in a more descriptive manner and ask an overall better question, thereby increasing the chances of pulling more relevant documents as well as enhancing the response of the LLM.<li><strong><em><code class="language-plaintext highlighter-rouge">Document Pre-processing and Enrichment</code></em></strong> → Enriching documents is a process of using an LLM to rewrite (essentially) the document in a particular structure that makes it easy to maintain rich context in every document chunk so that correct documents are pulled during the retrieval process.<li><strong><em><code class="language-plaintext highlighter-rouge">Docker Compose</code></em></strong> → Using Docker compose to launch a RAG pipeline is a quality of life improvement making it repeatable and easy to run the pipeline. A compose file is already a part of my <a href="https://github.com/Tanq16/RAGaaS">RAGaaS</a> project.</ul><p>And that’s a wrap on the quick RAG lesson. Cheers!</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/blog/categories/computers-and-security/">Computers and Security</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/blog/tags/ai/" class="post-tag no-text-decoration" >ai</a> <a href="/blog/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/blog/tags/container/" class="post-tag no-text-decoration" >container</a> <a href="/blog/tags/rag/" class="post-tag no-text-decoration" >rag</a> <a href="/blog/tags/machine-learning/" class="post-tag no-text-decoration" >machine-learning</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Exploring%20RAG%20(Retrieval-Augmented%20Generation)%20and%20Talking%20to%20my%20Notes%20-%20Tanishq%20Rupaal&url=https%3A%2F%2Ftanishq.page%2Fblog%2Fposts%2Fai-rag%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Exploring%20RAG%20(Retrieval-Augmented%20Generation)%20and%20Talking%20to%20my%20Notes%20-%20Tanishq%20Rupaal&u=https%3A%2F%2Ftanishq.page%2Fblog%2Fposts%2Fai-rag%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Ftanishq.page%2Fblog%2Fposts%2Fai-rag%2F&text=Exploring%20RAG%20(Retrieval-Augmented%20Generation)%20and%20Talking%20to%20my%20Notes%20-%20Tanishq%20Rupaal" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Ftanishq.page%2Fblog%2Fposts%2Fai-rag%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Linkedin" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/blog/posts/go-coding/">The Way to Go(lang)</a><li class="text-truncate lh-lg"> <a href="/blog/posts/homelab-budgetlord/">Expense Tracking in Home Lab</a><li class="text-truncate lh-lg"> <a href="/blog/posts/homelab-fusion/">Leantime in Home Lab</a><li class="text-truncate lh-lg"> <a href="/blog/posts/ai-rag/">Exploring RAG (Retrieval-Augmented Generation) and Talking to my Notes</a><li class="text-truncate lh-lg"> <a href="/blog/posts/cst-guide/">Containerized Security Toolkit - A Guide</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/blog/tags/home-lab/">home-lab</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/lab/">lab</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/productivity/">productivity</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/offsec-proving-grounds/">offsec-proving-grounds</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/oscp/">oscp</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/security/">security</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/aws/">aws</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/container/">container</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/docker/">docker</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/github/">github</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/blog/posts/cst-guide/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1729965120" data-df="ll" > Oct 26, 2024 </time><h4 class="pt-0 my-2">Containerized Security Toolkit - A Guide</h4><div class="text-muted"><p>Since writing this blog, I’ve updated the project significantly with multiple image options. The bulk of the usage below remains relevant, but the images are now named cst-&amp;lt;variant&amp;gt;:arm (or a...</p></div></div></a></article><article class="col"> <a href="/blog/posts/gha-docker-arch/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1717714320" data-df="ll" > Jun 6, 2024 </time><h4 class="pt-0 my-2">GitHub Actions & ARM Architecture</h4><div class="text-muted"><p>This blog post covers my troubleshooting efforts and research on how we can use GitHub workflows for various architectures. It focuses more on how to build container images for ARM in the absence o...</p></div></div></a></article><article class="col"> <a href="/blog/posts/docker-for-security/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1682833140" data-df="ll" > Apr 30, 2023 </time><h4 class="pt-0 my-2">Streamlining Security-Related Workflows with Docker Containers</h4><div class="text-muted"><p>I’m generally always looking for ways to improve my workflow and make my work as a cybersec professional more efficient. One of the tools that has had the biggest impact on my work is Docker. What...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/blog/posts/cst-guide/" class="btn btn-outline-primary" aria-label="Older" ><p>Containerized Security Toolkit - A Guide</p></a> <a href="/blog/posts/homelab-fusion/" class="btn btn-outline-primary" aria-label="Newer" ><p>Leantime in Home Lab</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2024</time> <a href="https://twitter.com/etheriosking">Tanishq Rupaal</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.2.4" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/blog/tags/home-lab/">home-lab</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/lab/">lab</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/productivity/">productivity</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/offsec-proving-grounds/">offsec-proving-grounds</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/oscp/">oscp</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/security/">security</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/aws/">aws</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/container/">container</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/docker/">docker</a> <a class="post-tag btn btn-outline-primary" href="/blog/tags/github/">github</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/blog/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
